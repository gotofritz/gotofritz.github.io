import{S as Dd,i as Md,s as qd,e as o,t as i,k as u,c as p,a as l,d as e,h as r,m as k,b as c,g as t,I as n,E as $r}from"./index-6e518972.js";function Fd(uk){let E,R,gn,sp,Aa,Pe,ep,Sa,I,np,xn,ap,tp,xs,op,pp,Ca,D,M,En,lp,za,y,ip,Es,rp,cp,Is,up,kp,Ts,dp,fp,La,Ps,kk=`<code class="language-txt">foo foo [nach dem] bar bar => foo foo [xxxx dxxx] bar bar
foo foo [in diesem]  bar bar => foo foo [xxxxx diesxxx] bar bar
foo foo [mit einem] bar bar => foo foo [xxxx einxxx] bar bar</code>`,Ra,Ae,hp,Da,q,F,In,mp,Ma,f,Tn,wp,yp,Pn,_p,vp,An,bp,gp,Sn,xp,Ep,Cn,Ip,qa,H,N,zn,Tp,Fa,As,Pp,Se,Ap,Ha,Ss,dk=`<code class="language-bash"><span class="token operator">></span> cookiecutter gh:gotofritz/cookiecutter-gotofritz-poetry
You've downloaded /Users/fritz/.cookiecutters/cookiecutter-gotofritz-poetry before. Is it okay to delete and re-download it? <span class="token punctuation">[</span>yes<span class="token punctuation">]</span>:
project_name <span class="token punctuation">[</span>new-project<span class="token punctuation">]</span>: german-learning
package_name <span class="token punctuation">[</span>germanlearning<span class="token punctuation">]</span>:
verbose_project_name <span class="token punctuation">[</span>My Awesome Project<span class="token punctuation">]</span>: German Language Drills
full_name <span class="token punctuation">[</span>Your Name<span class="token punctuation">]</span>: gotofritz
github_username <span class="token punctuation">[</span>github_username<span class="token punctuation">]</span>: gotofritz
mastodon_handle <span class="token punctuation">[</span>@your_name@mastodon.social<span class="token punctuation">]</span>: @gotofritz@mastodon.social
mastodon_url <span class="token punctuation">[</span>https://mastodon.social/@your_name<span class="token punctuation">]</span>: https://mastodon.social/@gotofritz
project_description <span class="token punctuation">[</span>this is a project<span class="token punctuation">]</span>: Some basic German language drills
python_version <span class="token punctuation">[</span><span class="token number">3.10</span>.4<span class="token punctuation">]</span>: <span class="token number">3.10</span>.6
<span class="token number">3.10</span>.6
Updating dependencies
Resolving dependencies<span class="token punctuation">..</span>. <span class="token punctuation">(</span><span class="token number">0</span>.4s<span class="token punctuation">)</span>
<span class="token punctuation">..</span>.</code>`,Na,Ce,Sp,ja,Cs,fk=`<code class="language-bash"><span class="token operator">></span> <span class="token function">mkdir</span> data
<span class="token operator">></span> <span class="token function">cp</span> ~/Downloads/articles.csv data/
<span class="token operator">></span> tree <span class="token builtin class-name">.</span>
<span class="token builtin class-name">.</span>
\u251C\u2500\u2500 CHANGELOG.md
\u251C\u2500\u2500 LICENSE.md
\u251C\u2500\u2500 Makefile
\u251C\u2500\u2500 README.md
\u251C\u2500\u2500 data
\u2502   \u2514\u2500\u2500 articles.csv
\u251C\u2500\u2500 <span class="token function">mkdir</span>
\u251C\u2500\u2500 poetry.lock
\u251C\u2500\u2500 pyproject.toml
\u251C\u2500\u2500 src
\u2502   \u2514\u2500\u2500 germanlearning
\u2502       \u251C\u2500\u2500 __init__.py
\u2502       \u2514\u2500\u2500 __pycache__
\u2514\u2500\u2500 tests
    \u251C\u2500\u2500 __init__.py
    \u251C\u2500\u2500 __pycache__
    \u251C\u2500\u2500 conftest.py
    \u2514\u2500\u2500 test_setup.py</code>`,Ba,j,B,Ln,Cp,Oa,O,G,Rn,zp,Ga,ze,Lp,Wa,W,Dn,Rp,Dp,Mn,Mp,Ua,Le,qp,Va,T,Fp,zs,Hp,Np,Ls,jp,Bp,Ka,Rs,hk=`<code class="language-bash"><span class="token operator">></span> poetry <span class="token function">add</span> spacy
Using version ^3.4.1 <span class="token keyword">for</span> spacy
<span class="token punctuation">..</span>.
<span class="token operator">></span> python <span class="token parameter variable">-m</span> spacy download de_core_news_sm
Collecting de-core-news-sm<span class="token operator">==</span><span class="token number">3.4</span>.0
<span class="token punctuation">..</span>.</code>`,Ja,Re,Op,Qa,Ds,mk=`<code class="language-python"><span class="token comment"># scripts/convert_raw_csv_to_something_usable.py</span>
<span class="token triple-quoted-string string">"""Converts a raw document with a full article per line to one split into sentences

Usage: python scripts/convert_raw_csv_to_something_usable.py data/articles.csv data/docs.tsv
"""</span>

<span class="token keyword">import</span> re
<span class="token keyword">import</span> sys
<span class="token keyword">from</span> pathlib <span class="token keyword">import</span> Path

<span class="token keyword">import</span> spacy

MIN_WORDS_IN_SENTENCE <span class="token operator">=</span> <span class="token number">3</span>


<span class="token keyword">def</span> <span class="token function">die</span><span class="token punctuation">(</span>msg<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""print an error message and exit"""</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>msg<span class="token punctuation">)</span>
    exit<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>


<span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>sys<span class="token punctuation">.</span>argv<span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token number">3</span><span class="token punctuation">:</span>
    die<span class="token punctuation">(</span><span class="token string">"Expecting two arguments: path to input file and path to output file"</span><span class="token punctuation">)</span>

input_path <span class="token operator">=</span> Path<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>argv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">if</span> <span class="token keyword">not</span> input_path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    die<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Path doesn't exist </span><span class="token interpolation"><span class="token punctuation">&#123;</span>input_path<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>

output_path <span class="token operator">=</span> Path<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>argv<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">if</span> <span class="token keyword">not</span> output_path<span class="token punctuation">.</span>parent<span class="token punctuation">.</span>exists<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    die<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Path doesn't exist </span><span class="token interpolation"><span class="token punctuation">&#123;</span>output_path<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
<span class="token keyword">if</span> output_path<span class="token punctuation">.</span>suffix <span class="token operator">!=</span> <span class="token string">".tsv"</span><span class="token punctuation">:</span>
    die<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Path should be a tsv file </span><span class="token interpolation"><span class="token punctuation">&#123;</span>output_path<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>

<span class="token comment"># loads the fast model for German</span>
nlp <span class="token operator">=</span> spacy<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"de_core_news_sm"</span><span class="token punctuation">)</span>

<span class="token comment"># ARTICLES_FILE is a CSV with two semicolon separated fields; I only care about the second.</span>
<span class="token comment"># Also remove any tabs, as they will interfere with</span>
noise_remover <span class="token operator">=</span> re<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span><span class="token string">r"^.+?;|&#92;t+"</span><span class="token punctuation">)</span>
<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>input_path<span class="token punctuation">,</span> <span class="token string">"r"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> csvfile<span class="token punctuation">:</span>
    docs <span class="token operator">=</span> <span class="token punctuation">[</span>noise_remover<span class="token punctuation">.</span>sub<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">,</span> line<span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> line <span class="token keyword">in</span> csvfile<span class="token punctuation">]</span>

output_lines <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> doc <span class="token keyword">in</span> docs<span class="token punctuation">:</span>
    <span class="token comment"># nlp(doc).sent splits a doc into sentences</span>
    sents <span class="token operator">=</span> <span class="token punctuation">[</span>sent<span class="token punctuation">.</span>text <span class="token keyword">for</span> sent <span class="token keyword">in</span> nlp<span class="token punctuation">(</span>doc<span class="token punctuation">)</span><span class="token punctuation">.</span>sents <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>sent<span class="token punctuation">)</span> <span class="token operator">>=</span> MIN_WORDS_IN_SENTENCE<span class="token punctuation">]</span>
    output_lines<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">"&#92;t"</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>sents<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"&#92;n"</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>output_lines<span class="token punctuation">)</span>
<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>output_path<span class="token punctuation">,</span> <span class="token string">"w"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> csvfile<span class="token punctuation">:</span>
    csvfile<span class="token punctuation">.</span>writelines<span class="token punctuation">(</span>output_lines<span class="token punctuation">)</span></code>`,Za,De,Gp,Xa,Ms,wk=`<code class="language-bash"><span class="token operator">></span> python scripts/convert_raw_csv_to_something_usable.py data/articles.csv data/docs.tsv

<span class="token comment"># look at first line of source</span>
\u276F <span class="token function">head</span> <span class="token parameter variable">-n1</span> data/articles.csv  <span class="token operator">|</span>  <span class="token function">fold</span> <span class="token parameter variable">-w</span> <span class="token number">64</span>
Etat<span class="token punctuation">;</span>Die ARD-Tochter Degeto hat sich verpflichtet, ab August ein
er Quotenregelung zu folgen, die f\xFCr die Gleichstellung von Regi
sseurinnen sorgen soll. In mindestens <span class="token number">20</span> Prozent der Filme, die
die ARD-Tochter Degeto produziert oder mitfinanziert, sollen ab
Mitte August Frauen Regie f\xFChren. Degeto-Chefin Christine Strobl
 folgt mit dieser Selbstverpflichtung der Forderung von Pro Quot
e Regie. Die Vereinigung von Regisseurinnen hatte im vergangenen
 Jahr eine Quotenregelung gefordert, um den weiblichen Filmschaf
fenden mehr Geh\xF6r und \xF6konomische Gleichstellung zu verschaffen.
 Pro Quote Regie kritisiert, dass, w\xE4hrend rund <span class="token number">50</span> Prozent der R
egie-Studierenden weiblich seien, der Anteil der Regisseurinnen
bei Fernsehfilmen nur bei <span class="token number">13</span> bis <span class="token number">15</span> Prozent liege. In \xD6sterreich
 sieht die Situation \xE4hnlich aus, auch hier wird von unterschied
lichen Seiten Handlungsbedarf angemahnt. Aber wie soll dieser au
ssehen? Ist die Einf\xFChrung der Quotenregelung auch f\xFCr die \xF6ster
reichische Film- und Fernsehlandschaft sinnvoll? Diskutieren Sie
 im Forum.

<span class="token comment"># Try to see whether it has split them into fields</span>
<span class="token operator">></span> <span class="token function">head</span> <span class="token parameter variable">-n1</span> data/docs.tsv  <span class="token operator">|</span>  <span class="token function">awk</span> -F<span class="token punctuation"></span>t <span class="token string">'&#123; print $2 &#125;'</span> <span class="token operator">|</span> <span class="token function">fold</span> <span class="token parameter variable">-w</span> <span class="token number">64</span>
In mindestens <span class="token number">20</span> Prozent der Filme, die die ARD-Tochter Degeto p
roduziert oder mitfinanziert, sollen ab Mitte August Frauen Regi
e f\xFChren.</code>`,Ya,U,V,qn,Wp,$a,Me,Up,st,_,Fn,Vp,Kp,Hn,Jp,Qp,Nn,Zp,Xp,jn,Yp,et,qe,$p,nt,qs,yk=`<code class="language-python"><span class="token comment"># tests/repositories/tsv_sentence_repository.py</span>
<span class="token keyword">from</span> germanlearning<span class="token punctuation">.</span>repositories<span class="token punctuation">.</span>tsv_sentence_repository <span class="token keyword">import</span> TsvSentenceRepository
<span class="token keyword">from</span> tests<span class="token punctuation">.</span>conftest <span class="token keyword">import</span> is_sentence

<span class="token keyword">def</span> <span class="token function">test_first_sentence_from_random_article</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Happy path for get_sentence"""</span>
    sut <span class="token operator">=</span> TsvSentenceRepository<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">assert</span> is_sentence<span class="token punctuation">(</span>sut<span class="token punctuation">.</span>next_sentence<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code>`,at,K,sl,Bn,el,nl,tt,Fs,_k=`<code class="language-python"><span class="token comment"># tests/test_utils.py</span>
<span class="token keyword">from</span> tests<span class="token punctuation">.</span>conftest <span class="token keyword">import</span> is_sentence

are_sentences <span class="token operator">=</span> <span class="token builtin">frozenset</span><span class="token punctuation">(</span><span class="token punctuation">[</span>
        <span class="token string">"By default, the Faker generates the data in English."</span><span class="token punctuation">,</span>
        <span class="token string">"What if you - want localized data?"</span><span class="token punctuation">,</span>
        <span class="token string">"There are two: different; types of provides!"</span><span class="token punctuation">,</span>
    <span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">test_is_sentence_ends_with_punctuation</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""True if it ends with ?.! and false otherwise"""</span>
    <span class="token keyword">assert</span> <span class="token builtin">all</span><span class="token punctuation">(</span>is_sentence<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span> <span class="token keyword">for</span> sentence <span class="token keyword">in</span> are_sentences<span class="token punctuation">)</span>

    should_not_pass <span class="token operator">=</span> <span class="token punctuation">[</span>sentence<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token keyword">for</span> sentence <span class="token keyword">in</span> are_sentences<span class="token punctuation">]</span>
    <span class="token keyword">assert</span> <span class="token keyword">not</span> <span class="token builtin">any</span><span class="token punctuation">(</span>is_sentence<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span> <span class="token keyword">for</span> sentence <span class="token keyword">in</span> should_not_pass<span class="token punctuation">)</span></code>`,ot,J,al,Fe,tl,ol,pt,Hs,vk=`<code class="language-python"><span class="token keyword">class</span> <span class="token class-name">TsvSentenceRepository</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Simple TSV based sentence repository

    Reads sentences from disk storage and allows a consumer to request
    the next one

    next_sentence: returns the next unread sentence from current doc

    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Read document source and stores all line boundaries"""</span>

    <span class="token keyword">def</span> <span class="token function">next_sentence</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">str</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Return a sentence from the data store"""</span>
        <span class="token keyword">return</span> <span class="token string">""</span></code>`,lt,He,pl,it,Ns,bk=`<code class="language-bash"><span class="token operator">></span>       assert is_sentence<span class="token punctuation">(</span>sut.get_sentence<span class="token punctuation">(</span><span class="token punctuation">))</span>
E       AssertionError: assert False
E        +  where False <span class="token operator">=</span> is_sentence<span class="token punctuation">(</span><span class="token string">''</span><span class="token punctuation">)</span></code>`,rt,Ne,ll,ct,js,gk=`<code class="language-python"><span class="token comment"># src/germanlearning/config.py</span>

<span class="token keyword">from</span> pathlib <span class="token keyword">import</span> Path

ROOT_DIR <span class="token operator">=</span> Path<span class="token punctuation">.</span>cwd<span class="token punctuation">(</span><span class="token punctuation">)</span>
DATA_DIR <span class="token operator">=</span> ROOT_DIR <span class="token operator">/</span> <span class="token string">"data"</span>
DOCS_FILE <span class="token operator">=</span> DATA_DIR <span class="token operator">/</span> <span class="token string">"docs.tsv"</span></code>`,ut,Q,il,On,rl,cl,kt,Bs,xk=`<code class="language-python"><span class="token keyword">from</span> typing <span class="token keyword">import</span> List

<span class="token keyword">from</span> germanlearning<span class="token punctuation">.</span>config <span class="token keyword">import</span> DOCS_FILE

SENTENCES_SEPARATOR <span class="token operator">=</span> <span class="token string">"&#92;t"</span>


<span class="token keyword">class</span> <span class="token class-name">TsvSentenceRepository</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Simple TSV based sentence repository

    Reads sentences from disk storage and allows a consumer to request
    the next one

    fetch_doc: reads a doc's list of sentences from storage
    next_sentence: returns the next unread sentence from current doc

    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Read document source and stores all line boundaries"""</span>
        self<span class="token punctuation">.</span>doc_boundaries<span class="token punctuation">:</span> List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>current<span class="token punctuation">:</span> List<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

        <span class="token comment"># SIM115: file kept open on purpose to minimise I/O activity</span>
        self<span class="token punctuation">.</span>docs <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span>DOCS_FILE<span class="token punctuation">,</span> <span class="token string">"rt"</span><span class="token punctuation">)</span>  <span class="token comment"># noqa SIM115</span>
        <span class="token keyword">while</span> self<span class="token punctuation">.</span>docs<span class="token punctuation">.</span>readline<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>doc_boundaries<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>docs<span class="token punctuation">.</span>tell<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">next_sentence</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">str</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Return a sentence from the data store"""</span>
        self<span class="token punctuation">.</span>_ensure_doc<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>current<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">fetch_doc</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> List<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Use the data boundaries to find a list of sentences for a given line"""</span>
        self<span class="token punctuation">.</span>docs<span class="token punctuation">.</span>seek<span class="token punctuation">(</span>self<span class="token punctuation">.</span>doc_boundaries<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token builtin">str</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>docs<span class="token punctuation">.</span>readline<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span>SENTENCES_SEPARATOR<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_ensure_doc</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Ensure there is a current doc"""</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>current<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>current <span class="token operator">=</span> self<span class="token punctuation">.</span>fetch_doc<span class="token punctuation">(</span><span class="token punctuation">)</span></code>`,dt,Z,ul,Gn,kl,dl,ft,Os,Ek=`<code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> germanlearning<span class="token punctuation">.</span>repositories<span class="token punctuation">.</span>tsv_sentence_repository <span class="token keyword">import</span> TsvSentenceRepository
<span class="token operator">>></span><span class="token operator">></span> t <span class="token operator">=</span> TsvSentenceRepository<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> t<span class="token punctuation">.</span>next_sentence<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token string">'Die ARD-Tochter Degeto hat sich verpflichtet, ab August einer Quotenregelung zu folgen, die f\xFCr die Gleichstellung von Regisseurinnen sorgen soll.'</span>
<span class="token operator">>></span><span class="token operator">></span> t<span class="token punctuation">.</span>next_sentence<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token string">'In mindestens 20 Prozent der Filme, die die ARD-Tochter Degeto produziert oder mitfinanziert, sollen ab Mitte August Frauen Regie f\xFChren.'</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token operator">>></span><span class="token operator">></span> t<span class="token punctuation">.</span>next_sentence<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token string">'Die ARD-Tochter Degeto hat sich verpflichtet, ab August einer Quotenregelung zu folgen, die f\xFCr die Gleichstellung von Regisseurinnen sorgen soll.'</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span></code>`,ht,je,fl,mt,X,Y,Wn,hl,wt,Be,ml,yt,Gs,Ik=`<code class="language-txt">foo foo nach  dem  bar bar f\xFCr   einen pop pop
foo foo xxxxx dxxx bar bar xxxxx einxx pop pop</code>`,_t,$,wl,Ws,yl,_l,vt,ss,es,Un,vl,bt,Oe,bl,gt,v,Us,gl,Vn,xl,El,Il,Vs,Tl,Kn,Pl,Al,Sl,Ks,Cl,Jn,zl,Ll,Rl,Qn,Dl,xt,Ge,Ml,Et,Js,Tk=`<code class="language-python"><span class="token comment"># tests/models/test_cloze_entity.py</span>
<span class="token keyword">from</span> germanlearning<span class="token punctuation">.</span>models <span class="token keyword">import</span> Cloze

<span class="token keyword">def</span> <span class="token function">test_new_cloze_returns_obfuscated</span><span class="token punctuation">(</span>fake<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""When a cloze is created, it will return the obfuscated text in string context"""</span>
    text <span class="token operator">=</span> fake<span class="token punctuation">.</span>pystr<span class="token punctuation">(</span><span class="token punctuation">)</span>
    obfuscated <span class="token operator">=</span> fake<span class="token punctuation">.</span>pystr<span class="token punctuation">(</span><span class="token punctuation">)</span>
    sut <span class="token operator">=</span> Cloze<span class="token punctuation">(</span>text<span class="token operator">=</span>text<span class="token punctuation">,</span> obfuscated<span class="token operator">=</span>obfuscated<span class="token punctuation">)</span>
    <span class="token keyword">assert</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>sut<span class="token punctuation">&#125;</span></span><span class="token string"> 123"</span></span> <span class="token operator">==</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>obfuscated<span class="token punctuation">&#125;</span></span><span class="token string"> 123"</span></span></code>`,It,We,ql,Tt,Qs,Pk=`<code class="language-bash"><span class="token operator">></span> poetry <span class="token function">add</span> pydantic@^1.7
Updating dependencies
Resolving dependencies<span class="token punctuation">..</span>. <span class="token punctuation">(</span><span class="token number">0</span>.3s<span class="token punctuation">)</span>
<span class="token punctuation">..</span>.</code>`,Pt,ns,Fl,Zn,Hl,Nl,At,Zs,Ak=`<code class="language-python"><span class="token comment"># src/models/__init__.py</span>
<span class="token keyword">from</span> <span class="token punctuation">.</span>cloze_entity <span class="token keyword">import</span> Cloze</code>`,St,Ue,jl,Ct,Xs,Sk=`<code class="language-python"><span class="token comment"># src/models/cloze_entity.py</span>
<span class="token keyword">from</span> pydantic <span class="token keyword">import</span> BaseModel<span class="token punctuation">,</span> Field


<span class="token keyword">class</span> <span class="token class-name">Cloze</span><span class="token punctuation">(</span>BaseModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    text<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> Field<span class="token punctuation">(</span>description<span class="token operator">=</span><span class="token string">"The text to be guessed"</span><span class="token punctuation">)</span>
    obfuscated<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> Field<span class="token punctuation">(</span>description<span class="token operator">=</span><span class="token string">"Shows the text as"</span><span class="token punctuation">)</span></code>`,zt,Ve,Bl,Lt,Ys,Ck=`<code class="language-bash">\u276F <span class="token function">make</span> <span class="token builtin class-name">test</span>
poetry run pytest tests
<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span> <span class="token builtin class-name">test</span> session starts <span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span>
<span class="token punctuation">..</span>.
<span class="token operator">></span>       assert sut <span class="token operator">==</span> obfuscated
E       AssertionError: assert Cloze<span class="token punctuation">(</span>text<span class="token operator">=</span><span class="token string">'FdscvPGgwNGEUxZCvWnj'</span>, <span class="token assign-left variable">obfuscated</span><span class="token operator">=</span><span class="token string">'ZGErTPzfeNSnbSFvvtPR'</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">'ZGErTPzfeNSnbSFvvtPR'</span></code>`,Rt,as,Ol,Xn,Gl,Wl,Dt,$s,zk=`<code class="language-diff">class Cloze(BaseModel):
<span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   text: str = Field(description="The text to be guessed")
</span><span class="token prefix unchanged"> </span><span class="token line">   obfuscated: str = Field(description="Shows the text as")
</span></span>
<span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    def __str__(self) -> str:
</span><span class="token prefix inserted">+</span><span class="token line">        """In string context it should return the string that reflects its state"""
</span><span class="token prefix inserted">+</span><span class="token line">        return self.text if self.guessed is True else self.obfuscated</span></span></code>`,Mt,Ke,Ul,qt,se,Lk=`<code class="language-python"><span class="token comment"># tests/models/test_cloze_entity.py</span>

<span class="token keyword">def</span> <span class="token function">test_cloze_guess</span><span class="token punctuation">(</span>fake<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""When you guess the state can change"""</span>
    text <span class="token operator">=</span> fake<span class="token punctuation">.</span>pystr<span class="token punctuation">(</span>count<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">)</span>
    obfuscated <span class="token operator">=</span> fake<span class="token punctuation">.</span>pystr<span class="token punctuation">(</span><span class="token punctuation">)</span>
    sut <span class="token operator">=</span> Cloze<span class="token punctuation">(</span>text<span class="token operator">=</span>text<span class="token punctuation">,</span> obfuscated<span class="token operator">=</span>obfuscated<span class="token punctuation">)</span>
    <span class="token keyword">assert</span> sut<span class="token punctuation">.</span>guessed <span class="token keyword">is</span> <span class="token boolean">False</span>
    <span class="token keyword">assert</span> sut<span class="token punctuation">.</span>guess<span class="token punctuation">(</span>fake<span class="token punctuation">.</span>pystr<span class="token punctuation">(</span>count<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">is</span> <span class="token boolean">False</span>
    <span class="token keyword">assert</span> sut<span class="token punctuation">.</span>guessed <span class="token keyword">is</span> <span class="token boolean">False</span>
    <span class="token keyword">assert</span> sut<span class="token punctuation">.</span>guess<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
    <span class="token keyword">assert</span> sut<span class="token punctuation">.</span>guessed <span class="token keyword">is</span> <span class="token boolean">True</span></code>`,Ft,Je,Vl,Ht,ee,Rk=`<code class="language-diff">class Cloze(BaseModel):
<span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   text: str = Field(description="The text to be guessed")
</span><span class="token prefix unchanged"> </span><span class="token line">   obfuscated: str = Field(description="Shows the text as")
</span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    guessed = False
</span></span>
<span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   def __str__(self) -> str:
</span><span class="token prefix unchanged"> </span><span class="token line">       """In string context it should return the string that reflects its state"""
</span><span class="token prefix unchanged"> </span><span class="token line">       return self.text if self.guessed is True else self.obfuscated
</span></span>
<span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    def guess(self, candidate: str) -> bool:
</span><span class="token prefix inserted">+</span><span class="token line">        """If the candidate str is the same as the text, it's guessed"""
</span><span class="token prefix inserted">+</span><span class="token line">        self.guessed = candidate == self.text
</span><span class="token prefix inserted">+</span><span class="token line">        return self.guessed</span></span></code>`,Nt,Qe,Kl,jt,ne,Dk=`<code class="language-diff">def test_new_cloze_returns_obfuscated(fake):
<span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   """When a cloze is created, it will return the obfuscated text in string context"""
</span><span class="token prefix unchanged"> </span><span class="token line">   text = fake.pystr()
</span><span class="token prefix unchanged"> </span><span class="token line">   obfuscated = fake.pystr()
</span><span class="token prefix unchanged"> </span><span class="token line">   sut = Cloze(text=text, obfuscated=obfuscated)
</span><span class="token prefix unchanged"> </span><span class="token line">   assert f"&#123;sut&#125; 123" == f"&#123;obfuscated&#125; 123"
</span></span>
<span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    sut.guess(fake.pystr())
</span><span class="token prefix inserted">+</span><span class="token line">    assert f"&#123;sut&#125; 123" == f"&#123;obfuscated&#125; 123"
</span></span>
<span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    sut.guess(text)
</span><span class="token prefix inserted">+</span><span class="token line">    assert f"&#123;sut&#125; 123" == f"&#123;text&#125; 123"</span></span></code>`,Bt,Ze,Jl,Ot,ae,Mk=`<code class="language-python"><span class="token comment"># tests/models/test_cloze_entity.py</span>

<span class="token keyword">def</span> <span class="token function">test_cloze_has_unique_id</span><span class="token punctuation">(</span>fake<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Like all entities, a Cloze has an immutable, unique ID"""</span>
    sut <span class="token operator">=</span> Cloze<span class="token punctuation">(</span>text<span class="token operator">=</span>fake<span class="token punctuation">.</span>pystr<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> obfuscated<span class="token operator">=</span>fake<span class="token punctuation">.</span>pystr<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    immutable_id <span class="token operator">=</span> sut<span class="token punctuation">.</span><span class="token builtin">id</span>
    <span class="token keyword">assert</span> immutable_id

    <span class="token keyword">with</span> pytest<span class="token punctuation">.</span>raises<span class="token punctuation">(</span>ValueError<span class="token punctuation">,</span> <span class="token keyword">match</span><span class="token operator">=</span><span class="token string">'"Cloze" object has no field "id"'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        sut<span class="token punctuation">.</span><span class="token builtin">id</span> <span class="token operator">=</span> fake<span class="token punctuation">.</span>uuid4<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># type: ignore</span>

    <span class="token keyword">assert</span> sut<span class="token punctuation">.</span><span class="token builtin">id</span> <span class="token operator">==</span> immutable_id</code>`,Gt,ts,Ql,Yn,Zl,Xl,Wt,te,qk=`<code class="language-diff"># src/models/cloze_entity.py

class Cloze(BaseModel):
<span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   text: str = Field(description="The text to be guessed")
</span><span class="token prefix unchanged"> </span><span class="token line">   obfuscated: str = Field(description="Shows the text as")
</span><span class="token prefix unchanged"> </span><span class="token line">   guessed: bool = Field(False, read_only=True)
</span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    _id: UUID = uuid4()
</span></span>
<span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    @property
</span><span class="token prefix inserted">+</span><span class="token line">    def id(self):
</span><span class="token prefix inserted">+</span><span class="token line">        """This is read only"""
</span><span class="token prefix inserted">+</span><span class="token line">        return self._id
</span></span>
<span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">    @id.setter
</span><span class="token prefix deleted">-</span><span class="token line">    def id(self, whatever):
</span><span class="token prefix deleted">-</span><span class="token line">        """You don't need this in this case"""</span></span></code>`,Ut,Xe,Yl,Vt,os,ps,$n,$l,Kt,P,si,sa,ei,ni,Ye,ai,ti,Jt,ls,is,ea,oi,Qt,$e,pi,Zt,sn,li,Xt,oe,Fk=`<code class="language-python"><span class="token comment"># src/models/base_entity.py</span>
<span class="token triple-quoted-string string">"""Represent a very basic DDD entity"""</span>

<span class="token keyword">from</span> uuid <span class="token keyword">import</span> UUID<span class="token punctuation">,</span> uuid4

<span class="token keyword">from</span> pydantic <span class="token keyword">import</span> BaseModel

<span class="token keyword">class</span> <span class="token class-name">BaseEntity</span><span class="token punctuation">(</span>BaseModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Represents a very basic DDD entity

    An entity has an immutable ID, and then some ValueObjects.
    """</span>

    _id<span class="token punctuation">:</span> UUID <span class="token operator">=</span> uuid4<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token decorator annotation punctuation">@property</span>
    <span class="token keyword">def</span> <span class="token function">id</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""This is read only"""</span>  <span class="token comment"># noqa D401</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>_id</code>`,Yt,en,ii,$t,pe,Hk=`<code class="language-diff"># src/models/cloze_entity.py

<span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">class Cloze(BaseModel):
</span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">class Cloze(BaseEntity):
</span></span><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   text: str = Field(description="The text to be guessed")
</span><span class="token prefix unchanged"> </span><span class="token line">   obfuscated: str = Field(description="Shows the text as")
</span><span class="token prefix unchanged"> </span><span class="token line">   guessed: bool = Field(False, read_only=True)
</span></span><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">    _id: UUID = uuid4()
</span></span>
<span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">    @property
</span><span class="token prefix deleted">-</span><span class="token line">    def id(self):
</span><span class="token prefix deleted">-</span><span class="token line">        """This is read only"""
</span><span class="token prefix deleted">-</span><span class="token line">        return self._id
</span></span></code>`,so,rs,ri,na,ci,ui,eo,nn,ki,no,d,aa,di,fi,ta,hi,mi,le,wi,oa,yi,_i,vi,pa,bi,gi,la,xi,Ei,ia,Ii,ao,cs,Ti,an,Pi,Ai,to,us,ks,ra,Si,oo,tn,Ci,po,A,ca,zi,Li,ua,Ri,Di,ka,Mi,lo,on,qi,io,ie,Nk=`<code class="language-python"><span class="token comment"># tests/services/guess_preposition_exercise_service.py</span>

<span class="token keyword">from</span> germanlearning<span class="token punctuation">.</span>models<span class="token punctuation">.</span>exercise <span class="token keyword">import</span> Exercise
<span class="token keyword">from</span> germanlearning<span class="token punctuation">.</span>services<span class="token punctuation">.</span>guess_preposition_exercise_service <span class="token keyword">import</span> <span class="token punctuation">(</span>
    GuessPrepositionExerciseService<span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">test_return_an_exercise_by_default</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    sut <span class="token operator">=</span> GuessPrepositionExerciseService<span class="token punctuation">(</span><span class="token punctuation">)</span>
    exercises_got <span class="token operator">=</span> sut<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">assert</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>exercises_got<span class="token punctuation">,</span> <span class="token builtin">list</span><span class="token punctuation">)</span></code>`,ro,h,Fi,da,Hi,Ni,fa,ji,Bi,ha,Oi,Gi,re,Wi,Ui,co,pn,Vi,uo,ce,jk=`<code class="language-python"><span class="token comment"># germanlearning/services/guess_preposition_exercise_service.py</span>

<span class="token keyword">class</span> <span class="token class-name">GuessPrepositionExerciseService</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Fetch raw sentences from Repository and generate Exercises on demand"""</span>

    <span class="token keyword">def</span> <span class="token function">get</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Provide exercises on demand"""</span>
        <span class="token keyword">return</span> <span class="token string">""</span></code>`,ko,ln,Ki,fo,ue,Bk=`<code class="language-diff"><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   def test_return_an_exercise_by_default():
</span><span class="token prefix unchanged"> </span><span class="token line">       sut = GuessPrepositionExerciseService()
</span><span class="token prefix unchanged"> </span><span class="token line">       exercises_got = sut.get()
</span></span><span class="token inserted-arrow inserted"><span class="token prefix inserted">></span><span class="token line">       assert isinstance(exercises_got, Exercise)
</span></span>E       AssertionError: assert False
E        +  where False = isinstance('', Exercise)</code>`,ho,rn,Ji,mo,ke,Ok=`<code class="language-python"><span class="token comment"># germanlearning/services/guess_preposition_exercise_service.py</span>

<span class="token keyword">class</span> <span class="token class-name">GuessPrepositionExerciseService</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Fetch raw sentences from Repository and generate Exercises on demand"""</span>

    <span class="token keyword">def</span> <span class="token function">get</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Provide exercises on demand"""</span>
        <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token punctuation">]</span></code>`,wo,cn,Qi,yo,de,Gk=`<code class="language-python"><span class="token comment"># tests/services/guess_preposition_exercise_service.py</span>

<span class="token keyword">def</span> <span class="token function">test_can_inject_a_repo_in_service</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Use dependency injection to push repo into service"""</span>

    repo <span class="token operator">=</span> mock<span class="token punctuation">.</span>MagicMock<span class="token punctuation">(</span>spec<span class="token operator">=</span>TsvSentenceRepository<span class="token punctuation">)</span>
    sut <span class="token operator">=</span> GuessPrepositionExerciseService<span class="token punctuation">(</span>repository<span class="token operator">=</span>repo<span class="token punctuation">)</span>
    sut<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span>
    repo<span class="token punctuation">.</span>next_sentence<span class="token punctuation">.</span>assert_called_once<span class="token punctuation">(</span><span class="token punctuation">)</span></code>`,_o,un,Zi,vo,fe,Wk=`<code class="language-diff"># germanlearning/services/guess_preposition_exercise_service.py

class GuessPrepositionExerciseService:
<span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   """Fetch raw sentences from Repository and generate Exercises on demand"""
</span></span>
<span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    repository: TsvSentenceRepository | None = Field(
</span><span class="token prefix inserted">+</span><span class="token line">        default_factory=TsvSentenceRepository,
</span><span class="token prefix inserted">+</span><span class="token line">        description="the source of sentences",
</span><span class="token prefix inserted">+</span><span class="token line">    )
</span></span>
<span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    class Config:
</span><span class="token prefix inserted">+</span><span class="token line">        # this is needed otherwise pydantic will complain
</span><span class="token prefix inserted">+</span><span class="token line">        arbitrary_types_allowed = True
</span></span>
<span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   def get(self):
</span><span class="token prefix unchanged"> </span><span class="token line">       """Provide exercises on demand"""
</span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">        sentence = self.repository.next_sentence()  # type: ignore
</span></span><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">        return ""
</span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">        return [sentence]</span></span></code>`,bo,kn,Xi,go,he,Uk=`<code class="language-bash"><span class="token operator">></span> <span class="token function">sort</span> <span class="token parameter variable">-R</span> data/docs.tsv <span class="token operator">|</span> <span class="token function">head</span> <span class="token parameter variable">-n1</span> <span class="token operator">|</span> <span class="token function">awk</span> '<span class="token punctuation">&#123;</span>gsub<span class="token punctuation">(</span>/<span class="token punctuation"></span>t/,<span class="token string">"<span class="token entity" title="&#92;n">&#92;n</span>"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> print<span class="token punctuation">&#125;</span>
Kapitalkontrollen bleiben <span class="token keyword">in</span> Kraft \u2013
B\xFCrger bekommen erste Steuererh\xF6hungen zu sp\xFCren \u2013
Neuer Arbeitsminister k\xFCndigt harte Verhandlungen an.
Auch mit Kanzlerin Angela Merkel sei Sch\xE4uble <span class="token keyword">in</span> einem Riesenkonflikt.
<span class="token punctuation">..</span>.
Er wird voraussichtlich <span class="token number">2016</span> von der Zhejiang University <span class="token keyword">in</span> Hangzhou an das Department f\xFCr Molekulare Evolution und Entwicklung der Universit\xE4t Wien wechseln.
<span class="token punctuation">..</span>.</code>`,xo,b,Yi,dn,$i,sr,ma,er,nr,me,ar,tr,Eo,we,Vk=`<code class="language-python"><span class="token comment"># tests/services/test_guess_preposition_exercise_service.py</span>

<span class="token keyword">from</span> tests<span class="token punctuation">.</span>testsupport <span class="token keyword">import</span> sentences_as_arrays <span class="token keyword">as</span> sentences
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

<span class="token decorator annotation punctuation">@pytest<span class="token punctuation">.</span>mark<span class="token punctuation">.</span>parametrize</span><span class="token punctuation">(</span>
    <span class="token string">"sentence"</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span>
        sentences<span class="token punctuation">.</span>with_adp<span class="token punctuation">,</span>
        sentences<span class="token punctuation">.</span>manche<span class="token punctuation">,</span>
        sentences<span class="token punctuation">.</span>diesx<span class="token punctuation">,</span>
        sentences<span class="token punctuation">.</span>with_article_no_prep<span class="token punctuation">,</span>
        sentences<span class="token punctuation">.</span>standard<span class="token punctuation">,</span>
    <span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">test_extract_exercise_starts_with_adp</span><span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Check it works for a variety of sentences that were problematic"""</span>
    repo <span class="token operator">=</span> mock<span class="token punctuation">.</span>Mock<span class="token punctuation">(</span>spec<span class="token operator">=</span>TsvSentenceRepository<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    repo<span class="token punctuation">.</span>next_sentence <span class="token operator">=</span> mock<span class="token punctuation">.</span>Mock<span class="token punctuation">(</span>return_value<span class="token operator">=</span><span class="token string">""</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">)</span>
    sut <span class="token operator">=</span> GuessPrepositionExerciseService<span class="token punctuation">(</span>repository<span class="token operator">=</span>repo<span class="token punctuation">)</span>
    exercises_got <span class="token operator">=</span> sut<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">for</span> <span class="token punctuation">(</span>actual_token<span class="token punctuation">,</span> expected<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>exercises_got<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>tokens<span class="token punctuation">,</span> sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>actual_token<span class="token punctuation">,</span> Cloze<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">assert</span> actual_token<span class="token punctuation">.</span>text <span class="token operator">==</span> expected  <span class="token comment"># type: ignore</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">assert</span> actual_token <span class="token operator">==</span> expected</code>`,Io,ds,or,wa,pr,lr,To,ye,Kk=`<code class="language-python"><span class="token comment"># tests/testsupport/sentences.py</span>
<span class="token triple-quoted-string string">"""Test sentences as array. Each item should be translated to an ExerciseToken"""</span>

standard <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token string">"Auch "</span><span class="token punctuation">,</span>
    <span class="token string">"mit "</span><span class="token punctuation">,</span>
    <span class="token string">"Kanzlerin Angela Merkel sei Sch\xE4uble "</span><span class="token punctuation">,</span>
    <span class="token string">"in einem "</span><span class="token punctuation">,</span>
    <span class="token string">"Riesenkonflikt."</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span>

with_article_no_prep <span class="token operator">=</span> <span class="token punctuation">[</span>
  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span></code>`,Po,m,ir,ya,rr,cr,_a,ur,kr,va,dr,fr,ba,hr,mr,Ao,_e,Jk=`<code class="language-python"><span class="token comment"># germanlearning/services/guess_preposition_exercise_service/guess_preposition_exercise_service.py</span>

<span class="token keyword">class</span> <span class="token class-name">GuessPrepositionExerciseService</span><span class="token punctuation">(</span>BaseModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Fetch raw sentences from Repository and generate Exercises on demand"""</span>

    repository<span class="token punctuation">:</span> TsvSentenceRepository <span class="token operator">|</span> <span class="token boolean">None</span> <span class="token operator">=</span> Field<span class="token punctuation">(</span>
        default_factory<span class="token operator">=</span>TsvSentenceRepository<span class="token punctuation">,</span>
        description<span class="token operator">=</span><span class="token string">"the source of sentences"</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
    _nlp<span class="token punctuation">:</span> Language <span class="token operator">=</span> PrivateAttr<span class="token punctuation">(</span>default_factory<span class="token operator">=</span><span class="token keyword">lambda</span><span class="token punctuation">:</span> spacy<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"de_core_news_sm"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">class</span> <span class="token class-name">Config</span><span class="token punctuation">:</span>  <span class="token comment"># noqa: D106, WPS431</span>
        arbitrary_types_allowed <span class="token operator">=</span> <span class="token boolean">True</span>

    <span class="token keyword">def</span> <span class="token function">get</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> how_many<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> List<span class="token punctuation">[</span>Exercise<span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Provide exercises on demand"""</span>
        collected_exercises<span class="token punctuation">:</span> List<span class="token punctuation">[</span>Exercise<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">while</span> <span class="token builtin">len</span><span class="token punctuation">(</span>collected_exercises<span class="token punctuation">)</span> <span class="token operator">&lt;</span> how_many<span class="token punctuation">:</span>
            sentence <span class="token operator">=</span> self<span class="token punctuation">.</span>repository<span class="token punctuation">.</span>next_sentence<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># type: ignore</span>
            <span class="token keyword">if</span> sentence <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                <span class="token keyword">raise</span> TypeError<span class="token punctuation">(</span><span class="token string">"No sentences available"</span><span class="token punctuation">)</span>
            tokens <span class="token operator">=</span> self<span class="token punctuation">.</span>_parse<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span>
            collected_exercises<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Exercise<span class="token punctuation">(</span>tokens<span class="token operator">=</span>tokens<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> collected_exercises

    <span class="token keyword">def</span> <span class="token function">_parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sentence<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> List<span class="token punctuation">[</span>ExerciseToken<span class="token punctuation">]</span><span class="token punctuation">:</span>
        parser <span class="token operator">=</span> TokenParser<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> parser<span class="token punctuation">.</span>parse<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_nlp<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># type: ignore</span></code>`,So,fn,wr,Co,ve,Qk=`<code class="language-python"><span class="token keyword">class</span> <span class="token class-name">TokenParser</span><span class="token punctuation">:</span>  <span class="token comment"># noqa: WPS214</span>
    <span class="token triple-quoted-string string">"""Parse nlp tokens generated by Spacy and generate events.

    Inspired by SAX parsers
    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> nlp_tokens<span class="token punctuation">:</span> Doc<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> List<span class="token punctuation">[</span>ExerciseToken<span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Go through the Spacy tokens and generates DSL based ExerciseToken"""</span>
        <span class="token keyword">for</span> nlp_token <span class="token keyword">in</span> nlp_tokens<span class="token punctuation">:</span>
            change_event <span class="token operator">=</span> self<span class="token punctuation">.</span>_change_event<span class="token punctuation">(</span>nlp_token<span class="token punctuation">)</span>
            <span class="token keyword">if</span> change_event <span class="token operator">==</span> ParseEvents<span class="token punctuation">.</span>ENDSTR<span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>_end_string<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">elif</span> change_event <span class="token operator">==</span> ParseEvents<span class="token punctuation">.</span>ENDCLOZE<span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>_end_cloze<span class="token punctuation">(</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>_dispatch_to_buffer<span class="token punctuation">(</span>nlp_token<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_end_doc<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>exercises_tokens</code>`,zo,fs,hs,ga,yr,Lo,hn,_r,Ro,g,xa,vr,br,Ea,gr,xr,Ia,Er,Ir,Ta,Tr,Do,ms,ws,Pa,Pr,Mo,w,Ar,be,Sr,Cr,ge,zr,Lr,xe,Rr,Dr,Ee,Mr,qr,qo,mn,Ie,Fr,Fo,wn,Te,Hr;return{c(){E=o("h2"),R=o("a"),gn=o("span"),sp=i("What I\u2019m trying to do"),Aa=u(),Pe=o("p"),ep=i("Oh, the never ending quest to master German. I am at the stage where I get it most of it right and can hold conversations. But I still make a lot of small mistakes. I do take language classes every so often to raise the level. But although the improvement is visible, it never fully eradicates those mistakes. I need many more repetitions than the the 10-12 offered by textbooks. In other words, I need Python."),Sa=u(),I=o("p"),np=i("Textbooks exercises seem to follow a few patterns. Sentences with words in the wrong order, and missing words seem to be the most popular. It shouldn\u2019t be too hard to replicate those exercises. Better, to "),xn=o("em"),ap=i("automate"),tp=i(" this replication so that I have an endless supply of exercises. I can use "),xs=o("a"),op=i("the free dataset Timo Block made available on GitHub"),pp=i(" as a starting point."),Ca=u(),D=o("h3"),M=o("a"),En=o("span"),lp=i("The end result"),za=u(),y=o("p"),ip=i("The end result will have some similarities with flashcards software. Particularly "),Es=o("a"),rp=i("Anki"),cp=i(", which I use a lot. I will use fall back on Anki concepts when needed, as I have vague plans to integrate some of these exercises with it. But what I am building now is a self contained CLI script. It will pick a German document and feed it to the interface sentence by sentence. The UI will obscure, or partially obscure, parts of the sentences. The task is to guess the obscured words. The first iteration will obscure \u201D"),Is=o("a"),up=i("adpositions"),kp=i("\u201D, (or \u201Cprepositions\u201D in traditional grammar). And will partially obscure \u201D"),Ts=o("a"),dp=i("determiners"),fp=i("\u201D (articles and some pronounous and some adjectives). An example should make this clearer"),La=u(),Ps=o("pre"),Ra=u(),Ae=o("p"),hp=i("A nice to have would also be to insert trick ones where there shouldn\u2019t be any. For example before years - it\u2019s a common mistake by English speakers to say \u201CIn 1989\u2026\u201D, but in German it\u2019s either \u201C1989\u2026\u201D or \u201CIm Jahr 1989\u2026\u201D"),Da=u(),q=o("h3"),F=o("a"),In=o("span"),mp=i("The plan"),Ma=u(),f=o("ol"),Tn=o("li"),wp=i("The raw dataset is a CSV file with a list of \u201Cdocs\u201D and labels. I don\u2019t need the label, but I need each doc\u2019s sequential \u201Csentences\u201D. That makes the CSV in its current form unsuitable. I will need to preprocess the CSV file"),yp=u(),Pn=o("li"),_p=i("A repository will load up the persistent storage (the CSV file). It will fetch a \u2018next\u2019 sentence whenever asked. It may also save updated versions of the CSV file."),vp=u(),An=o("li"),bp=i("An exercise generator will ask the repository for a sentence. It will then use NLP to turn into an exercise."),gp=u(),Sn=o("li"),xp=i("A session service will ask the exercise generator for N exercises. It will organise them as a queue, and will provide them to the UI when asked. It will also reorganise the queue depending on the outcome of the exercise."),Ep=u(),Cn=o("li"),Ip=i("The UI will ask the session service for the next exercise, display it, and allow the user to guess. It will then communicate success / error to the session service and get the next exercise."),qa=u(),H=o("h2"),N=o("a"),zn=o("span"),Tp=i("Step 0: Setting up the python project"),Fa=u(),As=o("p"),Pp=i("For that I simply use "),Se=o("a"),Ap=i("my trusty cookiecutter template"),Ha=u(),Ss=o("pre"),Na=u(),Ce=o("p"),Sp=i("And I\u2019m good to go. All I need is to create a folder with the article data"),ja=u(),Cs=o("pre"),Ba=u(),j=o("h2"),B=o("a"),Ln=o("span"),Cp=i("Creating a CLI app for prepositions training"),Oa=u(),O=o("h3"),G=o("a"),Rn=o("span"),zp=i("Step 1: Preprocessing the documents"),Ga=u(),ze=o("p"),Lp=i("So the source data is a csv where each row has two fields: a label which I don\u2019t need, and a \u201Cdoc\u201D which I need to split into \u201Csentences\u201D. I can think of two ways of handling that:"),Wa=u(),W=o("ol"),Dn=o("li"),Rp=i("remove the labels from the CSV and turn it into a TXT with a doc on each line. The repository will then be responsible for lazily splitting a doc in sentences, and holding them in memory"),Dp=u(),Mn=o("li"),Mp=i("remove the labels and split each row\u2019s single doc field into multiple sentence fields. The repository will not do any splitting"),Ua=u(),Le=o("p"),qp=i("(1) is easier on the pre-processing side; it can be a one line awk command. But I think (2) is cleaner. Typically a repository only knows about I/O and interactions with the persistent layer. It shouldn\u2019t need to know how to extract sentences from docs."),Va=u(),T=o("p"),Fp=i("I will use "),zs=o("a"),Hp=i("SpaCy"),Np=i(" to split docs into sentences. Before using it I need not only to install SpaCy, but to download the model too. "),Ls=o("a"),jp=i("A list of downloadable language models"),Bp=i(" is available on the SpaCy website."),Ka=u(),Rs=o("pre"),Ja=u(),Re=o("p"),Op=i("This is the script I came up with. Nothing fancy and no tests, it\u2019s just a one off script. No CLI frameworks, it simply reads input from sys.argv."),Qa=u(),Ds=o("pre"),Za=u(),De=o("p"),Gp=i("It runs for about 10 mins on an M1 laptop, but gets the job done"),Xa=u(),Ms=o("pre"),Ya=u(),U=o("h3"),V=o("a"),qn=o("span"),Wp=i("Step 2: a repository for German sentences"),$a=u(),Me=o("p"),Up=i("The repository module is responsible for"),st=u(),_=o("ul"),Fn=o("li"),Vp=i("interacting with the persistent storage"),Kp=u(),Hn=o("li"),Jp=i("fetching a \u2018next\u2019 sentence from the doc currently marked as \u2018current\u2019"),Qp=u(),Nn=o("li"),Zp=i("if no doc is marked as \u2018current\u2019, one will be picked randomly"),Xp=u(),jn=o("li"),Yp=i("when the last sentence of a doc is picked, the \u2018current\u2019 marker is cleared"),et=u(),qe=o("p"),$p=i("In my personal projects I like using TDD, so I\u2019ll start with a test"),nt=u(),qs=o("pre"),at=u(),K=o("p"),sl=i("The "),Bn=o("i"),el=i("is_sentence"),nl=i(" is an as yet to coded util function. It can easily be overcomplicated, so I will create tests for it too."),tt=u(),Fs=o("pre"),ot=u(),J=o("p"),al=i("And a few more; the "),Fe=o("a"),tl=i("test file is available on github"),ol=i(". But back to the task at and. Here\u2019s the minimum amount of scaffolding needed to run the tests\u2026"),pt=u(),Hs=o("pre"),lt=u(),He=o("p"),pl=i("\u2026and fail them, as expected - I am returning an empty string after all"),it=u(),Ns=o("pre"),rt=u(),Ne=o("p"),ll=i("Returning a sentence requires a few steps. First the repo must get hold of the data file and open it. As this is a first iteration, I\u2019m going to keep the \u2018getting hold of the data\u2019 part as simple as possible. But a config file in which to store the location of the data file is a must. Even at this early stage, coupling the location of the code and the data is a bad idea."),ct=u(),js=o("pre"),ut=u(),Q=o("p"),il=i("Since the .tsv file could be quite large, I\u2019d rather not load it in memory. Instead the "),On=o("code"),rl=i("__init__"),cl=i(" method will store all the boundaries between lines. When needed, it will use that information to fetch a doc\u2019s sentences. And finally, next_sentence will pop the next sentence from memory. When the repo runs out of sentences, it will fetch a new doc. For now it\u2019s always the same doc that is being fetched. That\u2019s enough to pass the test."),kt=u(),Bs=o("pre"),dt=u(),Z=o("p"),ul=i("A quick smoke test on the REPL shows it works. Calling "),Gn=o("code"),kl=i("t.next_sentence"),dl=i(" a few times shows the repo going through all the sentences, and then starting again when they run out"),ft=u(),Os=o("pre"),ht=u(),je=o("p"),fl=i("There are a lot of improvements to be done - for example the docs file is hard coded, there is no error handling, and so on. But at this stage the aim is to get a working MVP, so I\u2019ll move on to the next step."),mt=u(),X=o("h3"),Y=o("a"),Wn=o("span"),hl=i("Step 3: generating the exercises"),wt=u(),Be=o("p"),ml=i("This module will take a German sentence as input. It will turn it into a data structure with some slots at various stages of guessing."),yt=u(),Gs=o("pre"),_t=u(),$=o("p"),wl=i("How best to represent that? I\u2019ll go for a sequence of tokens. Some will be strings (\u201Cfoo foo\u201D) and some will be guessable slots (\u201Cnach dem/xxxxx dxxx\u201D). Anki has a similar concept to \u201Cguessable slot\u201D, i.e., \u2019"),Ws=o("a"),yl=i("cloze deletions"),_l=i("\u2019. I will use that name."),vt=u(),ss=o("h4"),es=o("a"),Un=o("span"),vl=i("Cloze: an entity to represent a guessable slot within exercise"),bt=u(),Oe=o("p"),bl=i("The cloze will be initialised with the string to be guessed and the obfuscated version. It will keep an internal state: guessed or not guessed. The behaviours it will need to implement are"),gt=u(),v=o("ul"),Us=o("li"),gl=i("return the correct string representation depending on state (method: "),Vn=o("code"),xl=i("__str__"),El=i(")"),Il=u(),Vs=o("li"),Tl=i("return the state as a boolean (property: "),Kn=o("code"),Pl=i("guessed"),Al=i(")"),Sl=u(),Ks=o("li"),Cl=i("accept a guess and change state accordingly; return state (method: "),Jn=o("code"),zl=i("guess(str)"),Ll=i(")"),Rl=u(),Qn=o("li"),Dl=i("being an entity, it has a unique, immutable ID"),xt=u(),Ge=o("p"),Ml=i("That\u2019s it for now - no hints like Anki, no counting wrong attempts, nothing fancy. Just focus on the minimal functionality to get something that works. As usual I\u2019ll start with a test, and watch it fail. \u201Cfake\u201D is a Faker instance which is included in the cookiecutter template which generates the project. I use it all the time."),Et=u(),Js=o("pre"),It=u(),We=o("p"),ql=i("What I am creating is essential an Entity, in DDD speak, and will live in the /models folder. I use pydantic for enforcing validation and some type checking. Pydantic is already installed because it\u2019s a dependency of SpaCy. Sadly SpaCy decided to pin its versions to ^1.7.4, so I am forced to do the same"),Tt=u(),Qs=o("pre"),Pt=u(),ns=o("p"),Fl=i("Never mind. On with the model. First of all I create an "),Zn=o("code"),Hl=i("__init__.py"),Nl=i(" file, since this is a new folder"),At=u(),Zs=o("pre"),St=u(),Ue=o("p"),jl=i("Then the Cloze itself"),Ct=u(),Xs=o("pre"),zt=u(),Ve=o("p"),Bl=i("The test will fail with a more sensible message"),Lt=u(),Ys=o("pre"),Rt=u(),as=o("p"),Ol=i("Getting closer. All I need to pass the text is a "),Xn=o("code"),Gl=i("__str__"),Wl=i(" method"),Dt=u(),$s=o("pre"),Mt=u(),Ke=o("p"),Ul=i("The next requirements are about being able to guess, and the state changing. Here\u2019s the test\u2026"),qt=u(),se=o("pre"),Ft=u(),Je=o("p"),Vl=i("And the code that makes it pass"),Ht=u(),ee=o("pre"),Nt=u(),Qe=o("p"),Kl=i("I can also improve the previous test to ensure the string representation changes to mimic the state. It should still pass."),jt=u(),ne=o("pre"),Bt=u(),Ze=o("p"),Jl=i("And it does. The last requirement is an immutable ID field."),Ot=u(),ae=o("pre"),Gt=u(),ts=o("p"),Ql=i("To make a Pydantic field immutable after initialisation, I create a similarly named private field (in this case, \u201D_id\u201D). It doesn\u2019t "),Yn=o("em"),Zl=i("need"),Xl=i(" to be similarly named, but it would just be confusing if it wasn\u2019t. Then I add a @property with the name I want to expose. But I will not add a setter - therefore there will be no way to overwrite the id"),Wt=u(),te=o("pre"),Ut=u(),Xe=o("p"),Yl=i("This is enough for now - all tests pass. As usual, there\u2019s plenty more I could do. But right now the focus is on getting a working prototype"),Vt=u(),os=o("h5"),ps=o("a"),$n=o("span"),$l=i("Creating a Faker provider for the Cloze"),Kt=u(),P=o("p"),si=i("Actually, that was "),sa=o("em"),ei=i("almost"),ni=i(" all. Another thing I tend to do when creating data structures, is to create Faker providers for them. It makes stubbing them in tests much more easier. I have "),Ye=o("a"),ai=i("a post about creating Faker providers"),ti=i(", so I won\u2019t repeat myself here."),Jt=u(),ls=o("h4"),is=o("a"),ea=o("span"),oi=i("An exercise entity"),Qt=u(),$e=o("p"),pi=i("An Exercise is also an entity; it contains a list of either strings or clozes. Since it is an entity, some of the tests will be the same as Cloze. That\u2019s repetitive and hence error prone. It\u2019s time for the first refactor! I will extract the entity-ness of Cloze into a BaseEntity class. Since I\u2019m refactoring, no need for new tests just yet; the existing ones should do."),Zt=u(),sn=o("p"),li=i("I extract the _id/id handling part to a new class, BaseEntity\u2026"),Xt=u(),oe=o("pre"),Yt=u(),en=o("p"),ii=i("\u2026and make the Cloze inherit from it"),$t=u(),pe=o("pre"),so=u(),rs=o("p"),ri=i("The tests still pass. I move the id test from test_cloze to a new test file, "),na=o("code"),ci=i("test_base_entity.py"),ui=i(", and commit."),eo=u(),nn=o("p"),ki=i("I\u2019m read to start on the Exercise entity. The requirement:"),no=u(),d=o("ul"),aa=o("li"),di=i("it consists of a list of tokens which can be either strings or Clozes"),fi=u(),ta=o("li"),hi=i("there must be at least one of each in the list"),mi=u(),le=o("li"),wi=i("it can provide a guessed flag, which is true if "),oa=o("em"),yi=i("all"),_i=i(" Clozes are guessed, false otherwise"),vi=u(),pa=o("li"),bi=i("it returns the current string representation of the exercise, with each Cloze either obfuscated or not depending on status"),gi=u(),la=o("li"),xi=i("it can receive a guess for a given Cloze, and flip its state accordingly"),Ei=u(),ia=o("li"),Ii=i("it can return a list of all Clozes"),ao=u(),cs=o("p"),Ti=i("So there are quite a lot of requirements. I\u2019m not going to go through all of them here; this entity is not that interesting. It\u2019s similar to Cloze but with a couple more bells and whistles. Just like a Cloze, I built a fake for it. If interested, "),an=o("a"),Pi=i("the code is available in the repo"),Ai=i("."),to=u(),us=o("h4"),ks=o("a"),ra=o("span"),Si=i("An exercises service"),oo=u(),tn=o("p"),Ci=i("OK, I have a data structure for an exercise. But what will I do with it? A service will provide them to the rest of the application. The requirements for the service are:"),po=u(),A=o("ul"),ca=o("li"),zi=i("it can be asked to provide N exercises, where N is one by default"),Li=u(),ua=o("li"),Ri=i("these exercises will be based on raw sentences from the sentence repository"),Di=u(),ka=o("li"),Mi=i("it will not remember any information about the last fetched sentence, but it will rely on the repository for that"),lo=u(),on=o("p"),qi=i("This is just the bare minimum for a working MVP. But it\u2019s totally non-scalable. In a production app I\u2019d either cache or precompute the exercises. But on with the tests. Starting with the simplest step, make sure I get a list back"),io=u(),ie=o("pre"),ro=u(),h=o("p"),Fi=i("This is the minimal code that fails the test "),da=o("em"),Hi=i("properly"),Ni=i(". Note that failing the test properly is part of the process. The point is to avoid tests that never fail, or fail for the wrong reason. Failing with \u201Cmethod not implemented\u201D or similar is not useful. The test should fail with an "),fa=o("em"),ji=i("AssertionError"),Bi=i(", so that I know it\u2019s by design. Not only that, but it should be the "),ha=o("em"),Oi=i("last"),Gi=i(" assertion. Again, it\u2019s all about ensuring failures are controlled, not just coincidental. The worst thing in testing is tests that keep on passing after you have changed the code. "),re=o("a"),Wi=i("Mark Seemann\u2019s \u201CA red-green-refactor checklist \u201D"),Ui=i(" is a good read on this topic"),co=u(),pn=o("p"),Vi=i("The minimal code that breaks the test is"),uo=u(),ce=o("pre"),ko=u(),ln=o("p"),Ki=i("Which indeed fails with my last assertion"),fo=u(),ue=o("pre"),ho=u(),rn=o("p"),Ji=i("And I can easily fix by turning the return value to a []"),mo=u(),ke=o("pre"),wo=u(),cn=o("p"),Qi=i("I tend to avoid mocks when building prototypes. They often end up taking up a lot of development effort. But to trust a service that transforms data, one needs to test it with different data. For that, I need the ability to inject different data repositories into the service. I.e., I need mocks, and dependency injection. I start by writing a test for that."),yo=u(),de=o("pre"),_o=u(),un=o("p"),Zi=i("Which fails, as expected, but can be fixed with"),vo=u(),fe=o("pre"),bo=u(),kn=o("p"),Xi=i("Now finally generating an exercise. I need to pick some German sentences to use in the mock. I do it with the following bash command. It sorts all the lines randomly. Then it passes them to head which picks the top (1). The selected, random line has its tabs separated by a \\n character"),go=u(),he=o("pre"),xo=u(),b=o("p"),Yi=i("I\u2019m going to skip the rest of the TDD journey, fun as it was, and jump straight to the end result. Here\u2019s probably the most important test (the rest are avaialble "),dn=o("a"),$i=i("on github"),sr=i("). It uses the same mock as before, but this time it also returns a sentence (the "),ma=o("code"),er=i("repo.next_sentence =..."),nr=i(" line). The sentence is "),me=o("a"),ar=i("parametrised"),tr=i(", meaning the test will be run once for each sentence."),Eo=u(),we=o("pre"),Io=u(),ds=o("p"),or=i("The sentences are in a separate folder, "),wa=o("code"),pr=i("tests/testsupport"),lr=i(". They are organised as an array. Each item should correspond to a token. So \u201CAuch\u201D would be a string, \u201Cmit\u201D would be a cloze, \u201CKanzlerin Angela Merkel sei Sch\xE4uble \u201D would be a string, etc."),To=u(),ye=o("pre"),Po=u(),m=o("p"),ir=i("The code that makes it happen was quite complex. "),ya=o("code"),rr=i("GuessPrepositionExerciseService"),cr=i(" has a private spacy instance, "),_a=o("code"),ur=i("_nlp"),kr=i(", which it is used to parse sentences on demand. The result of the spacy parsing is a series of token objects, one for each word or punctuation. Organising those spacy word tokens into my own exercise string fragment tokens was the complex part. I moved it to a separate class, "),va=o("code"),dr=i("TokenParser"),fr=i(". A new instance is create whenever a sentence needs parsing. It uses the centralised "),ba=o("code"),hr=i("_nlp"),mr=i(" instance."),Ao=u(),_e=o("pre"),So=u(),fn=o("p"),wr=i(`The TokenParser is inspired by XML Sax parser. Sax parsers go through the tokens (words, or chars, doesn\u2019t matter), keeping a copy. Whenever there is a boundary (the start of a comment, the closing of a tag, etc) it fires an event, and adds the tokens saved so to the event.
Different parts of the code listen for specific events, and run when those happen. In my case I followed the basic idea of looking out for boundary events and firing off a specific function corresponding to each boundary. But I didn\u2019t use events at this stage. The only three boundaries I am interested in are end of a string (I need to add the string to the list of tokens), and end of cloze (I need to create the Cloze and add it to the list of tokens).`),Co=u(),ve=o("pre"),zo=u(),fs=o("h4"),hs=o("a"),ga=o("span"),yr=i("A session service"),Lo=u(),hn=o("p"),_r=i("I am getting close the user interface now. A Session is a service that manages Exercises for a user. Its requirements"),Ro=u(),g=o("ul"),xa=o("li"),vr=i("When asked by user, it fetches n Exercises from the Exercise service (by default N=4) and organises them as a queue"),br=u(),Ea=o("li"),gr=i("When asked by user, it provides the next exercise to the user"),xr=u(),Ia=o("li"),Er=i("It receives the user attempts at guessing and returns either \u2018success\u2019 or \u2018failure\u2019"),Ir=u(),Ta=o("li"),Tr=i("It does a very basic scheduling (like Anki) - exercises need to be solved successfully twice in a row per Session. When the current exercise is solved successfully for the first time, or when it is a failure, it is re-added to the end of the queue. When solved successfully for the second time, it is removed from the queue"),Do=u(),ms=o("h2"),ws=o("a"),Pa=o("span"),Pr=i("A CLI app with blessed. And a state machine"),Mo=u(),w=o("p"),Ar=i("The fastest way to play with the app is to make it a CLI app. For that I could use "),be=o("a"),Sr=i("click"),Cr=i(", or "),ge=o("a"),zr=i("typer"),Lr=i(". Or even just the built-in "),xe=o("a"),Rr=i("input"),Dr=i(" and "),Ee=o("a"),Mr=i("argparse"),qr=i(". But even on the CLI, it needs a minimal interface. It must show the user an exercise, allow them to pick any of the answers, update itself, eventually show which answers where right and which weren\u2019t."),qo=u(),mn=o("p"),Ie=o("a"),Fr=i("https://medium.com/steve-cruz/domain-driven-design-ddd-file-structure-ade7fb26553d"),Fo=u(),wn=o("p"),Te=o("a"),Hr=i("https://blog.jacobsdata.com/2020/03/02/a-clean-domain-driven-design-architectural-template"),this.h()},l(s){E=p(s,"H2",{id:!0});var a=l(E);R=p(a,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var sc=l(R);gn=p(sc,"SPAN",{class:!0}),l(gn).forEach(e),sc.forEach(e),sp=r(a,"What I\u2019m trying to do"),a.forEach(e),Aa=k(s),Pe=p(s,"P",{});var ec=l(Pe);ep=r(ec,"Oh, the never ending quest to master German. I am at the stage where I get it most of it right and can hold conversations. But I still make a lot of small mistakes. I do take language classes every so often to raise the level. But although the improvement is visible, it never fully eradicates those mistakes. I need many more repetitions than the the 10-12 offered by textbooks. In other words, I need Python."),ec.forEach(e),Sa=k(s),I=p(s,"P",{});var yn=l(I);np=r(yn,"Textbooks exercises seem to follow a few patterns. Sentences with words in the wrong order, and missing words seem to be the most popular. It shouldn\u2019t be too hard to replicate those exercises. Better, to "),xn=p(yn,"EM",{});var nc=l(xn);ap=r(nc,"automate"),nc.forEach(e),tp=r(yn," this replication so that I have an endless supply of exercises. I can use "),xs=p(yn,"A",{href:!0,rel:!0});var ac=l(xs);op=r(ac,"the free dataset Timo Block made available on GitHub"),ac.forEach(e),pp=r(yn," as a starting point."),yn.forEach(e),Ca=k(s),D=p(s,"H3",{id:!0});var Nr=l(D);M=p(Nr,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var tc=l(M);En=p(tc,"SPAN",{class:!0}),l(En).forEach(e),tc.forEach(e),lp=r(Nr,"The end result"),Nr.forEach(e),za=k(s),y=p(s,"P",{});var ys=l(y);ip=r(ys,"The end result will have some similarities with flashcards software. Particularly "),Es=p(ys,"A",{href:!0,rel:!0});var oc=l(Es);rp=r(oc,"Anki"),oc.forEach(e),cp=r(ys,", which I use a lot. I will use fall back on Anki concepts when needed, as I have vague plans to integrate some of these exercises with it. But what I am building now is a self contained CLI script. It will pick a German document and feed it to the interface sentence by sentence. The UI will obscure, or partially obscure, parts of the sentences. The task is to guess the obscured words. The first iteration will obscure \u201D"),Is=p(ys,"A",{href:!0,rel:!0});var pc=l(Is);up=r(pc,"adpositions"),pc.forEach(e),kp=r(ys,"\u201D, (or \u201Cprepositions\u201D in traditional grammar). And will partially obscure \u201D"),Ts=p(ys,"A",{href:!0,rel:!0});var lc=l(Ts);dp=r(lc,"determiners"),lc.forEach(e),fp=r(ys,"\u201D (articles and some pronounous and some adjectives). An example should make this clearer"),ys.forEach(e),La=k(s),Ps=p(s,"PRE",{class:!0});var Zk=l(Ps);Zk.forEach(e),Ra=k(s),Ae=p(s,"P",{});var ic=l(Ae);hp=r(ic,"A nice to have would also be to insert trick ones where there shouldn\u2019t be any. For example before years - it\u2019s a common mistake by English speakers to say \u201CIn 1989\u2026\u201D, but in German it\u2019s either \u201C1989\u2026\u201D or \u201CIm Jahr 1989\u2026\u201D"),ic.forEach(e),Da=k(s),q=p(s,"H3",{id:!0});var jr=l(q);F=p(jr,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var rc=l(F);In=p(rc,"SPAN",{class:!0}),l(In).forEach(e),rc.forEach(e),mp=r(jr,"The plan"),jr.forEach(e),Ma=k(s),f=p(s,"OL",{});var S=l(f);Tn=p(S,"LI",{});var cc=l(Tn);wp=r(cc,"The raw dataset is a CSV file with a list of \u201Cdocs\u201D and labels. I don\u2019t need the label, but I need each doc\u2019s sequential \u201Csentences\u201D. That makes the CSV in its current form unsuitable. I will need to preprocess the CSV file"),cc.forEach(e),yp=k(S),Pn=p(S,"LI",{});var uc=l(Pn);_p=r(uc,"A repository will load up the persistent storage (the CSV file). It will fetch a \u2018next\u2019 sentence whenever asked. It may also save updated versions of the CSV file."),uc.forEach(e),vp=k(S),An=p(S,"LI",{});var kc=l(An);bp=r(kc,"An exercise generator will ask the repository for a sentence. It will then use NLP to turn into an exercise."),kc.forEach(e),gp=k(S),Sn=p(S,"LI",{});var dc=l(Sn);xp=r(dc,"A session service will ask the exercise generator for N exercises. It will organise them as a queue, and will provide them to the UI when asked. It will also reorganise the queue depending on the outcome of the exercise."),dc.forEach(e),Ep=k(S),Cn=p(S,"LI",{});var fc=l(Cn);Ip=r(fc,"The UI will ask the session service for the next exercise, display it, and allow the user to guess. It will then communicate success / error to the session service and get the next exercise."),fc.forEach(e),S.forEach(e),qa=k(s),H=p(s,"H2",{id:!0});var Br=l(H);N=p(Br,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var hc=l(N);zn=p(hc,"SPAN",{class:!0}),l(zn).forEach(e),hc.forEach(e),Tp=r(Br,"Step 0: Setting up the python project"),Br.forEach(e),Fa=k(s),As=p(s,"P",{});var Or=l(As);Pp=r(Or,"For that I simply use "),Se=p(Or,"A",{href:!0});var mc=l(Se);Ap=r(mc,"my trusty cookiecutter template"),mc.forEach(e),Or.forEach(e),Ha=k(s),Ss=p(s,"PRE",{class:!0});var Xk=l(Ss);Xk.forEach(e),Na=k(s),Ce=p(s,"P",{});var wc=l(Ce);Sp=r(wc,"And I\u2019m good to go. All I need is to create a folder with the article data"),wc.forEach(e),ja=k(s),Cs=p(s,"PRE",{class:!0});var Yk=l(Cs);Yk.forEach(e),Ba=k(s),j=p(s,"H2",{id:!0});var Gr=l(j);B=p(Gr,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var yc=l(B);Ln=p(yc,"SPAN",{class:!0}),l(Ln).forEach(e),yc.forEach(e),Cp=r(Gr,"Creating a CLI app for prepositions training"),Gr.forEach(e),Oa=k(s),O=p(s,"H3",{id:!0});var Wr=l(O);G=p(Wr,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var _c=l(G);Rn=p(_c,"SPAN",{class:!0}),l(Rn).forEach(e),_c.forEach(e),zp=r(Wr,"Step 1: Preprocessing the documents"),Wr.forEach(e),Ga=k(s),ze=p(s,"P",{});var vc=l(ze);Lp=r(vc,"So the source data is a csv where each row has two fields: a label which I don\u2019t need, and a \u201Cdoc\u201D which I need to split into \u201Csentences\u201D. I can think of two ways of handling that:"),vc.forEach(e),Wa=k(s),W=p(s,"OL",{});var Ho=l(W);Dn=p(Ho,"LI",{});var bc=l(Dn);Rp=r(bc,"remove the labels from the CSV and turn it into a TXT with a doc on each line. The repository will then be responsible for lazily splitting a doc in sentences, and holding them in memory"),bc.forEach(e),Dp=k(Ho),Mn=p(Ho,"LI",{});var gc=l(Mn);Mp=r(gc,"remove the labels and split each row\u2019s single doc field into multiple sentence fields. The repository will not do any splitting"),gc.forEach(e),Ho.forEach(e),Ua=k(s),Le=p(s,"P",{});var xc=l(Le);qp=r(xc,"(1) is easier on the pre-processing side; it can be a one line awk command. But I think (2) is cleaner. Typically a repository only knows about I/O and interactions with the persistent layer. It shouldn\u2019t need to know how to extract sentences from docs."),xc.forEach(e),Va=k(s),T=p(s,"P",{});var _n=l(T);Fp=r(_n,"I will use "),zs=p(_n,"A",{href:!0,rel:!0});var Ec=l(zs);Hp=r(Ec,"SpaCy"),Ec.forEach(e),Np=r(_n," to split docs into sentences. Before using it I need not only to install SpaCy, but to download the model too. "),Ls=p(_n,"A",{href:!0,rel:!0});var Ic=l(Ls);jp=r(Ic,"A list of downloadable language models"),Ic.forEach(e),Bp=r(_n," is available on the SpaCy website."),_n.forEach(e),Ka=k(s),Rs=p(s,"PRE",{class:!0});var $k=l(Rs);$k.forEach(e),Ja=k(s),Re=p(s,"P",{});var Tc=l(Re);Op=r(Tc,"This is the script I came up with. Nothing fancy and no tests, it\u2019s just a one off script. No CLI frameworks, it simply reads input from sys.argv."),Tc.forEach(e),Qa=k(s),Ds=p(s,"PRE",{class:!0});var sd=l(Ds);sd.forEach(e),Za=k(s),De=p(s,"P",{});var Pc=l(De);Gp=r(Pc,"It runs for about 10 mins on an M1 laptop, but gets the job done"),Pc.forEach(e),Xa=k(s),Ms=p(s,"PRE",{class:!0});var ed=l(Ms);ed.forEach(e),Ya=k(s),U=p(s,"H3",{id:!0});var Ur=l(U);V=p(Ur,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var Ac=l(V);qn=p(Ac,"SPAN",{class:!0}),l(qn).forEach(e),Ac.forEach(e),Wp=r(Ur,"Step 2: a repository for German sentences"),Ur.forEach(e),$a=k(s),Me=p(s,"P",{});var Sc=l(Me);Up=r(Sc,"The repository module is responsible for"),Sc.forEach(e),st=k(s),_=p(s,"UL",{});var _s=l(_);Fn=p(_s,"LI",{});var Cc=l(Fn);Vp=r(Cc,"interacting with the persistent storage"),Cc.forEach(e),Kp=k(_s),Hn=p(_s,"LI",{});var zc=l(Hn);Jp=r(zc,"fetching a \u2018next\u2019 sentence from the doc currently marked as \u2018current\u2019"),zc.forEach(e),Qp=k(_s),Nn=p(_s,"LI",{});var Lc=l(Nn);Zp=r(Lc,"if no doc is marked as \u2018current\u2019, one will be picked randomly"),Lc.forEach(e),Xp=k(_s),jn=p(_s,"LI",{});var Rc=l(jn);Yp=r(Rc,"when the last sentence of a doc is picked, the \u2018current\u2019 marker is cleared"),Rc.forEach(e),_s.forEach(e),et=k(s),qe=p(s,"P",{});var Dc=l(qe);$p=r(Dc,"In my personal projects I like using TDD, so I\u2019ll start with a test"),Dc.forEach(e),nt=k(s),qs=p(s,"PRE",{class:!0});var nd=l(qs);nd.forEach(e),at=k(s),K=p(s,"P",{});var No=l(K);sl=r(No,"The "),Bn=p(No,"I",{});var Mc=l(Bn);el=r(Mc,"is_sentence"),Mc.forEach(e),nl=r(No," is an as yet to coded util function. It can easily be overcomplicated, so I will create tests for it too."),No.forEach(e),tt=k(s),Fs=p(s,"PRE",{class:!0});var ad=l(Fs);ad.forEach(e),ot=k(s),J=p(s,"P",{});var jo=l(J);al=r(jo,"And a few more; the "),Fe=p(jo,"A",{href:!0});var qc=l(Fe);tl=r(qc,"test file is available on github"),qc.forEach(e),ol=r(jo,". But back to the task at and. Here\u2019s the minimum amount of scaffolding needed to run the tests\u2026"),jo.forEach(e),pt=k(s),Hs=p(s,"PRE",{class:!0});var td=l(Hs);td.forEach(e),lt=k(s),He=p(s,"P",{});var Fc=l(He);pl=r(Fc,"\u2026and fail them, as expected - I am returning an empty string after all"),Fc.forEach(e),it=k(s),Ns=p(s,"PRE",{class:!0});var od=l(Ns);od.forEach(e),rt=k(s),Ne=p(s,"P",{});var Hc=l(Ne);ll=r(Hc,"Returning a sentence requires a few steps. First the repo must get hold of the data file and open it. As this is a first iteration, I\u2019m going to keep the \u2018getting hold of the data\u2019 part as simple as possible. But a config file in which to store the location of the data file is a must. Even at this early stage, coupling the location of the code and the data is a bad idea."),Hc.forEach(e),ct=k(s),js=p(s,"PRE",{class:!0});var pd=l(js);pd.forEach(e),ut=k(s),Q=p(s,"P",{});var Bo=l(Q);il=r(Bo,"Since the .tsv file could be quite large, I\u2019d rather not load it in memory. Instead the "),On=p(Bo,"CODE",{});var Nc=l(On);rl=r(Nc,"__init__"),Nc.forEach(e),cl=r(Bo," method will store all the boundaries between lines. When needed, it will use that information to fetch a doc\u2019s sentences. And finally, next_sentence will pop the next sentence from memory. When the repo runs out of sentences, it will fetch a new doc. For now it\u2019s always the same doc that is being fetched. That\u2019s enough to pass the test."),Bo.forEach(e),kt=k(s),Bs=p(s,"PRE",{class:!0});var ld=l(Bs);ld.forEach(e),dt=k(s),Z=p(s,"P",{});var Oo=l(Z);ul=r(Oo,"A quick smoke test on the REPL shows it works. Calling "),Gn=p(Oo,"CODE",{});var jc=l(Gn);kl=r(jc,"t.next_sentence"),jc.forEach(e),dl=r(Oo," a few times shows the repo going through all the sentences, and then starting again when they run out"),Oo.forEach(e),ft=k(s),Os=p(s,"PRE",{class:!0});var id=l(Os);id.forEach(e),ht=k(s),je=p(s,"P",{});var Bc=l(je);fl=r(Bc,"There are a lot of improvements to be done - for example the docs file is hard coded, there is no error handling, and so on. But at this stage the aim is to get a working MVP, so I\u2019ll move on to the next step."),Bc.forEach(e),mt=k(s),X=p(s,"H3",{id:!0});var Vr=l(X);Y=p(Vr,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var Oc=l(Y);Wn=p(Oc,"SPAN",{class:!0}),l(Wn).forEach(e),Oc.forEach(e),hl=r(Vr,"Step 3: generating the exercises"),Vr.forEach(e),wt=k(s),Be=p(s,"P",{});var Gc=l(Be);ml=r(Gc,"This module will take a German sentence as input. It will turn it into a data structure with some slots at various stages of guessing."),Gc.forEach(e),yt=k(s),Gs=p(s,"PRE",{class:!0});var rd=l(Gs);rd.forEach(e),_t=k(s),$=p(s,"P",{});var Go=l($);wl=r(Go,"How best to represent that? I\u2019ll go for a sequence of tokens. Some will be strings (\u201Cfoo foo\u201D) and some will be guessable slots (\u201Cnach dem/xxxxx dxxx\u201D). Anki has a similar concept to \u201Cguessable slot\u201D, i.e., \u2019"),Ws=p(Go,"A",{href:!0,rel:!0});var Wc=l(Ws);yl=r(Wc,"cloze deletions"),Wc.forEach(e),_l=r(Go,"\u2019. I will use that name."),Go.forEach(e),vt=k(s),ss=p(s,"H4",{id:!0});var Kr=l(ss);es=p(Kr,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var Uc=l(es);Un=p(Uc,"SPAN",{class:!0}),l(Un).forEach(e),Uc.forEach(e),vl=r(Kr,"Cloze: an entity to represent a guessable slot within exercise"),Kr.forEach(e),bt=k(s),Oe=p(s,"P",{});var Vc=l(Oe);bl=r(Vc,"The cloze will be initialised with the string to be guessed and the obfuscated version. It will keep an internal state: guessed or not guessed. The behaviours it will need to implement are"),Vc.forEach(e),gt=k(s),v=p(s,"UL",{});var vs=l(v);Us=p(vs,"LI",{});var Wo=l(Us);gl=r(Wo,"return the correct string representation depending on state (method: "),Vn=p(Wo,"CODE",{});var Kc=l(Vn);xl=r(Kc,"__str__"),Kc.forEach(e),El=r(Wo,")"),Wo.forEach(e),Il=k(vs),Vs=p(vs,"LI",{});var Uo=l(Vs);Tl=r(Uo,"return the state as a boolean (property: "),Kn=p(Uo,"CODE",{});var Jc=l(Kn);Pl=r(Jc,"guessed"),Jc.forEach(e),Al=r(Uo,")"),Uo.forEach(e),Sl=k(vs),Ks=p(vs,"LI",{});var Vo=l(Ks);Cl=r(Vo,"accept a guess and change state accordingly; return state (method: "),Jn=p(Vo,"CODE",{});var Qc=l(Jn);zl=r(Qc,"guess(str)"),Qc.forEach(e),Ll=r(Vo,")"),Vo.forEach(e),Rl=k(vs),Qn=p(vs,"LI",{});var Zc=l(Qn);Dl=r(Zc,"being an entity, it has a unique, immutable ID"),Zc.forEach(e),vs.forEach(e),xt=k(s),Ge=p(s,"P",{});var Xc=l(Ge);Ml=r(Xc,"That\u2019s it for now - no hints like Anki, no counting wrong attempts, nothing fancy. Just focus on the minimal functionality to get something that works. As usual I\u2019ll start with a test, and watch it fail. \u201Cfake\u201D is a Faker instance which is included in the cookiecutter template which generates the project. I use it all the time."),Xc.forEach(e),Et=k(s),Js=p(s,"PRE",{class:!0});var cd=l(Js);cd.forEach(e),It=k(s),We=p(s,"P",{});var Yc=l(We);ql=r(Yc,"What I am creating is essential an Entity, in DDD speak, and will live in the /models folder. I use pydantic for enforcing validation and some type checking. Pydantic is already installed because it\u2019s a dependency of SpaCy. Sadly SpaCy decided to pin its versions to ^1.7.4, so I am forced to do the same"),Yc.forEach(e),Tt=k(s),Qs=p(s,"PRE",{class:!0});var ud=l(Qs);ud.forEach(e),Pt=k(s),ns=p(s,"P",{});var Ko=l(ns);Fl=r(Ko,"Never mind. On with the model. First of all I create an "),Zn=p(Ko,"CODE",{});var $c=l(Zn);Hl=r($c,"__init__.py"),$c.forEach(e),Nl=r(Ko," file, since this is a new folder"),Ko.forEach(e),At=k(s),Zs=p(s,"PRE",{class:!0});var kd=l(Zs);kd.forEach(e),St=k(s),Ue=p(s,"P",{});var su=l(Ue);jl=r(su,"Then the Cloze itself"),su.forEach(e),Ct=k(s),Xs=p(s,"PRE",{class:!0});var dd=l(Xs);dd.forEach(e),zt=k(s),Ve=p(s,"P",{});var eu=l(Ve);Bl=r(eu,"The test will fail with a more sensible message"),eu.forEach(e),Lt=k(s),Ys=p(s,"PRE",{class:!0});var fd=l(Ys);fd.forEach(e),Rt=k(s),as=p(s,"P",{});var Jo=l(as);Ol=r(Jo,"Getting closer. All I need to pass the text is a "),Xn=p(Jo,"CODE",{});var nu=l(Xn);Gl=r(nu,"__str__"),nu.forEach(e),Wl=r(Jo," method"),Jo.forEach(e),Dt=k(s),$s=p(s,"PRE",{class:!0});var hd=l($s);hd.forEach(e),Mt=k(s),Ke=p(s,"P",{});var au=l(Ke);Ul=r(au,"The next requirements are about being able to guess, and the state changing. Here\u2019s the test\u2026"),au.forEach(e),qt=k(s),se=p(s,"PRE",{class:!0});var md=l(se);md.forEach(e),Ft=k(s),Je=p(s,"P",{});var tu=l(Je);Vl=r(tu,"And the code that makes it pass"),tu.forEach(e),Ht=k(s),ee=p(s,"PRE",{class:!0});var wd=l(ee);wd.forEach(e),Nt=k(s),Qe=p(s,"P",{});var ou=l(Qe);Kl=r(ou,"I can also improve the previous test to ensure the string representation changes to mimic the state. It should still pass."),ou.forEach(e),jt=k(s),ne=p(s,"PRE",{class:!0});var yd=l(ne);yd.forEach(e),Bt=k(s),Ze=p(s,"P",{});var pu=l(Ze);Jl=r(pu,"And it does. The last requirement is an immutable ID field."),pu.forEach(e),Ot=k(s),ae=p(s,"PRE",{class:!0});var _d=l(ae);_d.forEach(e),Gt=k(s),ts=p(s,"P",{});var Qo=l(ts);Ql=r(Qo,"To make a Pydantic field immutable after initialisation, I create a similarly named private field (in this case, \u201D_id\u201D). It doesn\u2019t "),Yn=p(Qo,"EM",{});var lu=l(Yn);Zl=r(lu,"need"),lu.forEach(e),Xl=r(Qo," to be similarly named, but it would just be confusing if it wasn\u2019t. Then I add a @property with the name I want to expose. But I will not add a setter - therefore there will be no way to overwrite the id"),Qo.forEach(e),Wt=k(s),te=p(s,"PRE",{class:!0});var vd=l(te);vd.forEach(e),Ut=k(s),Xe=p(s,"P",{});var iu=l(Xe);Yl=r(iu,"This is enough for now - all tests pass. As usual, there\u2019s plenty more I could do. But right now the focus is on getting a working prototype"),iu.forEach(e),Vt=k(s),os=p(s,"H5",{id:!0});var Jr=l(os);ps=p(Jr,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var ru=l(ps);$n=p(ru,"SPAN",{class:!0}),l($n).forEach(e),ru.forEach(e),$l=r(Jr,"Creating a Faker provider for the Cloze"),Jr.forEach(e),Kt=k(s),P=p(s,"P",{});var vn=l(P);si=r(vn,"Actually, that was "),sa=p(vn,"EM",{});var cu=l(sa);ei=r(cu,"almost"),cu.forEach(e),ni=r(vn," all. Another thing I tend to do when creating data structures, is to create Faker providers for them. It makes stubbing them in tests much more easier. I have "),Ye=p(vn,"A",{href:!0});var uu=l(Ye);ai=r(uu,"a post about creating Faker providers"),uu.forEach(e),ti=r(vn,", so I won\u2019t repeat myself here."),vn.forEach(e),Jt=k(s),ls=p(s,"H4",{id:!0});var Qr=l(ls);is=p(Qr,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var ku=l(is);ea=p(ku,"SPAN",{class:!0}),l(ea).forEach(e),ku.forEach(e),oi=r(Qr,"An exercise entity"),Qr.forEach(e),Qt=k(s),$e=p(s,"P",{});var du=l($e);pi=r(du,"An Exercise is also an entity; it contains a list of either strings or clozes. Since it is an entity, some of the tests will be the same as Cloze. That\u2019s repetitive and hence error prone. It\u2019s time for the first refactor! I will extract the entity-ness of Cloze into a BaseEntity class. Since I\u2019m refactoring, no need for new tests just yet; the existing ones should do."),du.forEach(e),Zt=k(s),sn=p(s,"P",{});var fu=l(sn);li=r(fu,"I extract the _id/id handling part to a new class, BaseEntity\u2026"),fu.forEach(e),Xt=k(s),oe=p(s,"PRE",{class:!0});var bd=l(oe);bd.forEach(e),Yt=k(s),en=p(s,"P",{});var hu=l(en);ii=r(hu,"\u2026and make the Cloze inherit from it"),hu.forEach(e),$t=k(s),pe=p(s,"PRE",{class:!0});var gd=l(pe);gd.forEach(e),so=k(s),rs=p(s,"P",{});var Zo=l(rs);ri=r(Zo,"The tests still pass. I move the id test from test_cloze to a new test file, "),na=p(Zo,"CODE",{});var mu=l(na);ci=r(mu,"test_base_entity.py"),mu.forEach(e),ui=r(Zo,", and commit."),Zo.forEach(e),eo=k(s),nn=p(s,"P",{});var wu=l(nn);ki=r(wu,"I\u2019m read to start on the Exercise entity. The requirement:"),wu.forEach(e),no=k(s),d=p(s,"UL",{});var x=l(d);aa=p(x,"LI",{});var yu=l(aa);di=r(yu,"it consists of a list of tokens which can be either strings or Clozes"),yu.forEach(e),fi=k(x),ta=p(x,"LI",{});var _u=l(ta);hi=r(_u,"there must be at least one of each in the list"),_u.forEach(e),mi=k(x),le=p(x,"LI",{});var Xo=l(le);wi=r(Xo,"it can provide a guessed flag, which is true if "),oa=p(Xo,"EM",{});var vu=l(oa);yi=r(vu,"all"),vu.forEach(e),_i=r(Xo," Clozes are guessed, false otherwise"),Xo.forEach(e),vi=k(x),pa=p(x,"LI",{});var bu=l(pa);bi=r(bu,"it returns the current string representation of the exercise, with each Cloze either obfuscated or not depending on status"),bu.forEach(e),gi=k(x),la=p(x,"LI",{});var gu=l(la);xi=r(gu,"it can receive a guess for a given Cloze, and flip its state accordingly"),gu.forEach(e),Ei=k(x),ia=p(x,"LI",{});var xu=l(ia);Ii=r(xu,"it can return a list of all Clozes"),xu.forEach(e),x.forEach(e),ao=k(s),cs=p(s,"P",{});var Yo=l(cs);Ti=r(Yo,"So there are quite a lot of requirements. I\u2019m not going to go through all of them here; this entity is not that interesting. It\u2019s similar to Cloze but with a couple more bells and whistles. Just like a Cloze, I built a fake for it. If interested, "),an=p(Yo,"A",{href:!0});var Eu=l(an);Pi=r(Eu,"the code is available in the repo"),Eu.forEach(e),Ai=r(Yo,"."),Yo.forEach(e),to=k(s),us=p(s,"H4",{id:!0});var Zr=l(us);ks=p(Zr,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var Iu=l(ks);ra=p(Iu,"SPAN",{class:!0}),l(ra).forEach(e),Iu.forEach(e),Si=r(Zr,"An exercises service"),Zr.forEach(e),oo=k(s),tn=p(s,"P",{});var Tu=l(tn);Ci=r(Tu,"OK, I have a data structure for an exercise. But what will I do with it? A service will provide them to the rest of the application. The requirements for the service are:"),Tu.forEach(e),po=k(s),A=p(s,"UL",{});var bn=l(A);ca=p(bn,"LI",{});var Pu=l(ca);zi=r(Pu,"it can be asked to provide N exercises, where N is one by default"),Pu.forEach(e),Li=k(bn),ua=p(bn,"LI",{});var Au=l(ua);Ri=r(Au,"these exercises will be based on raw sentences from the sentence repository"),Au.forEach(e),Di=k(bn),ka=p(bn,"LI",{});var Su=l(ka);Mi=r(Su,"it will not remember any information about the last fetched sentence, but it will rely on the repository for that"),Su.forEach(e),bn.forEach(e),lo=k(s),on=p(s,"P",{});var Cu=l(on);qi=r(Cu,"This is just the bare minimum for a working MVP. But it\u2019s totally non-scalable. In a production app I\u2019d either cache or precompute the exercises. But on with the tests. Starting with the simplest step, make sure I get a list back"),Cu.forEach(e),io=k(s),ie=p(s,"PRE",{class:!0});var xd=l(ie);xd.forEach(e),ro=k(s),h=p(s,"P",{});var C=l(h);Fi=r(C,"This is the minimal code that fails the test "),da=p(C,"EM",{});var zu=l(da);Hi=r(zu,"properly"),zu.forEach(e),Ni=r(C,". Note that failing the test properly is part of the process. The point is to avoid tests that never fail, or fail for the wrong reason. Failing with \u201Cmethod not implemented\u201D or similar is not useful. The test should fail with an "),fa=p(C,"EM",{});var Lu=l(fa);ji=r(Lu,"AssertionError"),Lu.forEach(e),Bi=r(C,", so that I know it\u2019s by design. Not only that, but it should be the "),ha=p(C,"EM",{});var Ru=l(ha);Oi=r(Ru,"last"),Ru.forEach(e),Gi=r(C," assertion. Again, it\u2019s all about ensuring failures are controlled, not just coincidental. The worst thing in testing is tests that keep on passing after you have changed the code. "),re=p(C,"A",{href:!0,rel:!0});var Du=l(re);Wi=r(Du,"Mark Seemann\u2019s \u201CA red-green-refactor checklist \u201D"),Du.forEach(e),Ui=r(C," is a good read on this topic"),C.forEach(e),co=k(s),pn=p(s,"P",{});var Mu=l(pn);Vi=r(Mu,"The minimal code that breaks the test is"),Mu.forEach(e),uo=k(s),ce=p(s,"PRE",{class:!0});var Ed=l(ce);Ed.forEach(e),ko=k(s),ln=p(s,"P",{});var qu=l(ln);Ki=r(qu,"Which indeed fails with my last assertion"),qu.forEach(e),fo=k(s),ue=p(s,"PRE",{class:!0});var Id=l(ue);Id.forEach(e),ho=k(s),rn=p(s,"P",{});var Fu=l(rn);Ji=r(Fu,"And I can easily fix by turning the return value to a []"),Fu.forEach(e),mo=k(s),ke=p(s,"PRE",{class:!0});var Td=l(ke);Td.forEach(e),wo=k(s),cn=p(s,"P",{});var Hu=l(cn);Qi=r(Hu,"I tend to avoid mocks when building prototypes. They often end up taking up a lot of development effort. But to trust a service that transforms data, one needs to test it with different data. For that, I need the ability to inject different data repositories into the service. I.e., I need mocks, and dependency injection. I start by writing a test for that."),Hu.forEach(e),yo=k(s),de=p(s,"PRE",{class:!0});var Pd=l(de);Pd.forEach(e),_o=k(s),un=p(s,"P",{});var Nu=l(un);Zi=r(Nu,"Which fails, as expected, but can be fixed with"),Nu.forEach(e),vo=k(s),fe=p(s,"PRE",{class:!0});var Ad=l(fe);Ad.forEach(e),bo=k(s),kn=p(s,"P",{});var ju=l(kn);Xi=r(ju,"Now finally generating an exercise. I need to pick some German sentences to use in the mock. I do it with the following bash command. It sorts all the lines randomly. Then it passes them to head which picks the top (1). The selected, random line has its tabs separated by a \\n character"),ju.forEach(e),go=k(s),he=p(s,"PRE",{class:!0});var Sd=l(he);Sd.forEach(e),xo=k(s),b=p(s,"P",{});var bs=l(b);Yi=r(bs,"I\u2019m going to skip the rest of the TDD journey, fun as it was, and jump straight to the end result. Here\u2019s probably the most important test (the rest are avaialble "),dn=p(bs,"A",{href:!0});var Bu=l(dn);$i=r(Bu,"on github"),Bu.forEach(e),sr=r(bs,"). It uses the same mock as before, but this time it also returns a sentence (the "),ma=p(bs,"CODE",{});var Ou=l(ma);er=r(Ou,"repo.next_sentence =..."),Ou.forEach(e),nr=r(bs," line). The sentence is "),me=p(bs,"A",{href:!0,rel:!0});var Gu=l(me);ar=r(Gu,"parametrised"),Gu.forEach(e),tr=r(bs,", meaning the test will be run once for each sentence."),bs.forEach(e),Eo=k(s),we=p(s,"PRE",{class:!0});var Cd=l(we);Cd.forEach(e),Io=k(s),ds=p(s,"P",{});var $o=l(ds);or=r($o,"The sentences are in a separate folder, "),wa=p($o,"CODE",{});var Wu=l(wa);pr=r(Wu,"tests/testsupport"),Wu.forEach(e),lr=r($o,". They are organised as an array. Each item should correspond to a token. So \u201CAuch\u201D would be a string, \u201Cmit\u201D would be a cloze, \u201CKanzlerin Angela Merkel sei Sch\xE4uble \u201D would be a string, etc."),$o.forEach(e),To=k(s),ye=p(s,"PRE",{class:!0});var zd=l(ye);zd.forEach(e),Po=k(s),m=p(s,"P",{});var z=l(m);ir=r(z,"The code that makes it happen was quite complex. "),ya=p(z,"CODE",{});var Uu=l(ya);rr=r(Uu,"GuessPrepositionExerciseService"),Uu.forEach(e),cr=r(z," has a private spacy instance, "),_a=p(z,"CODE",{});var Vu=l(_a);ur=r(Vu,"_nlp"),Vu.forEach(e),kr=r(z,", which it is used to parse sentences on demand. The result of the spacy parsing is a series of token objects, one for each word or punctuation. Organising those spacy word tokens into my own exercise string fragment tokens was the complex part. I moved it to a separate class, "),va=p(z,"CODE",{});var Ku=l(va);dr=r(Ku,"TokenParser"),Ku.forEach(e),fr=r(z,". A new instance is create whenever a sentence needs parsing. It uses the centralised "),ba=p(z,"CODE",{});var Ju=l(ba);hr=r(Ju,"_nlp"),Ju.forEach(e),mr=r(z," instance."),z.forEach(e),Ao=k(s),_e=p(s,"PRE",{class:!0});var Ld=l(_e);Ld.forEach(e),So=k(s),fn=p(s,"P",{});var Qu=l(fn);wr=r(Qu,`The TokenParser is inspired by XML Sax parser. Sax parsers go through the tokens (words, or chars, doesn\u2019t matter), keeping a copy. Whenever there is a boundary (the start of a comment, the closing of a tag, etc) it fires an event, and adds the tokens saved so to the event.
Different parts of the code listen for specific events, and run when those happen. In my case I followed the basic idea of looking out for boundary events and firing off a specific function corresponding to each boundary. But I didn\u2019t use events at this stage. The only three boundaries I am interested in are end of a string (I need to add the string to the list of tokens), and end of cloze (I need to create the Cloze and add it to the list of tokens).`),Qu.forEach(e),Co=k(s),ve=p(s,"PRE",{class:!0});var Rd=l(ve);Rd.forEach(e),zo=k(s),fs=p(s,"H4",{id:!0});var Xr=l(fs);hs=p(Xr,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var Zu=l(hs);ga=p(Zu,"SPAN",{class:!0}),l(ga).forEach(e),Zu.forEach(e),yr=r(Xr,"A session service"),Xr.forEach(e),Lo=k(s),hn=p(s,"P",{});var Xu=l(hn);_r=r(Xu,"I am getting close the user interface now. A Session is a service that manages Exercises for a user. Its requirements"),Xu.forEach(e),Ro=k(s),g=p(s,"UL",{});var gs=l(g);xa=p(gs,"LI",{});var Yu=l(xa);vr=r(Yu,"When asked by user, it fetches n Exercises from the Exercise service (by default N=4) and organises them as a queue"),Yu.forEach(e),br=k(gs),Ea=p(gs,"LI",{});var $u=l(Ea);gr=r($u,"When asked by user, it provides the next exercise to the user"),$u.forEach(e),xr=k(gs),Ia=p(gs,"LI",{});var sk=l(Ia);Er=r(sk,"It receives the user attempts at guessing and returns either \u2018success\u2019 or \u2018failure\u2019"),sk.forEach(e),Ir=k(gs),Ta=p(gs,"LI",{});var ek=l(Ta);Tr=r(ek,"It does a very basic scheduling (like Anki) - exercises need to be solved successfully twice in a row per Session. When the current exercise is solved successfully for the first time, or when it is a failure, it is re-added to the end of the queue. When solved successfully for the second time, it is removed from the queue"),ek.forEach(e),gs.forEach(e),Do=k(s),ms=p(s,"H2",{id:!0});var Yr=l(ms);ws=p(Yr,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var nk=l(ws);Pa=p(nk,"SPAN",{class:!0}),l(Pa).forEach(e),nk.forEach(e),Pr=r(Yr,"A CLI app with blessed. And a state machine"),Yr.forEach(e),Mo=k(s),w=p(s,"P",{});var L=l(w);Ar=r(L,"The fastest way to play with the app is to make it a CLI app. For that I could use "),be=p(L,"A",{href:!0,rel:!0});var ak=l(be);Sr=r(ak,"click"),ak.forEach(e),Cr=r(L,", or "),ge=p(L,"A",{href:!0,rel:!0});var tk=l(ge);zr=r(tk,"typer"),tk.forEach(e),Lr=r(L,". Or even just the built-in "),xe=p(L,"A",{href:!0,rel:!0});var ok=l(xe);Rr=r(ok,"input"),ok.forEach(e),Dr=r(L," and "),Ee=p(L,"A",{href:!0,rel:!0});var pk=l(Ee);Mr=r(pk,"argparse"),pk.forEach(e),qr=r(L,". But even on the CLI, it needs a minimal interface. It must show the user an exercise, allow them to pick any of the answers, update itself, eventually show which answers where right and which weren\u2019t."),L.forEach(e),qo=k(s),mn=p(s,"P",{});var lk=l(mn);Ie=p(lk,"A",{href:!0,rel:!0});var ik=l(Ie);Fr=r(ik,"https://medium.com/steve-cruz/domain-driven-design-ddd-file-structure-ade7fb26553d"),ik.forEach(e),lk.forEach(e),Fo=k(s),wn=p(s,"P",{});var rk=l(wn);Te=p(rk,"A",{href:!0,rel:!0});var ck=l(Te);Hr=r(ck,"https://blog.jacobsdata.com/2020/03/02/a-clean-domain-driven-design-architectural-template"),ck.forEach(e),rk.forEach(e),this.h()},h(){c(gn,"class","icon icon-link"),c(R,"aria-hidden","true"),c(R,"tabindex","-1"),c(R,"href","#what-im-trying-to-do"),c(E,"id","what-im-trying-to-do"),c(xs,"href","https://tblock.github.io/10kGNAD/"),c(xs,"rel","nofollow"),c(En,"class","icon icon-link"),c(M,"aria-hidden","true"),c(M,"tabindex","-1"),c(M,"href","#the-end-result"),c(D,"id","the-end-result"),c(Es,"href","https://apps.ankiweb.net/"),c(Es,"rel","nofollow"),c(Is,"href","https://universaldependencies.org/u/pos/ADP.html"),c(Is,"rel","nofollow"),c(Ts,"href","https://universaldependencies.org/u/pos/DET.html"),c(Ts,"rel","nofollow"),c(Ps,"class","language-txt"),c(In,"class","icon icon-link"),c(F,"aria-hidden","true"),c(F,"tabindex","-1"),c(F,"href","#the-plan"),c(q,"id","the-plan"),c(zn,"class","icon icon-link"),c(N,"aria-hidden","true"),c(N,"tabindex","-1"),c(N,"href","#step-0-setting-up-the-python-project"),c(H,"id","step-0-setting-up-the-python-project"),c(Se,"href","/blog/creating-a-poetry-driven-python-project-template-with-cookiecutter"),c(Ss,"class","language-bash"),c(Cs,"class","language-bash"),c(Ln,"class","icon icon-link"),c(B,"aria-hidden","true"),c(B,"tabindex","-1"),c(B,"href","#creating-a-cli-app-for-prepositions-training"),c(j,"id","creating-a-cli-app-for-prepositions-training"),c(Rn,"class","icon icon-link"),c(G,"aria-hidden","true"),c(G,"tabindex","-1"),c(G,"href","#step-1-preprocessing-the-documents"),c(O,"id","step-1-preprocessing-the-documents"),c(zs,"href","https://spacy.io/"),c(zs,"rel","nofollow"),c(Ls,"href","https://spacy.io/usage/models"),c(Ls,"rel","nofollow"),c(Rs,"class","language-bash"),c(Ds,"class","language-python"),c(Ms,"class","language-bash"),c(qn,"class","icon icon-link"),c(V,"aria-hidden","true"),c(V,"tabindex","-1"),c(V,"href","#step-2-a-repository-for-german-sentences"),c(U,"id","step-2-a-repository-for-german-sentences"),c(qs,"class","language-python"),c(Fs,"class","language-python"),c(Fe,"href","TODO"),c(Hs,"class","language-python"),c(Ns,"class","language-bash"),c(js,"class","language-python"),c(Bs,"class","language-python"),c(Os,"class","language-python"),c(Wn,"class","icon icon-link"),c(Y,"aria-hidden","true"),c(Y,"tabindex","-1"),c(Y,"href","#step-3-generating-the-exercises"),c(X,"id","step-3-generating-the-exercises"),c(Gs,"class","language-txt"),c(Ws,"href","https://docs.ankiweb.net/editing.html#cloze-deletion"),c(Ws,"rel","nofollow"),c(Un,"class","icon icon-link"),c(es,"aria-hidden","true"),c(es,"tabindex","-1"),c(es,"href","#cloze-an-entity-to-represent-a-guessable-slot-within-exercise"),c(ss,"id","cloze-an-entity-to-represent-a-guessable-slot-within-exercise"),c(Js,"class","language-python"),c(Qs,"class","language-bash"),c(Zs,"class","language-python"),c(Xs,"class","language-python"),c(Ys,"class","language-bash"),c($s,"class","language-diff"),c(se,"class","language-python"),c(ee,"class","language-diff"),c(ne,"class","language-diff"),c(ae,"class","language-python"),c(te,"class","language-diff"),c($n,"class","icon icon-link"),c(ps,"aria-hidden","true"),c(ps,"tabindex","-1"),c(ps,"href","#creating-a-faker-provider-for-the-cloze"),c(os,"id","creating-a-faker-provider-for-the-cloze"),c(Ye,"href","/blog/create-fake-dataset-fixtures-testing-with-faker"),c(ea,"class","icon icon-link"),c(is,"aria-hidden","true"),c(is,"tabindex","-1"),c(is,"href","#an-exercise-entity"),c(ls,"id","an-exercise-entity"),c(oe,"class","language-python"),c(pe,"class","language-diff"),c(an,"href","TODO"),c(ra,"class","icon icon-link"),c(ks,"aria-hidden","true"),c(ks,"tabindex","-1"),c(ks,"href","#an-exercises-service"),c(us,"id","an-exercises-service"),c(ie,"class","language-python"),c(re,"href","https://blog.ploeh.dk/2019/10/21/a-red-green-refactor-checklist/"),c(re,"rel","nofollow"),c(ce,"class","language-python"),c(ue,"class","language-diff"),c(ke,"class","language-python"),c(de,"class","language-python"),c(fe,"class","language-diff"),c(he,"class","language-bash"),c(dn,"href","TODO"),c(me,"href","https://docs.pytest.org/en/7.1.x/example/parametrize.html"),c(me,"rel","nofollow"),c(we,"class","language-python"),c(ye,"class","language-python"),c(_e,"class","language-python"),c(ve,"class","language-python"),c(ga,"class","icon icon-link"),c(hs,"aria-hidden","true"),c(hs,"tabindex","-1"),c(hs,"href","#a-session-service"),c(fs,"id","a-session-service"),c(Pa,"class","icon icon-link"),c(ws,"aria-hidden","true"),c(ws,"tabindex","-1"),c(ws,"href","#a-cli-app-with-blessed-and-a-state-machine"),c(ms,"id","a-cli-app-with-blessed-and-a-state-machine"),c(be,"href","https://pypi.org/project/click/"),c(be,"rel","nofollow"),c(ge,"href","https://pypi.org/project/typer/"),c(ge,"rel","nofollow"),c(xe,"href","https://docs.python.org/3/library/functions.html?highlight=input#input"),c(xe,"rel","nofollow"),c(Ee,"href","https://docs.python.org/3/library/argparse.html?highlight=argparse#module-argparse"),c(Ee,"rel","nofollow"),c(Ie,"href","https://medium.com/steve-cruz/domain-driven-design-ddd-file-structure-ade7fb26553d"),c(Ie,"rel","nofollow"),c(Te,"href","https://blog.jacobsdata.com/2020/03/02/a-clean-domain-driven-design-architectural-template"),c(Te,"rel","nofollow")},m(s,a){t(s,E,a),n(E,R),n(R,gn),n(E,sp),t(s,Aa,a),t(s,Pe,a),n(Pe,ep),t(s,Sa,a),t(s,I,a),n(I,np),n(I,xn),n(xn,ap),n(I,tp),n(I,xs),n(xs,op),n(I,pp),t(s,Ca,a),t(s,D,a),n(D,M),n(M,En),n(D,lp),t(s,za,a),t(s,y,a),n(y,ip),n(y,Es),n(Es,rp),n(y,cp),n(y,Is),n(Is,up),n(y,kp),n(y,Ts),n(Ts,dp),n(y,fp),t(s,La,a),t(s,Ps,a),Ps.innerHTML=kk,t(s,Ra,a),t(s,Ae,a),n(Ae,hp),t(s,Da,a),t(s,q,a),n(q,F),n(F,In),n(q,mp),t(s,Ma,a),t(s,f,a),n(f,Tn),n(Tn,wp),n(f,yp),n(f,Pn),n(Pn,_p),n(f,vp),n(f,An),n(An,bp),n(f,gp),n(f,Sn),n(Sn,xp),n(f,Ep),n(f,Cn),n(Cn,Ip),t(s,qa,a),t(s,H,a),n(H,N),n(N,zn),n(H,Tp),t(s,Fa,a),t(s,As,a),n(As,Pp),n(As,Se),n(Se,Ap),t(s,Ha,a),t(s,Ss,a),Ss.innerHTML=dk,t(s,Na,a),t(s,Ce,a),n(Ce,Sp),t(s,ja,a),t(s,Cs,a),Cs.innerHTML=fk,t(s,Ba,a),t(s,j,a),n(j,B),n(B,Ln),n(j,Cp),t(s,Oa,a),t(s,O,a),n(O,G),n(G,Rn),n(O,zp),t(s,Ga,a),t(s,ze,a),n(ze,Lp),t(s,Wa,a),t(s,W,a),n(W,Dn),n(Dn,Rp),n(W,Dp),n(W,Mn),n(Mn,Mp),t(s,Ua,a),t(s,Le,a),n(Le,qp),t(s,Va,a),t(s,T,a),n(T,Fp),n(T,zs),n(zs,Hp),n(T,Np),n(T,Ls),n(Ls,jp),n(T,Bp),t(s,Ka,a),t(s,Rs,a),Rs.innerHTML=hk,t(s,Ja,a),t(s,Re,a),n(Re,Op),t(s,Qa,a),t(s,Ds,a),Ds.innerHTML=mk,t(s,Za,a),t(s,De,a),n(De,Gp),t(s,Xa,a),t(s,Ms,a),Ms.innerHTML=wk,t(s,Ya,a),t(s,U,a),n(U,V),n(V,qn),n(U,Wp),t(s,$a,a),t(s,Me,a),n(Me,Up),t(s,st,a),t(s,_,a),n(_,Fn),n(Fn,Vp),n(_,Kp),n(_,Hn),n(Hn,Jp),n(_,Qp),n(_,Nn),n(Nn,Zp),n(_,Xp),n(_,jn),n(jn,Yp),t(s,et,a),t(s,qe,a),n(qe,$p),t(s,nt,a),t(s,qs,a),qs.innerHTML=yk,t(s,at,a),t(s,K,a),n(K,sl),n(K,Bn),n(Bn,el),n(K,nl),t(s,tt,a),t(s,Fs,a),Fs.innerHTML=_k,t(s,ot,a),t(s,J,a),n(J,al),n(J,Fe),n(Fe,tl),n(J,ol),t(s,pt,a),t(s,Hs,a),Hs.innerHTML=vk,t(s,lt,a),t(s,He,a),n(He,pl),t(s,it,a),t(s,Ns,a),Ns.innerHTML=bk,t(s,rt,a),t(s,Ne,a),n(Ne,ll),t(s,ct,a),t(s,js,a),js.innerHTML=gk,t(s,ut,a),t(s,Q,a),n(Q,il),n(Q,On),n(On,rl),n(Q,cl),t(s,kt,a),t(s,Bs,a),Bs.innerHTML=xk,t(s,dt,a),t(s,Z,a),n(Z,ul),n(Z,Gn),n(Gn,kl),n(Z,dl),t(s,ft,a),t(s,Os,a),Os.innerHTML=Ek,t(s,ht,a),t(s,je,a),n(je,fl),t(s,mt,a),t(s,X,a),n(X,Y),n(Y,Wn),n(X,hl),t(s,wt,a),t(s,Be,a),n(Be,ml),t(s,yt,a),t(s,Gs,a),Gs.innerHTML=Ik,t(s,_t,a),t(s,$,a),n($,wl),n($,Ws),n(Ws,yl),n($,_l),t(s,vt,a),t(s,ss,a),n(ss,es),n(es,Un),n(ss,vl),t(s,bt,a),t(s,Oe,a),n(Oe,bl),t(s,gt,a),t(s,v,a),n(v,Us),n(Us,gl),n(Us,Vn),n(Vn,xl),n(Us,El),n(v,Il),n(v,Vs),n(Vs,Tl),n(Vs,Kn),n(Kn,Pl),n(Vs,Al),n(v,Sl),n(v,Ks),n(Ks,Cl),n(Ks,Jn),n(Jn,zl),n(Ks,Ll),n(v,Rl),n(v,Qn),n(Qn,Dl),t(s,xt,a),t(s,Ge,a),n(Ge,Ml),t(s,Et,a),t(s,Js,a),Js.innerHTML=Tk,t(s,It,a),t(s,We,a),n(We,ql),t(s,Tt,a),t(s,Qs,a),Qs.innerHTML=Pk,t(s,Pt,a),t(s,ns,a),n(ns,Fl),n(ns,Zn),n(Zn,Hl),n(ns,Nl),t(s,At,a),t(s,Zs,a),Zs.innerHTML=Ak,t(s,St,a),t(s,Ue,a),n(Ue,jl),t(s,Ct,a),t(s,Xs,a),Xs.innerHTML=Sk,t(s,zt,a),t(s,Ve,a),n(Ve,Bl),t(s,Lt,a),t(s,Ys,a),Ys.innerHTML=Ck,t(s,Rt,a),t(s,as,a),n(as,Ol),n(as,Xn),n(Xn,Gl),n(as,Wl),t(s,Dt,a),t(s,$s,a),$s.innerHTML=zk,t(s,Mt,a),t(s,Ke,a),n(Ke,Ul),t(s,qt,a),t(s,se,a),se.innerHTML=Lk,t(s,Ft,a),t(s,Je,a),n(Je,Vl),t(s,Ht,a),t(s,ee,a),ee.innerHTML=Rk,t(s,Nt,a),t(s,Qe,a),n(Qe,Kl),t(s,jt,a),t(s,ne,a),ne.innerHTML=Dk,t(s,Bt,a),t(s,Ze,a),n(Ze,Jl),t(s,Ot,a),t(s,ae,a),ae.innerHTML=Mk,t(s,Gt,a),t(s,ts,a),n(ts,Ql),n(ts,Yn),n(Yn,Zl),n(ts,Xl),t(s,Wt,a),t(s,te,a),te.innerHTML=qk,t(s,Ut,a),t(s,Xe,a),n(Xe,Yl),t(s,Vt,a),t(s,os,a),n(os,ps),n(ps,$n),n(os,$l),t(s,Kt,a),t(s,P,a),n(P,si),n(P,sa),n(sa,ei),n(P,ni),n(P,Ye),n(Ye,ai),n(P,ti),t(s,Jt,a),t(s,ls,a),n(ls,is),n(is,ea),n(ls,oi),t(s,Qt,a),t(s,$e,a),n($e,pi),t(s,Zt,a),t(s,sn,a),n(sn,li),t(s,Xt,a),t(s,oe,a),oe.innerHTML=Fk,t(s,Yt,a),t(s,en,a),n(en,ii),t(s,$t,a),t(s,pe,a),pe.innerHTML=Hk,t(s,so,a),t(s,rs,a),n(rs,ri),n(rs,na),n(na,ci),n(rs,ui),t(s,eo,a),t(s,nn,a),n(nn,ki),t(s,no,a),t(s,d,a),n(d,aa),n(aa,di),n(d,fi),n(d,ta),n(ta,hi),n(d,mi),n(d,le),n(le,wi),n(le,oa),n(oa,yi),n(le,_i),n(d,vi),n(d,pa),n(pa,bi),n(d,gi),n(d,la),n(la,xi),n(d,Ei),n(d,ia),n(ia,Ii),t(s,ao,a),t(s,cs,a),n(cs,Ti),n(cs,an),n(an,Pi),n(cs,Ai),t(s,to,a),t(s,us,a),n(us,ks),n(ks,ra),n(us,Si),t(s,oo,a),t(s,tn,a),n(tn,Ci),t(s,po,a),t(s,A,a),n(A,ca),n(ca,zi),n(A,Li),n(A,ua),n(ua,Ri),n(A,Di),n(A,ka),n(ka,Mi),t(s,lo,a),t(s,on,a),n(on,qi),t(s,io,a),t(s,ie,a),ie.innerHTML=Nk,t(s,ro,a),t(s,h,a),n(h,Fi),n(h,da),n(da,Hi),n(h,Ni),n(h,fa),n(fa,ji),n(h,Bi),n(h,ha),n(ha,Oi),n(h,Gi),n(h,re),n(re,Wi),n(h,Ui),t(s,co,a),t(s,pn,a),n(pn,Vi),t(s,uo,a),t(s,ce,a),ce.innerHTML=jk,t(s,ko,a),t(s,ln,a),n(ln,Ki),t(s,fo,a),t(s,ue,a),ue.innerHTML=Bk,t(s,ho,a),t(s,rn,a),n(rn,Ji),t(s,mo,a),t(s,ke,a),ke.innerHTML=Ok,t(s,wo,a),t(s,cn,a),n(cn,Qi),t(s,yo,a),t(s,de,a),de.innerHTML=Gk,t(s,_o,a),t(s,un,a),n(un,Zi),t(s,vo,a),t(s,fe,a),fe.innerHTML=Wk,t(s,bo,a),t(s,kn,a),n(kn,Xi),t(s,go,a),t(s,he,a),he.innerHTML=Uk,t(s,xo,a),t(s,b,a),n(b,Yi),n(b,dn),n(dn,$i),n(b,sr),n(b,ma),n(ma,er),n(b,nr),n(b,me),n(me,ar),n(b,tr),t(s,Eo,a),t(s,we,a),we.innerHTML=Vk,t(s,Io,a),t(s,ds,a),n(ds,or),n(ds,wa),n(wa,pr),n(ds,lr),t(s,To,a),t(s,ye,a),ye.innerHTML=Kk,t(s,Po,a),t(s,m,a),n(m,ir),n(m,ya),n(ya,rr),n(m,cr),n(m,_a),n(_a,ur),n(m,kr),n(m,va),n(va,dr),n(m,fr),n(m,ba),n(ba,hr),n(m,mr),t(s,Ao,a),t(s,_e,a),_e.innerHTML=Jk,t(s,So,a),t(s,fn,a),n(fn,wr),t(s,Co,a),t(s,ve,a),ve.innerHTML=Qk,t(s,zo,a),t(s,fs,a),n(fs,hs),n(hs,ga),n(fs,yr),t(s,Lo,a),t(s,hn,a),n(hn,_r),t(s,Ro,a),t(s,g,a),n(g,xa),n(xa,vr),n(g,br),n(g,Ea),n(Ea,gr),n(g,xr),n(g,Ia),n(Ia,Er),n(g,Ir),n(g,Ta),n(Ta,Tr),t(s,Do,a),t(s,ms,a),n(ms,ws),n(ws,Pa),n(ms,Pr),t(s,Mo,a),t(s,w,a),n(w,Ar),n(w,be),n(be,Sr),n(w,Cr),n(w,ge),n(ge,zr),n(w,Lr),n(w,xe),n(xe,Rr),n(w,Dr),n(w,Ee),n(Ee,Mr),n(w,qr),t(s,qo,a),t(s,mn,a),n(mn,Ie),n(Ie,Fr),t(s,Fo,a),t(s,wn,a),n(wn,Te),n(Te,Hr)},p:$r,i:$r,o:$r,d(s){s&&e(E),s&&e(Aa),s&&e(Pe),s&&e(Sa),s&&e(I),s&&e(Ca),s&&e(D),s&&e(za),s&&e(y),s&&e(La),s&&e(Ps),s&&e(Ra),s&&e(Ae),s&&e(Da),s&&e(q),s&&e(Ma),s&&e(f),s&&e(qa),s&&e(H),s&&e(Fa),s&&e(As),s&&e(Ha),s&&e(Ss),s&&e(Na),s&&e(Ce),s&&e(ja),s&&e(Cs),s&&e(Ba),s&&e(j),s&&e(Oa),s&&e(O),s&&e(Ga),s&&e(ze),s&&e(Wa),s&&e(W),s&&e(Ua),s&&e(Le),s&&e(Va),s&&e(T),s&&e(Ka),s&&e(Rs),s&&e(Ja),s&&e(Re),s&&e(Qa),s&&e(Ds),s&&e(Za),s&&e(De),s&&e(Xa),s&&e(Ms),s&&e(Ya),s&&e(U),s&&e($a),s&&e(Me),s&&e(st),s&&e(_),s&&e(et),s&&e(qe),s&&e(nt),s&&e(qs),s&&e(at),s&&e(K),s&&e(tt),s&&e(Fs),s&&e(ot),s&&e(J),s&&e(pt),s&&e(Hs),s&&e(lt),s&&e(He),s&&e(it),s&&e(Ns),s&&e(rt),s&&e(Ne),s&&e(ct),s&&e(js),s&&e(ut),s&&e(Q),s&&e(kt),s&&e(Bs),s&&e(dt),s&&e(Z),s&&e(ft),s&&e(Os),s&&e(ht),s&&e(je),s&&e(mt),s&&e(X),s&&e(wt),s&&e(Be),s&&e(yt),s&&e(Gs),s&&e(_t),s&&e($),s&&e(vt),s&&e(ss),s&&e(bt),s&&e(Oe),s&&e(gt),s&&e(v),s&&e(xt),s&&e(Ge),s&&e(Et),s&&e(Js),s&&e(It),s&&e(We),s&&e(Tt),s&&e(Qs),s&&e(Pt),s&&e(ns),s&&e(At),s&&e(Zs),s&&e(St),s&&e(Ue),s&&e(Ct),s&&e(Xs),s&&e(zt),s&&e(Ve),s&&e(Lt),s&&e(Ys),s&&e(Rt),s&&e(as),s&&e(Dt),s&&e($s),s&&e(Mt),s&&e(Ke),s&&e(qt),s&&e(se),s&&e(Ft),s&&e(Je),s&&e(Ht),s&&e(ee),s&&e(Nt),s&&e(Qe),s&&e(jt),s&&e(ne),s&&e(Bt),s&&e(Ze),s&&e(Ot),s&&e(ae),s&&e(Gt),s&&e(ts),s&&e(Wt),s&&e(te),s&&e(Ut),s&&e(Xe),s&&e(Vt),s&&e(os),s&&e(Kt),s&&e(P),s&&e(Jt),s&&e(ls),s&&e(Qt),s&&e($e),s&&e(Zt),s&&e(sn),s&&e(Xt),s&&e(oe),s&&e(Yt),s&&e(en),s&&e($t),s&&e(pe),s&&e(so),s&&e(rs),s&&e(eo),s&&e(nn),s&&e(no),s&&e(d),s&&e(ao),s&&e(cs),s&&e(to),s&&e(us),s&&e(oo),s&&e(tn),s&&e(po),s&&e(A),s&&e(lo),s&&e(on),s&&e(io),s&&e(ie),s&&e(ro),s&&e(h),s&&e(co),s&&e(pn),s&&e(uo),s&&e(ce),s&&e(ko),s&&e(ln),s&&e(fo),s&&e(ue),s&&e(ho),s&&e(rn),s&&e(mo),s&&e(ke),s&&e(wo),s&&e(cn),s&&e(yo),s&&e(de),s&&e(_o),s&&e(un),s&&e(vo),s&&e(fe),s&&e(bo),s&&e(kn),s&&e(go),s&&e(he),s&&e(xo),s&&e(b),s&&e(Eo),s&&e(we),s&&e(Io),s&&e(ds),s&&e(To),s&&e(ye),s&&e(Po),s&&e(m),s&&e(Ao),s&&e(_e),s&&e(So),s&&e(fn),s&&e(Co),s&&e(ve),s&&e(zo),s&&e(fs),s&&e(Lo),s&&e(hn),s&&e(Ro),s&&e(g),s&&e(Do),s&&e(ms),s&&e(Mo),s&&e(w),s&&e(qo),s&&e(mn),s&&e(Fo),s&&e(wn)}}}const Nd={excerpt:"Textbooks are good for learning grammar, but their exercises tend to be too limited. Doing 10 or 12 exercises once is a good start but does not come anywhere near what I need. Which is hundreds of exercises over the course of days, weeks even.",date:"2022-09-26T00:00:00.000Z",draft:!0,archived:!1,title:"A simple Python app for German grammar exercises, with SpaCy",tags:["german","python","spacy"]};class jd extends Dd{constructor(E){super(),Md(this,E,null,Fd,qd,{})}}export{jd as default,Nd as metadata};
