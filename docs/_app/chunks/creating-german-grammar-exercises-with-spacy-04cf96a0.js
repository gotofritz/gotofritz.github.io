import{S as Vc,i as Wc,s as Jc,e as o,t as i,k as u,c as p,a as l,d as e,h as r,m as d,b as c,g as a,I as n,E as ni}from"./index-6e518972.js";function Qc(jr){let v,A,He,Fa,Hn,$s,Ba,qn,g,Ha,qe,qa,Ma,us,Na,Ua,Mn,S,P,Me,Ga,Nn,w,ja,ds,Va,Wa,ks,Ja,Qa,fs,Za,Ya,Un,hs,Vr=`<code class="language-txt">foo foo [nach dem] bar bar => foo foo [xxxx dxxx] bar bar
foo foo [in diesem]  bar bar => foo foo [xxxxx diesxxx] bar bar
foo foo [mit einem] bar bar => foo foo [xxxx einxxx] bar bar</code>`,Gn,Ks,Xa,jn,C,z,Ne,$a,Vn,m,Ue,Ka,so,Ge,eo,no,je,to,ao,Ve,oo,po,We,lo,Wn,D,L,Je,io,Jn,ms,ro,se,co,Qn,ws,Wr=`<code class="language-bash"><span class="token operator">></span> cookiecutter gh:gotofritz/cookiecutter-gotofritz-poetry
You've downloaded /Users/fritz/.cookiecutters/cookiecutter-gotofritz-poetry before. Is it okay to delete and re-download it? <span class="token punctuation">[</span>yes<span class="token punctuation">]</span>:
project_name <span class="token punctuation">[</span>new-project<span class="token punctuation">]</span>: german-learning
package_name <span class="token punctuation">[</span>germanlearning<span class="token punctuation">]</span>:
verbose_project_name <span class="token punctuation">[</span>My Awesome Project<span class="token punctuation">]</span>: German Language Drills
full_name <span class="token punctuation">[</span>Your Name<span class="token punctuation">]</span>: gotofritz
github_username <span class="token punctuation">[</span>github_username<span class="token punctuation">]</span>: gotofritz
mastodon_handle <span class="token punctuation">[</span>@your_name@mastodon.social<span class="token punctuation">]</span>: @gotofritz@mastodon.social
mastodon_url <span class="token punctuation">[</span>https://mastodon.social/@your_name<span class="token punctuation">]</span>: https://mastodon.social/@gotofritz
project_description <span class="token punctuation">[</span>this is a project<span class="token punctuation">]</span>: Some basic German language drills
python_version <span class="token punctuation">[</span><span class="token number">3.10</span>.4<span class="token punctuation">]</span>: <span class="token number">3.10</span>.6
<span class="token number">3.10</span>.6
Updating dependencies
Resolving dependencies<span class="token punctuation">..</span>. <span class="token punctuation">(</span><span class="token number">0</span>.4s<span class="token punctuation">)</span>
<span class="token punctuation">..</span>.</code>`,Zn,ee,uo,Yn,_s,Jr=`<code class="language-bash"><span class="token operator">></span> <span class="token function">mkdir</span> data
<span class="token operator">></span> <span class="token function">cp</span> ~/Downloads/articles.csv data/
<span class="token operator">></span> tree <span class="token builtin class-name">.</span>
<span class="token builtin class-name">.</span>
\u251C\u2500\u2500 CHANGELOG.md
\u251C\u2500\u2500 LICENSE.md
\u251C\u2500\u2500 Makefile
\u251C\u2500\u2500 README.md
\u251C\u2500\u2500 data
\u2502   \u2514\u2500\u2500 articles.csv
\u251C\u2500\u2500 <span class="token function">mkdir</span>
\u251C\u2500\u2500 poetry.lock
\u251C\u2500\u2500 pyproject.toml
\u251C\u2500\u2500 src
\u2502   \u2514\u2500\u2500 germanlearning
\u2502       \u251C\u2500\u2500 __init__.py
\u2502       \u2514\u2500\u2500 __pycache__
\u2514\u2500\u2500 tests
    \u251C\u2500\u2500 __init__.py
    \u251C\u2500\u2500 __pycache__
    \u251C\u2500\u2500 conftest.py
    \u2514\u2500\u2500 test_setup.py</code>`,Xn,R,O,Qe,ko,$n,F,B,Ze,fo,Kn,ne,ho,st,H,Ye,mo,wo,Xe,_o,et,te,yo,nt,x,bo,ys,vo,go,bs,xo,Eo,tt,vs,Qr=`<code class="language-bash"><span class="token operator">></span> poetry <span class="token function">add</span> spacy
Using version ^3.4.1 <span class="token keyword">for</span> spacy
<span class="token punctuation">..</span>.
<span class="token operator">></span> python -m spacy download de_core_news_sm
Collecting de-core-news-sm<span class="token operator">==</span><span class="token number">3.4</span>.0
<span class="token punctuation">..</span>.</code>`,at,ae,Io,ot,gs,Zr=`<code class="language-python"><span class="token comment"># scripts/convert_raw_csv_to_something_usable.py</span>
<span class="token triple-quoted-string string">"""Converts a raw document with a full article per line to one split into sentences

Usage: python scripts/convert_raw_csv_to_something_usable.py data/articles.csv data/docs.tsv
"""</span>

<span class="token keyword">import</span> re
<span class="token keyword">import</span> sys
<span class="token keyword">from</span> pathlib <span class="token keyword">import</span> Path

<span class="token keyword">import</span> spacy

MIN_WORDS_IN_SENTENCE <span class="token operator">=</span> <span class="token number">3</span>


<span class="token keyword">def</span> <span class="token function">die</span><span class="token punctuation">(</span>msg<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""print an error message and exit"""</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>msg<span class="token punctuation">)</span>
    exit<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>


<span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>sys<span class="token punctuation">.</span>argv<span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token number">3</span><span class="token punctuation">:</span>
    die<span class="token punctuation">(</span><span class="token string">"Expecting two arguments: path to input file and path to output file"</span><span class="token punctuation">)</span>

input_path <span class="token operator">=</span> Path<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>argv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">if</span> <span class="token keyword">not</span> input_path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    die<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Path doesn't exist </span><span class="token interpolation"><span class="token punctuation">&#123;</span>input_path<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>

output_path <span class="token operator">=</span> Path<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>argv<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">if</span> <span class="token keyword">not</span> output_path<span class="token punctuation">.</span>parent<span class="token punctuation">.</span>exists<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    die<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Path doesn't exist </span><span class="token interpolation"><span class="token punctuation">&#123;</span>output_path<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
<span class="token keyword">if</span> output_path<span class="token punctuation">.</span>suffix <span class="token operator">!=</span> <span class="token string">".tsv"</span><span class="token punctuation">:</span>
    die<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Path should be a tsv file </span><span class="token interpolation"><span class="token punctuation">&#123;</span>output_path<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>

<span class="token comment"># loads the fast model for German</span>
nlp <span class="token operator">=</span> spacy<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"de_core_news_sm"</span><span class="token punctuation">)</span>

<span class="token comment"># ARTICLES_FILE is a CSV with two semicolon separated fields; I only care about the second.</span>
<span class="token comment"># Also remove any tabs, as they will interfere with</span>
noise_remover <span class="token operator">=</span> re<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span><span class="token string">r"^.+?;|&#92;t+"</span><span class="token punctuation">)</span>
<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>input_path<span class="token punctuation">,</span> <span class="token string">"r"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> csvfile<span class="token punctuation">:</span>
    docs <span class="token operator">=</span> <span class="token punctuation">[</span>noise_remover<span class="token punctuation">.</span>sub<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">,</span> line<span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> line <span class="token keyword">in</span> csvfile<span class="token punctuation">]</span>

output_lines <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> doc <span class="token keyword">in</span> docs<span class="token punctuation">:</span>
    <span class="token comment"># nlp(doc).sent splits a doc into sentences</span>
    sents <span class="token operator">=</span> <span class="token punctuation">[</span>sent<span class="token punctuation">.</span>text <span class="token keyword">for</span> sent <span class="token keyword">in</span> nlp<span class="token punctuation">(</span>doc<span class="token punctuation">)</span><span class="token punctuation">.</span>sents <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>sent<span class="token punctuation">)</span> <span class="token operator">>=</span> MIN_WORDS_IN_SENTENCE<span class="token punctuation">]</span>
    output_lines<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">"&#92;t"</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>sents<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"&#92;n"</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>output_lines<span class="token punctuation">)</span>
<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>output_path<span class="token punctuation">,</span> <span class="token string">"w"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> csvfile<span class="token punctuation">:</span>
    csvfile<span class="token punctuation">.</span>writelines<span class="token punctuation">(</span>output_lines<span class="token punctuation">)</span></code>`,pt,oe,To,lt,xs,Yr=`<code class="language-bash"><span class="token operator">></span> python scripts/convert_raw_csv_to_something_usable.py data/articles.csv data/docs.tsv

<span class="token comment"># look at first line of source</span>
\u276F <span class="token function">head</span> -n1 data/articles.csv  <span class="token operator">|</span>  <span class="token function">fold</span> -w <span class="token number">64</span>
Etat<span class="token punctuation">;</span>Die ARD-Tochter Degeto hat sich verpflichtet, ab August ein
er Quotenregelung zu folgen, die f\xFCr die Gleichstellung von Regi
sseurinnen sorgen soll. In mindestens <span class="token number">20</span> Prozent der Filme, die
die ARD-Tochter Degeto produziert oder mitfinanziert, sollen ab
Mitte August Frauen Regie f\xFChren. Degeto-Chefin Christine Strobl
 folgt mit dieser Selbstverpflichtung der Forderung von Pro Quot
e Regie. Die Vereinigung von Regisseurinnen hatte im vergangenen
 Jahr eine Quotenregelung gefordert, um den weiblichen Filmschaf
fenden mehr Geh\xF6r und \xF6konomische Gleichstellung zu verschaffen.
 Pro Quote Regie kritisiert, dass, w\xE4hrend rund <span class="token number">50</span> Prozent der R
egie-Studierenden weiblich seien, der Anteil der Regisseurinnen
bei Fernsehfilmen nur bei <span class="token number">13</span> bis <span class="token number">15</span> Prozent liege. In \xD6sterreich
 sieht die Situation \xE4hnlich aus, auch hier wird von unterschied
lichen Seiten Handlungsbedarf angemahnt. Aber wie soll dieser au
ssehen? Ist die Einf\xFChrung der Quotenregelung auch f\xFCr die \xF6ster
reichische Film- und Fernsehlandschaft sinnvoll? Diskutieren Sie
 im Forum.

<span class="token comment"># Try to see whether it has split them into fields</span>
<span class="token operator">></span> <span class="token function">head</span> -n1 data/docs.tsv  <span class="token operator">|</span>  <span class="token function">awk</span> -F<span class="token punctuation"></span>t <span class="token string">'&#123; print $2 &#125;'</span> <span class="token operator">|</span> <span class="token function">fold</span> -w <span class="token number">64</span>
In mindestens <span class="token number">20</span> Prozent der Filme, die die ARD-Tochter Degeto p
roduziert oder mitfinanziert, sollen ab Mitte August Frauen Regi
e f\xFChren.</code>`,it,q,M,$e,Ao,rt,pe,So,ct,_,Ke,Po,Co,sn,zo,Do,en,Lo,Ro,nn,Oo,ut,le,Fo,dt,Es,Xr=`<code class="language-python"><span class="token comment"># tests/repositories/tsv_sentence_repository.py</span>
<span class="token keyword">from</span> germanlearning<span class="token punctuation">.</span>repositories<span class="token punctuation">.</span>tsv_sentence_repository <span class="token keyword">import</span> TsvSentenceRepository
<span class="token keyword">from</span> tests<span class="token punctuation">.</span>conftest <span class="token keyword">import</span> is_sentence

<span class="token keyword">def</span> <span class="token function">test_first_sentence_from_random_article</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Happy path for get_sentence"""</span>
    sut <span class="token operator">=</span> TsvSentenceRepository<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">assert</span> is_sentence<span class="token punctuation">(</span>sut<span class="token punctuation">.</span>next_sentence<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code>`,kt,N,Bo,tn,Ho,qo,ft,Is,$r=`<code class="language-python"><span class="token comment"># tests/test_utils.py</span>
<span class="token keyword">from</span> tests<span class="token punctuation">.</span>conftest <span class="token keyword">import</span> is_sentence

are_sentences <span class="token operator">=</span> <span class="token builtin">frozenset</span><span class="token punctuation">(</span><span class="token punctuation">[</span>
        <span class="token string">"By default, the Faker generates the data in English."</span><span class="token punctuation">,</span>
        <span class="token string">"What if you - want localized data?"</span><span class="token punctuation">,</span>
        <span class="token string">"There are two: different; types of provides!"</span><span class="token punctuation">,</span>
    <span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">test_is_sentence_ends_with_punctuation</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""True if it ends with ?.! and false otherwise"""</span>
    <span class="token keyword">assert</span> <span class="token builtin">all</span><span class="token punctuation">(</span>is_sentence<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span> <span class="token keyword">for</span> sentence <span class="token keyword">in</span> are_sentences<span class="token punctuation">)</span>

    should_not_pass <span class="token operator">=</span> <span class="token punctuation">[</span>sentence<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token keyword">for</span> sentence <span class="token keyword">in</span> are_sentences<span class="token punctuation">]</span>
    <span class="token keyword">assert</span> <span class="token keyword">not</span> <span class="token builtin">any</span><span class="token punctuation">(</span>is_sentence<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span> <span class="token keyword">for</span> sentence <span class="token keyword">in</span> should_not_pass<span class="token punctuation">)</span></code>`,ht,U,Mo,ie,No,Uo,mt,Ts,Kr=`<code class="language-python"><span class="token keyword">class</span> <span class="token class-name">TsvSentenceRepository</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Simple TSV based sentence repository

    Reads sentences from disk storage and allows a consumer to request
    the next one

    next_sentence: returns the next unread sentence from current doc

    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Read document source and stores all line boundaries"""</span>

    <span class="token keyword">def</span> <span class="token function">next_sentence</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">str</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Return a sentence from the data store"""</span>
        <span class="token keyword">return</span> <span class="token string">""</span></code>`,wt,re,Go,_t,As,sc=`<code class="language-bash"><span class="token operator">></span>       assert is_sentence<span class="token punctuation">(</span>sut.get_sentence<span class="token punctuation">(</span><span class="token punctuation">))</span>
E       AssertionError: assert False
E        +  where False <span class="token operator">=</span> is_sentence<span class="token punctuation">(</span><span class="token string">''</span><span class="token punctuation">)</span></code>`,yt,ce,jo,bt,Ss,ec=`<code class="language-python"><span class="token comment"># src/germanlearning/config.py</span>

<span class="token keyword">from</span> pathlib <span class="token keyword">import</span> Path

ROOT_DIR <span class="token operator">=</span> Path<span class="token punctuation">.</span>cwd<span class="token punctuation">(</span><span class="token punctuation">)</span>
DATA_DIR <span class="token operator">=</span> ROOT_DIR <span class="token operator">/</span> <span class="token string">"data"</span>
DOCS_FILE <span class="token operator">=</span> DATA_DIR <span class="token operator">/</span> <span class="token string">"docs.tsv"</span></code>`,vt,G,Vo,an,Wo,Jo,gt,Ps,nc=`<code class="language-python"><span class="token keyword">from</span> typing <span class="token keyword">import</span> List

<span class="token keyword">from</span> germanlearning<span class="token punctuation">.</span>config <span class="token keyword">import</span> DOCS_FILE

SENTENCES_SEPARATOR <span class="token operator">=</span> <span class="token string">"&#92;t"</span>


<span class="token keyword">class</span> <span class="token class-name">TsvSentenceRepository</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Simple TSV based sentence repository

    Reads sentences from disk storage and allows a consumer to request
    the next one

    fetch_doc: reads a doc's list of sentences from storage
    next_sentence: returns the next unread sentence from current doc

    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Read document source and stores all line boundaries"""</span>
        self<span class="token punctuation">.</span>doc_boundaries<span class="token punctuation">:</span> List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>current<span class="token punctuation">:</span> List<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

        <span class="token comment"># SIM115: file kept open on purpose to minimise I/O activity</span>
        self<span class="token punctuation">.</span>docs <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span>DOCS_FILE<span class="token punctuation">,</span> <span class="token string">"rt"</span><span class="token punctuation">)</span>  <span class="token comment"># noqa SIM115</span>
        <span class="token keyword">while</span> self<span class="token punctuation">.</span>docs<span class="token punctuation">.</span>readline<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>doc_boundaries<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>docs<span class="token punctuation">.</span>tell<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">next_sentence</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">str</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Return a sentence from the data store"""</span>
        self<span class="token punctuation">.</span>_ensure_doc<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>current<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">fetch_doc</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> List<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Use the data boundaries to find a list of sentences for a given line"""</span>
        self<span class="token punctuation">.</span>docs<span class="token punctuation">.</span>seek<span class="token punctuation">(</span>self<span class="token punctuation">.</span>doc_boundaries<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token builtin">str</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>docs<span class="token punctuation">.</span>readline<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span>SENTENCES_SEPARATOR<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_ensure_doc</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Ensure there is a current doc"""</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>current<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>current <span class="token operator">=</span> self<span class="token punctuation">.</span>fetch_doc<span class="token punctuation">(</span><span class="token punctuation">)</span></code>`,xt,j,Qo,on,Zo,Yo,Et,Cs,tc=`<code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> germanlearning<span class="token punctuation">.</span>repositories<span class="token punctuation">.</span>tsv_sentence_repository <span class="token keyword">import</span> TsvSentenceRepository
<span class="token operator">>></span><span class="token operator">></span> t <span class="token operator">=</span> TsvSentenceRepository<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> t<span class="token punctuation">.</span>next_sentence<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token string">'Die ARD-Tochter Degeto hat sich verpflichtet, ab August einer Quotenregelung zu folgen, die f\xFCr die Gleichstellung von Regisseurinnen sorgen soll.'</span>
<span class="token operator">>></span><span class="token operator">></span> t<span class="token punctuation">.</span>next_sentence<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token string">'In mindestens 20 Prozent der Filme, die die ARD-Tochter Degeto produziert oder mitfinanziert, sollen ab Mitte August Frauen Regie f\xFChren.'</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token operator">>></span><span class="token operator">></span> t<span class="token punctuation">.</span>next_sentence<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token string">'Die ARD-Tochter Degeto hat sich verpflichtet, ab August einer Quotenregelung zu folgen, die f\xFCr die Gleichstellung von Regisseurinnen sorgen soll.'</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span></code>`,It,ue,Xo,Tt,V,W,pn,$o,At,de,Ko,St,zs,ac=`<code class="language-txt">foo foo nach  dem  bar bar f\xFCr   einen pop pop
foo foo xxxxx dxxx bar bar xxxxx einxx pop pop</code>`,Pt,J,sp,Ds,ep,np,Ct,Q,Z,ln,tp,zt,ke,ap,Dt,y,Ls,op,rn,pp,lp,ip,Rs,rp,cn,cp,up,dp,Os,kp,un,fp,hp,mp,dn,wp,Lt,fe,_p,Rt,Fs,oc=`<code class="language-python"><span class="token comment"># tests/models/test_cloze_entity.py</span>
<span class="token keyword">from</span> germanlearning<span class="token punctuation">.</span>models <span class="token keyword">import</span> Cloze

<span class="token keyword">def</span> <span class="token function">test_new_cloze_returns_obfuscated</span><span class="token punctuation">(</span>fake<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""When a cloze is created, it will return the obfuscated text in string context"""</span>
    text <span class="token operator">=</span> fake<span class="token punctuation">.</span>pystr<span class="token punctuation">(</span><span class="token punctuation">)</span>
    obfuscated <span class="token operator">=</span> fake<span class="token punctuation">.</span>pystr<span class="token punctuation">(</span><span class="token punctuation">)</span>
    sut <span class="token operator">=</span> Cloze<span class="token punctuation">(</span>text<span class="token operator">=</span>text<span class="token punctuation">,</span> obfuscated<span class="token operator">=</span>obfuscated<span class="token punctuation">)</span>
    <span class="token keyword">assert</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>sut<span class="token punctuation">&#125;</span></span><span class="token string"> 123"</span></span> <span class="token operator">==</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>obfuscated<span class="token punctuation">&#125;</span></span><span class="token string"> 123"</span></span></code>`,Ot,he,yp,Ft,Bs,pc=`<code class="language-bash"><span class="token operator">></span> poetry <span class="token function">add</span> pydantic@^1.7
Updating dependencies
Resolving dependencies<span class="token punctuation">..</span>. <span class="token punctuation">(</span><span class="token number">0</span>.3s<span class="token punctuation">)</span>
<span class="token punctuation">..</span>.</code>`,Bt,Y,bp,kn,vp,gp,Ht,Hs,lc=`<code class="language-python"><span class="token comment"># src/models/__init__.py</span>
<span class="token keyword">from</span> <span class="token punctuation">.</span>cloze_entity <span class="token keyword">import</span> Cloze</code>`,qt,me,xp,Mt,qs,ic=`<code class="language-python"><span class="token comment"># src/models/cloze_entity.py</span>
<span class="token keyword">from</span> pydantic <span class="token keyword">import</span> BaseModel<span class="token punctuation">,</span> Field


<span class="token keyword">class</span> <span class="token class-name">Cloze</span><span class="token punctuation">(</span>BaseModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    text<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> Field<span class="token punctuation">(</span>description<span class="token operator">=</span><span class="token string">"The text to be guessed"</span><span class="token punctuation">)</span>
    obfuscated<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> Field<span class="token punctuation">(</span>description<span class="token operator">=</span><span class="token string">"Shows the text as"</span><span class="token punctuation">)</span></code>`,Nt,we,Ep,Ut,Ms,rc=`<code class="language-bash">\u276F <span class="token function">make</span> <span class="token builtin class-name">test</span>
poetry run pytest tests
<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span> <span class="token builtin class-name">test</span> session starts <span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span>
<span class="token punctuation">..</span>.
<span class="token operator">></span>       assert sut <span class="token operator">==</span> obfuscated
E       AssertionError: assert Cloze<span class="token punctuation">(</span>text<span class="token operator">=</span><span class="token string">'FdscvPGgwNGEUxZCvWnj'</span>, <span class="token assign-left variable">obfuscated</span><span class="token operator">=</span><span class="token string">'ZGErTPzfeNSnbSFvvtPR'</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">'ZGErTPzfeNSnbSFvvtPR'</span></code>`,Gt,X,Ip,fn,Tp,Ap,jt,Ns,cc=`<code class="language-diff">class Cloze(BaseModel):
<span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   text: str = Field(description="The text to be guessed")
</span><span class="token prefix unchanged"> </span><span class="token line">   obfuscated: str = Field(description="Shows the text as")
</span></span>
<span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    def __str__(self) -> str:
</span><span class="token prefix inserted">+</span><span class="token line">        """In string context it should return the string that reflects its state"""
</span><span class="token prefix inserted">+</span><span class="token line">        return self.text if self.guessed is True else self.obfuscated</span></span></code>`,Vt,_e,Sp,Wt,Us,uc=`<code class="language-python"><span class="token comment"># tests/models/test_cloze_entity.py</span>

<span class="token keyword">def</span> <span class="token function">test_cloze_guess</span><span class="token punctuation">(</span>fake<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""When you guess the state can change"""</span>
    text <span class="token operator">=</span> fake<span class="token punctuation">.</span>pystr<span class="token punctuation">(</span>count<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">)</span>
    obfuscated <span class="token operator">=</span> fake<span class="token punctuation">.</span>pystr<span class="token punctuation">(</span><span class="token punctuation">)</span>
    sut <span class="token operator">=</span> Cloze<span class="token punctuation">(</span>text<span class="token operator">=</span>text<span class="token punctuation">,</span> obfuscated<span class="token operator">=</span>obfuscated<span class="token punctuation">)</span>
    <span class="token keyword">assert</span> sut<span class="token punctuation">.</span>guessed <span class="token keyword">is</span> <span class="token boolean">False</span>
    <span class="token keyword">assert</span> sut<span class="token punctuation">.</span>guess<span class="token punctuation">(</span>fake<span class="token punctuation">.</span>pystr<span class="token punctuation">(</span>count<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">is</span> <span class="token boolean">False</span>
    <span class="token keyword">assert</span> sut<span class="token punctuation">.</span>guessed <span class="token keyword">is</span> <span class="token boolean">False</span>
    <span class="token keyword">assert</span> sut<span class="token punctuation">.</span>guess<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
    <span class="token keyword">assert</span> sut<span class="token punctuation">.</span>guessed <span class="token keyword">is</span> <span class="token boolean">True</span></code>`,Jt,ye,Pp,Qt,Gs,dc=`<code class="language-diff">class Cloze(BaseModel):
<span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   text: str = Field(description="The text to be guessed")
</span><span class="token prefix unchanged"> </span><span class="token line">   obfuscated: str = Field(description="Shows the text as")
</span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    guessed = False
</span></span>
<span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   def __str__(self) -> str:
</span><span class="token prefix unchanged"> </span><span class="token line">       """In string context it should return the string that reflects its state"""
</span><span class="token prefix unchanged"> </span><span class="token line">       return self.text if self.guessed is True else self.obfuscated
</span></span>
<span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    def guess(self, candidate: str) -> bool:
</span><span class="token prefix inserted">+</span><span class="token line">        """If the candidate str is the same as the text, it's guessed"""
</span><span class="token prefix inserted">+</span><span class="token line">        self.guessed = candidate == self.text
</span><span class="token prefix inserted">+</span><span class="token line">        return self.guessed</span></span></code>`,Zt,be,Cp,Yt,js,kc=`<code class="language-diff">def test_new_cloze_returns_obfuscated(fake):
<span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   """When a cloze is created, it will return the obfuscated text in string context"""
</span><span class="token prefix unchanged"> </span><span class="token line">   text = fake.pystr()
</span><span class="token prefix unchanged"> </span><span class="token line">   obfuscated = fake.pystr()
</span><span class="token prefix unchanged"> </span><span class="token line">   sut = Cloze(text=text, obfuscated=obfuscated)
</span><span class="token prefix unchanged"> </span><span class="token line">   assert f"&#123;sut&#125; 123" == f"&#123;obfuscated&#125; 123"
</span></span>
<span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    sut.guess(fake.pystr())
</span><span class="token prefix inserted">+</span><span class="token line">    assert f"&#123;sut&#125; 123" == f"&#123;obfuscated&#125; 123"
</span></span>
<span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    sut.guess(text)
</span><span class="token prefix inserted">+</span><span class="token line">    assert f"&#123;sut&#125; 123" == f"&#123;text&#125; 123"</span></span></code>`,Xt,ve,zp,$t,Vs,fc=`<code class="language-python"><span class="token comment"># tests/models/test_cloze_entity.py</span>

<span class="token keyword">def</span> <span class="token function">test_cloze_has_unique_id</span><span class="token punctuation">(</span>fake<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Like all entities, a Cloze has an immutable, unique ID"""</span>
    sut <span class="token operator">=</span> Cloze<span class="token punctuation">(</span>text<span class="token operator">=</span>fake<span class="token punctuation">.</span>pystr<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> obfuscated<span class="token operator">=</span>fake<span class="token punctuation">.</span>pystr<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    immutable_id <span class="token operator">=</span> sut<span class="token punctuation">.</span><span class="token builtin">id</span>
    <span class="token keyword">assert</span> immutable_id

    <span class="token keyword">with</span> pytest<span class="token punctuation">.</span>raises<span class="token punctuation">(</span>ValueError<span class="token punctuation">,</span> <span class="token keyword">match</span><span class="token operator">=</span><span class="token string">'"Cloze" object has no field "id"'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        sut<span class="token punctuation">.</span><span class="token builtin">id</span> <span class="token operator">=</span> fake<span class="token punctuation">.</span>uuid4<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># type: ignore</span>

    <span class="token keyword">assert</span> sut<span class="token punctuation">.</span><span class="token builtin">id</span> <span class="token operator">==</span> immutable_id</code>`,Kt,$,Dp,hn,Lp,Rp,sa,Ws,hc=`<code class="language-diff"># src/models/cloze_entity.py

class Cloze(BaseModel):
<span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   text: str = Field(description="The text to be guessed")
</span><span class="token prefix unchanged"> </span><span class="token line">   obfuscated: str = Field(description="Shows the text as")
</span><span class="token prefix unchanged"> </span><span class="token line">   guessed: bool = Field(False, read_only=True)
</span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    _id: UUID = uuid4()
</span></span>
<span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    @property
</span><span class="token prefix inserted">+</span><span class="token line">    def id(self):
</span><span class="token prefix inserted">+</span><span class="token line">        """This is read only"""
</span><span class="token prefix inserted">+</span><span class="token line">        return self._id
</span></span>
<span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">    @id.setter
</span><span class="token prefix deleted">-</span><span class="token line">    def id(self, whatever):
</span><span class="token prefix deleted">-</span><span class="token line">        """You don't need this in this case"""</span></span></code>`,ea,ge,Op,na,K,ss,mn,Fp,ta,E,Bp,wn,Hp,qp,xe,Mp,Np,aa,es,ns,_n,Up,oa,Ee,Gp,pa,Ie,jp,la,Js,mc=`<code class="language-python"><span class="token comment"># src/models/base_entity.py</span>
<span class="token triple-quoted-string string">"""Represent a very basic DDD entity"""</span>

<span class="token keyword">from</span> uuid <span class="token keyword">import</span> UUID<span class="token punctuation">,</span> uuid4

<span class="token keyword">from</span> pydantic <span class="token keyword">import</span> BaseModel

<span class="token keyword">class</span> <span class="token class-name">BaseEntity</span><span class="token punctuation">(</span>BaseModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Represents a very basic DDD entity

    An entity has an immutable ID, and then some ValueObjects.
    """</span>

    _id<span class="token punctuation">:</span> UUID <span class="token operator">=</span> uuid4<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token decorator annotation punctuation">@property</span>
    <span class="token keyword">def</span> <span class="token function">id</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""This is read only"""</span>  <span class="token comment"># noqa D401</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>_id</code>`,ia,Te,Vp,ra,Qs,wc=`<code class="language-diff"># src/models/cloze_entity.py

<span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">class Cloze(BaseModel):
</span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">class Cloze(BaseEntity):
</span></span><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   text: str = Field(description="The text to be guessed")
</span><span class="token prefix unchanged"> </span><span class="token line">   obfuscated: str = Field(description="Shows the text as")
</span><span class="token prefix unchanged"> </span><span class="token line">   guessed: bool = Field(False, read_only=True)
</span></span><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">    _id: UUID = uuid4()
</span></span>
<span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">    @property
</span><span class="token prefix deleted">-</span><span class="token line">    def id(self):
</span><span class="token prefix deleted">-</span><span class="token line">        """This is read only"""
</span><span class="token prefix deleted">-</span><span class="token line">        return self._id
</span></span></code>`,ca,ts,Wp,yn,Jp,Qp,ua,Ae,Zp,da,f,bn,Yp,Xp,vn,$p,Kp,Zs,sl,gn,el,nl,tl,xn,al,ol,En,pl,ll,In,il,ka,as,rl,Se,cl,ul,fa,os,ps,Tn,dl,ha,ls,kl,An,fl,hl,ma,I,Pe,Sn,ml,wl,_l,Ce,Pn,yl,bl,vl,ze,gl,Cn,xl,wa,k,El,zn,Il,Tl,Dn,Al,Sl,Ln,Pl,Cl,Rn,zl,Dl,On,Ll,Rl,Fn,Ol,Fl,Bn,Bl,Hl,_a,De,Ys,ql,ya,Le,Xs,Ml;return{c(){v=o("h2"),A=o("a"),He=o("span"),Fa=i("What I\u2019m trying to do"),Hn=u(),$s=o("p"),Ba=i("Oh, the never ending quest to master German. I am at the stage where I get it most of it right and can hold conversations. But I still make a lot of small mistakes. I do take language classes every so often to raise the level. But although the improvement is visible, it never fully eradicates those mistakes. I need many more repetitions than the the 10-12 offered by textbooks. In other words, I need Python."),qn=u(),g=o("p"),Ha=i("Textbooks exercises seem to follow a few patterns. Sentences with words in the wrong order, and missing words seem to be the most popular. It shouldn\u2019t be too hard to replicate those exercises. Better, to "),qe=o("em"),qa=i("automate"),Ma=i(" this replication so that I have an endless supply of exercises. I can use "),us=o("a"),Na=i("the free dataset Timo Block made available on GitHub"),Ua=i(" as a starting point."),Mn=u(),S=o("h3"),P=o("a"),Me=o("span"),Ga=i("The end result"),Nn=u(),w=o("p"),ja=i("The end result will have some similarities with flashcards software. Particularly "),ds=o("a"),Va=i("Anki"),Wa=i(", which I use a lot. I will use fall back on Anki concepts when needed, as I have vague plans to integrate some of these exercises with it. But what I am building now is a self contained CLI script. It will pick a German document and feed it to the interface sentence by sentence. The UI will obscure, or partially obscure, parts of the sentences. The task is to guess the obscured words. The first iteration will obscure \u201D"),ks=o("a"),Ja=i("adpositions"),Qa=i("\u201D, (or \u201Cprepositions\u201D in traditional grammar). And will partially obscure \u201D"),fs=o("a"),Za=i("determiners"),Ya=i("\u201D (articles and some pronounous and some adjectives). An example should make this clearer"),Un=u(),hs=o("pre"),Gn=u(),Ks=o("p"),Xa=i("A nice to have would also be to insert trick ones where there shouldn\u2019t be any. For example before years - it\u2019s a common mistake by English speakers to say \u201CIn 1989\u2026\u201D, but in German it\u2019s either \u201C1989\u2026\u201D or \u201CIm Jahr 1989\u2026\u201D"),jn=u(),C=o("h3"),z=o("a"),Ne=o("span"),$a=i("The plan"),Vn=u(),m=o("ol"),Ue=o("li"),Ka=i("The raw dataset is a CSV file with a list of \u201Cdocs\u201D and labels. I don\u2019t need the label, but I need each doc\u2019s sequential \u201Csentences\u201D. That makes the CSV in its current form unsuitable. I will need to preprocess the CSV file"),so=u(),Ge=o("li"),eo=i("A repository will load up the persistent storage (the CSV file). It will fetch a \u2018next\u2019 sentence whenever asked. It may also save updated versions of the CSV file."),no=u(),je=o("li"),to=i("An exercise generator will ask the repository for a sentence. It will then use NLP to turn into an exercise."),ao=u(),Ve=o("li"),oo=i("A session service will ask the exercise generator for N exercises. It will organise them as a queue, and will provide them to the UI when asked. It will also reorganise the queue depending on the outcome of the exercise."),po=u(),We=o("li"),lo=i("The UI will ask the session service for the next exercise, display it, and allow the user to guess. It will then communicate success / error to the session service and get the next exercise."),Wn=u(),D=o("h2"),L=o("a"),Je=o("span"),io=i("Step 0: Setting up the python project"),Jn=u(),ms=o("p"),ro=i("For that I simply use "),se=o("a"),co=i("my trusty cookiecutter template"),Qn=u(),ws=o("pre"),Zn=u(),ee=o("p"),uo=i("And I\u2019m good to go. All I need is to create a folder with the article data"),Yn=u(),_s=o("pre"),Xn=u(),R=o("h2"),O=o("a"),Qe=o("span"),ko=i("Creating a CLI app for prepositions training"),$n=u(),F=o("h3"),B=o("a"),Ze=o("span"),fo=i("Step 1: Preprocessing the documents"),Kn=u(),ne=o("p"),ho=i("So the source data is a csv where each row has two fields: a label which I don\u2019t need, and a \u201Cdoc\u201D which I need to split into \u201Csentences\u201D. I can think of two ways of handling that:"),st=u(),H=o("ol"),Ye=o("li"),mo=i("remove the labels from the CSV and turn it into a TXT with a doc on each line. The repository will then be responsible for lazily splitting a doc in sentences, and holding them in memory"),wo=u(),Xe=o("li"),_o=i("remove the labels and split each row\u2019s single doc field into multiple sentence fields. The repository will not do any splitting"),et=u(),te=o("p"),yo=i("(1) is easier on the pre-processing side; it can be a one line awk command. But I think (2) is cleaner. Typically a repository only knows about I/O and interactions with the persistent layer. It shouldn\u2019t need to know how to extract sentences from docs."),nt=u(),x=o("p"),bo=i("I will use "),ys=o("a"),vo=i("SpaCy"),go=i(" to split docs into sentences. Before using it I need not only to install SpaCy, but to download the model too. "),bs=o("a"),xo=i("A list of downloadable language models"),Eo=i(" is available on the SpaCy website."),tt=u(),vs=o("pre"),at=u(),ae=o("p"),Io=i("This is the script I came up with. Nothing fancy and no tests, it\u2019s just a one off script. No CLI frameworks, it simply reads input from sys.argv."),ot=u(),gs=o("pre"),pt=u(),oe=o("p"),To=i("It runs for about 10 mins on an M1 laptop, but gets the job done"),lt=u(),xs=o("pre"),it=u(),q=o("h3"),M=o("a"),$e=o("span"),Ao=i("Step 2: a repository for German sentences"),rt=u(),pe=o("p"),So=i("The repository module is responsible for"),ct=u(),_=o("ul"),Ke=o("li"),Po=i("interacting with the persistent storage"),Co=u(),sn=o("li"),zo=i("fetching a \u2018next\u2019 sentence from the doc currently marked as \u2018current\u2019"),Do=u(),en=o("li"),Lo=i("if no doc is marked as \u2018current\u2019, one will be picked randomly"),Ro=u(),nn=o("li"),Oo=i("when the last sentence of a doc is picked, the \u2018current\u2019 marker is cleared"),ut=u(),le=o("p"),Fo=i("In my personal projects I like using TDD, so I\u2019ll start with a test"),dt=u(),Es=o("pre"),kt=u(),N=o("p"),Bo=i("The "),tn=o("i"),Ho=i("is_sentence"),qo=i(" is an as yet to coded util function. It can easily be overcomplicated, so I will create tests for it too."),ft=u(),Is=o("pre"),ht=u(),U=o("p"),Mo=i("And a few more; the "),ie=o("a"),No=i("test file is available on github"),Uo=i(". But back to the task at and. Here\u2019s the minimum amount of scaffolding needed to run the tests\u2026"),mt=u(),Ts=o("pre"),wt=u(),re=o("p"),Go=i("\u2026and fail them, as expected - I am returning an empty string after all"),_t=u(),As=o("pre"),yt=u(),ce=o("p"),jo=i("Returning a sentence requires a few steps. First the repo must get hold of the data file and open it. As this is a first iteration, I\u2019m going to keep the \u2018getting hold of the data\u2019 part as simple as possible. But a config file in which to store the location of the data file is a must. Even at this early stage, coupling the location of the code and the data is a bad idea."),bt=u(),Ss=o("pre"),vt=u(),G=o("p"),Vo=i("Since the .tsv file could be quite large, I\u2019d rather not load it in memory. Instead the "),an=o("code"),Wo=i("__init__"),Jo=i(" method will store all the boundaries between lines. When needed, it will use that information to fetch a doc\u2019s sentences. And finally, next_sentence will pop the next sentence from memory. When the repo runs out of sentences, it will fetch a new doc. For now it\u2019s always the same doc that is being fetched. That\u2019s enough to pass the test."),gt=u(),Ps=o("pre"),xt=u(),j=o("p"),Qo=i("A quick smoke test on the REPL shows it works. Calling "),on=o("code"),Zo=i("t.next_sentence"),Yo=i(" a few times shows the repo going through all the sentences, and then starting again when they run out"),Et=u(),Cs=o("pre"),It=u(),ue=o("p"),Xo=i("There are a lot of improvements to be done - for example the docs file is hard coded, there is no error handling, and so on. But at this stage the aim is to get a working MVP, so I\u2019ll move on to the next step."),Tt=u(),V=o("h3"),W=o("a"),pn=o("span"),$o=i("Step 3: generating the exercises"),At=u(),de=o("p"),Ko=i("This module will take a German sentence as input. It will turn it into a data structure with some slots at various stages of guessing."),St=u(),zs=o("pre"),Pt=u(),J=o("p"),sp=i("How best to represent that? I\u2019ll go for a sequence of tokens. Some will be strings (\u201Cfoo foo\u201D) and some will be guessable slots (\u201Cnach dem/xxxxx dxxx\u201D). Anki has a similar concept to \u201Cguessable slot\u201D, i.e., \u2019"),Ds=o("a"),ep=i("cloze deletions"),np=i("\u2019. I will use that name."),Ct=u(),Q=o("h4"),Z=o("a"),ln=o("span"),tp=i("Cloze: an entity to represent a guessable slot within exercise"),zt=u(),ke=o("p"),ap=i("The cloze will be initialised with the string to be guessed and the obfuscated version. It will keep an internal state: guessed or not guessed. The behaviours it will need to implement are"),Dt=u(),y=o("ul"),Ls=o("li"),op=i("return the correct string representation depending on state (method: "),rn=o("code"),pp=i("__str__"),lp=i(")"),ip=u(),Rs=o("li"),rp=i("return the state as a boolean (property: "),cn=o("code"),cp=i("guessed"),up=i(")"),dp=u(),Os=o("li"),kp=i("accept a guess and change state accordingly; return state (method: "),un=o("code"),fp=i("guess(str)"),hp=i(")"),mp=u(),dn=o("li"),wp=i("being an entity, it has a unique, immutable ID"),Lt=u(),fe=o("p"),_p=i("That\u2019s it for now - no hints like Anki, no counting wrong attempts, nothing fancy. Just focus on the minimal functionality to get something that works. As usual I\u2019ll start with a test, and watch it fail. \u201Cfake\u201D is a Faker instance which is included in the cookiecutter template which generates the project. I use it all the time."),Rt=u(),Fs=o("pre"),Ot=u(),he=o("p"),yp=i("What I am creating is essential an Entity, in DDD speak, and will live in the /models folder. I use pydantic for enforcing validation and some type checking. Pydantic is already installed because it\u2019s a dependency of SpaCy. Sadly SpaCy decided to pin its versions to ^1.7.4, so I am forced to do the same"),Ft=u(),Bs=o("pre"),Bt=u(),Y=o("p"),bp=i("Never mind. On with the model. First of all I create an "),kn=o("code"),vp=i("__init__.py"),gp=i(" file, since this is a new folder"),Ht=u(),Hs=o("pre"),qt=u(),me=o("p"),xp=i("Then the Cloze itself"),Mt=u(),qs=o("pre"),Nt=u(),we=o("p"),Ep=i("The test will fail with a more sensible message"),Ut=u(),Ms=o("pre"),Gt=u(),X=o("p"),Ip=i("Getting closer. All I need to pass the text is a "),fn=o("code"),Tp=i("__str__"),Ap=i(" method"),jt=u(),Ns=o("pre"),Vt=u(),_e=o("p"),Sp=i("The next requirements are about being able to guess, and the state changing. Here\u2019s the test\u2026"),Wt=u(),Us=o("pre"),Jt=u(),ye=o("p"),Pp=i("And the code that makes it pass"),Qt=u(),Gs=o("pre"),Zt=u(),be=o("p"),Cp=i("I can also improve the previous test to ensure the string representation changes to mimic the state. It should still pass."),Yt=u(),js=o("pre"),Xt=u(),ve=o("p"),zp=i("And it does. The last requirement is an immutable ID field."),$t=u(),Vs=o("pre"),Kt=u(),$=o("p"),Dp=i("To make a Pydantic field immutable after initialisation, I create a similarly named private field (in this case, \u201D_id\u201D). It doesn\u2019t "),hn=o("em"),Lp=i("need"),Rp=i(" to be similarly named, but it would just be confusing if it wasn\u2019t. Then I add a @property with the name I want to expose. But I will not add a setter - therefore there will be no way to overwrite the id"),sa=u(),Ws=o("pre"),ea=u(),ge=o("p"),Op=i("This is enough for now - all tests pass. As usual, there\u2019s plenty more I could do. But right now the focus is on getting a working prototype"),na=u(),K=o("h5"),ss=o("a"),mn=o("span"),Fp=i("Creating a Faker provider for the Cloze"),ta=u(),E=o("p"),Bp=i("Actually, that was "),wn=o("em"),Hp=i("almost"),qp=i(" all. Another thing I tend to do when creating data structures, is to create Faker providers for them. It makes stubbing them in tests much more easier. I have "),xe=o("a"),Mp=i("a post about creating Faker providers"),Np=i(", so I won\u2019t repeat myself here."),aa=u(),es=o("h4"),ns=o("a"),_n=o("span"),Up=i("An exercise entity"),oa=u(),Ee=o("p"),Gp=i("An Exercise is also an entity; it contains a list of either strings or clozes. Since it is an entity, some of the tests will be the same as Cloze. That\u2019s repetitive and hence error prone. It\u2019s time for the first refactor! I will extract the entity-ness of Cloze into a BaseEntity class. Since I\u2019m refactoring, no need for new tests just yet; the existing ones should do."),pa=u(),Ie=o("p"),jp=i("I extract the _id/id handling part to a new class, BaseEntity\u2026"),la=u(),Js=o("pre"),ia=u(),Te=o("p"),Vp=i("\u2026and make the Cloze inherit from it"),ra=u(),Qs=o("pre"),ca=u(),ts=o("p"),Wp=i("The tests still pass. I move the id test from test_cloze to a new test file, "),yn=o("code"),Jp=i("test_base_entity.py"),Qp=i(", and commit."),ua=u(),Ae=o("p"),Zp=i("I\u2019m read to start on the Exercise entity. The requirement:"),da=u(),f=o("ul"),bn=o("li"),Yp=i("it consists of a list of tokens which can be either strings or Clozes"),Xp=u(),vn=o("li"),$p=i("there must be at least one of each in the list"),Kp=u(),Zs=o("li"),sl=i("it can provide a guessed flag, which is true if "),gn=o("em"),el=i("all"),nl=i(" Clozes are guessed, false otherwise"),tl=u(),xn=o("li"),al=i("it returns the current string representation of the exercise, with each Cloze either obfuscated or not depending on status"),ol=u(),En=o("li"),pl=i("it can receive a guess for a given Cloze, and flip its state accordingly"),ll=u(),In=o("li"),il=i("it can return a list of all Clozes"),ka=u(),as=o("p"),rl=i("So there are quite a lot of requirements. I\u2019m not going to go through all of them here; this entity is not that interesting. It\u2019s similar to Cloze but with a couple more bells and whistles. Just like a Cloze, I built a fake for it. If interested, "),Se=o("a"),cl=i("the code is available in the repo"),ul=i("."),fa=u(),os=o("h4"),ps=o("a"),Tn=o("span"),dl=i("An exercises service"),ha=u(),ls=o("p"),kl=i("Anki would encode the exercise above as "),An=o("code"),fl=i("foo foo {{c1::nach dem::xxxx dxxx}} bar bar {{c2::f\xFCr einen::xxxx einxxx}} pop pop"),hl=i(", where"),ma=u(),I=o("ul"),Pe=o("li"),Sn=o("code"),ml=i("{{ }}"),wl=i(" is the cloze delimiter"),_l=u(),Ce=o("li"),Pn=o("code"),yl=i("::"),bl=i(" is the field separator within it"),vl=u(),ze=o("li"),gl=i("there are 3 fields, the first starting with \u2018c\u2019 is the label, the second is the text to guess, the (optional) third the hint "),Cn=o("code"),xl=i("{{c_LABEL::TO_BE_GUESSED::OPTIONAL_HINT}}"),wa=u(),k=o("p"),El=i("I will add en extra field after the label to store the state of the cloze, and will change the OPTIONAL_HINT to the OFUSCATED VERSION - "),zn=o("code"),Il=i("{{LABEL::STATE::TO_BE_GUESSED::OBFUSCATED}}"),Tl=i(". State will be one of "),Dn=o("code"),Al=i("__new__"),Sl=i(", "),Ln=o("code"),Pl=i("__correct__"),Cl=i(" or the bad guess. So for example "),Rn=o("code"),zl=i("{{c1::__new__::nach dem::xxxx dxxx}}"),Dl=i(" or "),On=o("code"),Ll=i("{{c1::f\xFCr die::nach dem::xxxx dxxx}}"),Rl=i(". In case there are only 3 fields, instead of 4, they are to be treated as "),Fn=o("code"),Ol=i("{{LABEL::TO_BE_GUESSED::OBFUSCATED}}"),Fl=i(". With "),Bn=o("code"),Bl=i("STATE"),Hl=i(" implied to be true"),_a=u(),De=o("p"),Ys=o("a"),ql=i("https://medium.com/steve-cruz/domain-driven-design-ddd-file-structure-ade7fb26553d"),ya=u(),Le=o("p"),Xs=o("a"),Ml=i("https://blog.jacobsdata.com/2020/03/02/a-clean-domain-driven-design-architectural-template"),this.h()},l(s){v=p(s,"H2",{id:!0});var t=l(v);A=p(t,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var ti=l(A);He=p(ti,"SPAN",{class:!0}),l(He).forEach(e),ti.forEach(e),Fa=r(t,"What I\u2019m trying to do"),t.forEach(e),Hn=d(s),$s=p(s,"P",{});var ai=l($s);Ba=r(ai,"Oh, the never ending quest to master German. I am at the stage where I get it most of it right and can hold conversations. But I still make a lot of small mistakes. I do take language classes every so often to raise the level. But although the improvement is visible, it never fully eradicates those mistakes. I need many more repetitions than the the 10-12 offered by textbooks. In other words, I need Python."),ai.forEach(e),qn=d(s),g=p(s,"P",{});var Re=l(g);Ha=r(Re,"Textbooks exercises seem to follow a few patterns. Sentences with words in the wrong order, and missing words seem to be the most popular. It shouldn\u2019t be too hard to replicate those exercises. Better, to "),qe=p(Re,"EM",{});var oi=l(qe);qa=r(oi,"automate"),oi.forEach(e),Ma=r(Re," this replication so that I have an endless supply of exercises. I can use "),us=p(Re,"A",{href:!0,rel:!0});var pi=l(us);Na=r(pi,"the free dataset Timo Block made available on GitHub"),pi.forEach(e),Ua=r(Re," as a starting point."),Re.forEach(e),Mn=d(s),S=p(s,"H3",{id:!0});var Nl=l(S);P=p(Nl,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var li=l(P);Me=p(li,"SPAN",{class:!0}),l(Me).forEach(e),li.forEach(e),Ga=r(Nl,"The end result"),Nl.forEach(e),Nn=d(s),w=p(s,"P",{});var is=l(w);ja=r(is,"The end result will have some similarities with flashcards software. Particularly "),ds=p(is,"A",{href:!0,rel:!0});var ii=l(ds);Va=r(ii,"Anki"),ii.forEach(e),Wa=r(is,", which I use a lot. I will use fall back on Anki concepts when needed, as I have vague plans to integrate some of these exercises with it. But what I am building now is a self contained CLI script. It will pick a German document and feed it to the interface sentence by sentence. The UI will obscure, or partially obscure, parts of the sentences. The task is to guess the obscured words. The first iteration will obscure \u201D"),ks=p(is,"A",{href:!0,rel:!0});var ri=l(ks);Ja=r(ri,"adpositions"),ri.forEach(e),Qa=r(is,"\u201D, (or \u201Cprepositions\u201D in traditional grammar). And will partially obscure \u201D"),fs=p(is,"A",{href:!0,rel:!0});var ci=l(fs);Za=r(ci,"determiners"),ci.forEach(e),Ya=r(is,"\u201D (articles and some pronounous and some adjectives). An example should make this clearer"),is.forEach(e),Un=d(s),hs=p(s,"PRE",{class:!0});var _c=l(hs);_c.forEach(e),Gn=d(s),Ks=p(s,"P",{});var ui=l(Ks);Xa=r(ui,"A nice to have would also be to insert trick ones where there shouldn\u2019t be any. For example before years - it\u2019s a common mistake by English speakers to say \u201CIn 1989\u2026\u201D, but in German it\u2019s either \u201C1989\u2026\u201D or \u201CIm Jahr 1989\u2026\u201D"),ui.forEach(e),jn=d(s),C=p(s,"H3",{id:!0});var Ul=l(C);z=p(Ul,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var di=l(z);Ne=p(di,"SPAN",{class:!0}),l(Ne).forEach(e),di.forEach(e),$a=r(Ul,"The plan"),Ul.forEach(e),Vn=d(s),m=p(s,"OL",{});var T=l(m);Ue=p(T,"LI",{});var ki=l(Ue);Ka=r(ki,"The raw dataset is a CSV file with a list of \u201Cdocs\u201D and labels. I don\u2019t need the label, but I need each doc\u2019s sequential \u201Csentences\u201D. That makes the CSV in its current form unsuitable. I will need to preprocess the CSV file"),ki.forEach(e),so=d(T),Ge=p(T,"LI",{});var fi=l(Ge);eo=r(fi,"A repository will load up the persistent storage (the CSV file). It will fetch a \u2018next\u2019 sentence whenever asked. It may also save updated versions of the CSV file."),fi.forEach(e),no=d(T),je=p(T,"LI",{});var hi=l(je);to=r(hi,"An exercise generator will ask the repository for a sentence. It will then use NLP to turn into an exercise."),hi.forEach(e),ao=d(T),Ve=p(T,"LI",{});var mi=l(Ve);oo=r(mi,"A session service will ask the exercise generator for N exercises. It will organise them as a queue, and will provide them to the UI when asked. It will also reorganise the queue depending on the outcome of the exercise."),mi.forEach(e),po=d(T),We=p(T,"LI",{});var wi=l(We);lo=r(wi,"The UI will ask the session service for the next exercise, display it, and allow the user to guess. It will then communicate success / error to the session service and get the next exercise."),wi.forEach(e),T.forEach(e),Wn=d(s),D=p(s,"H2",{id:!0});var Gl=l(D);L=p(Gl,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var _i=l(L);Je=p(_i,"SPAN",{class:!0}),l(Je).forEach(e),_i.forEach(e),io=r(Gl,"Step 0: Setting up the python project"),Gl.forEach(e),Jn=d(s),ms=p(s,"P",{});var jl=l(ms);ro=r(jl,"For that I simply use "),se=p(jl,"A",{href:!0});var yi=l(se);co=r(yi,"my trusty cookiecutter template"),yi.forEach(e),jl.forEach(e),Qn=d(s),ws=p(s,"PRE",{class:!0});var yc=l(ws);yc.forEach(e),Zn=d(s),ee=p(s,"P",{});var bi=l(ee);uo=r(bi,"And I\u2019m good to go. All I need is to create a folder with the article data"),bi.forEach(e),Yn=d(s),_s=p(s,"PRE",{class:!0});var bc=l(_s);bc.forEach(e),Xn=d(s),R=p(s,"H2",{id:!0});var Vl=l(R);O=p(Vl,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var vi=l(O);Qe=p(vi,"SPAN",{class:!0}),l(Qe).forEach(e),vi.forEach(e),ko=r(Vl,"Creating a CLI app for prepositions training"),Vl.forEach(e),$n=d(s),F=p(s,"H3",{id:!0});var Wl=l(F);B=p(Wl,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var gi=l(B);Ze=p(gi,"SPAN",{class:!0}),l(Ze).forEach(e),gi.forEach(e),fo=r(Wl,"Step 1: Preprocessing the documents"),Wl.forEach(e),Kn=d(s),ne=p(s,"P",{});var xi=l(ne);ho=r(xi,"So the source data is a csv where each row has two fields: a label which I don\u2019t need, and a \u201Cdoc\u201D which I need to split into \u201Csentences\u201D. I can think of two ways of handling that:"),xi.forEach(e),st=d(s),H=p(s,"OL",{});var ba=l(H);Ye=p(ba,"LI",{});var Ei=l(Ye);mo=r(Ei,"remove the labels from the CSV and turn it into a TXT with a doc on each line. The repository will then be responsible for lazily splitting a doc in sentences, and holding them in memory"),Ei.forEach(e),wo=d(ba),Xe=p(ba,"LI",{});var Ii=l(Xe);_o=r(Ii,"remove the labels and split each row\u2019s single doc field into multiple sentence fields. The repository will not do any splitting"),Ii.forEach(e),ba.forEach(e),et=d(s),te=p(s,"P",{});var Ti=l(te);yo=r(Ti,"(1) is easier on the pre-processing side; it can be a one line awk command. But I think (2) is cleaner. Typically a repository only knows about I/O and interactions with the persistent layer. It shouldn\u2019t need to know how to extract sentences from docs."),Ti.forEach(e),nt=d(s),x=p(s,"P",{});var Oe=l(x);bo=r(Oe,"I will use "),ys=p(Oe,"A",{href:!0,rel:!0});var Ai=l(ys);vo=r(Ai,"SpaCy"),Ai.forEach(e),go=r(Oe," to split docs into sentences. Before using it I need not only to install SpaCy, but to download the model too. "),bs=p(Oe,"A",{href:!0,rel:!0});var Si=l(bs);xo=r(Si,"A list of downloadable language models"),Si.forEach(e),Eo=r(Oe," is available on the SpaCy website."),Oe.forEach(e),tt=d(s),vs=p(s,"PRE",{class:!0});var vc=l(vs);vc.forEach(e),at=d(s),ae=p(s,"P",{});var Pi=l(ae);Io=r(Pi,"This is the script I came up with. Nothing fancy and no tests, it\u2019s just a one off script. No CLI frameworks, it simply reads input from sys.argv."),Pi.forEach(e),ot=d(s),gs=p(s,"PRE",{class:!0});var gc=l(gs);gc.forEach(e),pt=d(s),oe=p(s,"P",{});var Ci=l(oe);To=r(Ci,"It runs for about 10 mins on an M1 laptop, but gets the job done"),Ci.forEach(e),lt=d(s),xs=p(s,"PRE",{class:!0});var xc=l(xs);xc.forEach(e),it=d(s),q=p(s,"H3",{id:!0});var Jl=l(q);M=p(Jl,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var zi=l(M);$e=p(zi,"SPAN",{class:!0}),l($e).forEach(e),zi.forEach(e),Ao=r(Jl,"Step 2: a repository for German sentences"),Jl.forEach(e),rt=d(s),pe=p(s,"P",{});var Di=l(pe);So=r(Di,"The repository module is responsible for"),Di.forEach(e),ct=d(s),_=p(s,"UL",{});var rs=l(_);Ke=p(rs,"LI",{});var Li=l(Ke);Po=r(Li,"interacting with the persistent storage"),Li.forEach(e),Co=d(rs),sn=p(rs,"LI",{});var Ri=l(sn);zo=r(Ri,"fetching a \u2018next\u2019 sentence from the doc currently marked as \u2018current\u2019"),Ri.forEach(e),Do=d(rs),en=p(rs,"LI",{});var Oi=l(en);Lo=r(Oi,"if no doc is marked as \u2018current\u2019, one will be picked randomly"),Oi.forEach(e),Ro=d(rs),nn=p(rs,"LI",{});var Fi=l(nn);Oo=r(Fi,"when the last sentence of a doc is picked, the \u2018current\u2019 marker is cleared"),Fi.forEach(e),rs.forEach(e),ut=d(s),le=p(s,"P",{});var Bi=l(le);Fo=r(Bi,"In my personal projects I like using TDD, so I\u2019ll start with a test"),Bi.forEach(e),dt=d(s),Es=p(s,"PRE",{class:!0});var Ec=l(Es);Ec.forEach(e),kt=d(s),N=p(s,"P",{});var va=l(N);Bo=r(va,"The "),tn=p(va,"I",{});var Hi=l(tn);Ho=r(Hi,"is_sentence"),Hi.forEach(e),qo=r(va," is an as yet to coded util function. It can easily be overcomplicated, so I will create tests for it too."),va.forEach(e),ft=d(s),Is=p(s,"PRE",{class:!0});var Ic=l(Is);Ic.forEach(e),ht=d(s),U=p(s,"P",{});var ga=l(U);Mo=r(ga,"And a few more; the "),ie=p(ga,"A",{href:!0});var qi=l(ie);No=r(qi,"test file is available on github"),qi.forEach(e),Uo=r(ga,". But back to the task at and. Here\u2019s the minimum amount of scaffolding needed to run the tests\u2026"),ga.forEach(e),mt=d(s),Ts=p(s,"PRE",{class:!0});var Tc=l(Ts);Tc.forEach(e),wt=d(s),re=p(s,"P",{});var Mi=l(re);Go=r(Mi,"\u2026and fail them, as expected - I am returning an empty string after all"),Mi.forEach(e),_t=d(s),As=p(s,"PRE",{class:!0});var Ac=l(As);Ac.forEach(e),yt=d(s),ce=p(s,"P",{});var Ni=l(ce);jo=r(Ni,"Returning a sentence requires a few steps. First the repo must get hold of the data file and open it. As this is a first iteration, I\u2019m going to keep the \u2018getting hold of the data\u2019 part as simple as possible. But a config file in which to store the location of the data file is a must. Even at this early stage, coupling the location of the code and the data is a bad idea."),Ni.forEach(e),bt=d(s),Ss=p(s,"PRE",{class:!0});var Sc=l(Ss);Sc.forEach(e),vt=d(s),G=p(s,"P",{});var xa=l(G);Vo=r(xa,"Since the .tsv file could be quite large, I\u2019d rather not load it in memory. Instead the "),an=p(xa,"CODE",{});var Ui=l(an);Wo=r(Ui,"__init__"),Ui.forEach(e),Jo=r(xa," method will store all the boundaries between lines. When needed, it will use that information to fetch a doc\u2019s sentences. And finally, next_sentence will pop the next sentence from memory. When the repo runs out of sentences, it will fetch a new doc. For now it\u2019s always the same doc that is being fetched. That\u2019s enough to pass the test."),xa.forEach(e),gt=d(s),Ps=p(s,"PRE",{class:!0});var Pc=l(Ps);Pc.forEach(e),xt=d(s),j=p(s,"P",{});var Ea=l(j);Qo=r(Ea,"A quick smoke test on the REPL shows it works. Calling "),on=p(Ea,"CODE",{});var Gi=l(on);Zo=r(Gi,"t.next_sentence"),Gi.forEach(e),Yo=r(Ea," a few times shows the repo going through all the sentences, and then starting again when they run out"),Ea.forEach(e),Et=d(s),Cs=p(s,"PRE",{class:!0});var Cc=l(Cs);Cc.forEach(e),It=d(s),ue=p(s,"P",{});var ji=l(ue);Xo=r(ji,"There are a lot of improvements to be done - for example the docs file is hard coded, there is no error handling, and so on. But at this stage the aim is to get a working MVP, so I\u2019ll move on to the next step."),ji.forEach(e),Tt=d(s),V=p(s,"H3",{id:!0});var Ql=l(V);W=p(Ql,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var Vi=l(W);pn=p(Vi,"SPAN",{class:!0}),l(pn).forEach(e),Vi.forEach(e),$o=r(Ql,"Step 3: generating the exercises"),Ql.forEach(e),At=d(s),de=p(s,"P",{});var Wi=l(de);Ko=r(Wi,"This module will take a German sentence as input. It will turn it into a data structure with some slots at various stages of guessing."),Wi.forEach(e),St=d(s),zs=p(s,"PRE",{class:!0});var zc=l(zs);zc.forEach(e),Pt=d(s),J=p(s,"P",{});var Ia=l(J);sp=r(Ia,"How best to represent that? I\u2019ll go for a sequence of tokens. Some will be strings (\u201Cfoo foo\u201D) and some will be guessable slots (\u201Cnach dem/xxxxx dxxx\u201D). Anki has a similar concept to \u201Cguessable slot\u201D, i.e., \u2019"),Ds=p(Ia,"A",{href:!0,rel:!0});var Ji=l(Ds);ep=r(Ji,"cloze deletions"),Ji.forEach(e),np=r(Ia,"\u2019. I will use that name."),Ia.forEach(e),Ct=d(s),Q=p(s,"H4",{id:!0});var Zl=l(Q);Z=p(Zl,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var Qi=l(Z);ln=p(Qi,"SPAN",{class:!0}),l(ln).forEach(e),Qi.forEach(e),tp=r(Zl,"Cloze: an entity to represent a guessable slot within exercise"),Zl.forEach(e),zt=d(s),ke=p(s,"P",{});var Zi=l(ke);ap=r(Zi,"The cloze will be initialised with the string to be guessed and the obfuscated version. It will keep an internal state: guessed or not guessed. The behaviours it will need to implement are"),Zi.forEach(e),Dt=d(s),y=p(s,"UL",{});var cs=l(y);Ls=p(cs,"LI",{});var Ta=l(Ls);op=r(Ta,"return the correct string representation depending on state (method: "),rn=p(Ta,"CODE",{});var Yi=l(rn);pp=r(Yi,"__str__"),Yi.forEach(e),lp=r(Ta,")"),Ta.forEach(e),ip=d(cs),Rs=p(cs,"LI",{});var Aa=l(Rs);rp=r(Aa,"return the state as a boolean (property: "),cn=p(Aa,"CODE",{});var Xi=l(cn);cp=r(Xi,"guessed"),Xi.forEach(e),up=r(Aa,")"),Aa.forEach(e),dp=d(cs),Os=p(cs,"LI",{});var Sa=l(Os);kp=r(Sa,"accept a guess and change state accordingly; return state (method: "),un=p(Sa,"CODE",{});var $i=l(un);fp=r($i,"guess(str)"),$i.forEach(e),hp=r(Sa,")"),Sa.forEach(e),mp=d(cs),dn=p(cs,"LI",{});var Ki=l(dn);wp=r(Ki,"being an entity, it has a unique, immutable ID"),Ki.forEach(e),cs.forEach(e),Lt=d(s),fe=p(s,"P",{});var sr=l(fe);_p=r(sr,"That\u2019s it for now - no hints like Anki, no counting wrong attempts, nothing fancy. Just focus on the minimal functionality to get something that works. As usual I\u2019ll start with a test, and watch it fail. \u201Cfake\u201D is a Faker instance which is included in the cookiecutter template which generates the project. I use it all the time."),sr.forEach(e),Rt=d(s),Fs=p(s,"PRE",{class:!0});var Dc=l(Fs);Dc.forEach(e),Ot=d(s),he=p(s,"P",{});var er=l(he);yp=r(er,"What I am creating is essential an Entity, in DDD speak, and will live in the /models folder. I use pydantic for enforcing validation and some type checking. Pydantic is already installed because it\u2019s a dependency of SpaCy. Sadly SpaCy decided to pin its versions to ^1.7.4, so I am forced to do the same"),er.forEach(e),Ft=d(s),Bs=p(s,"PRE",{class:!0});var Lc=l(Bs);Lc.forEach(e),Bt=d(s),Y=p(s,"P",{});var Pa=l(Y);bp=r(Pa,"Never mind. On with the model. First of all I create an "),kn=p(Pa,"CODE",{});var nr=l(kn);vp=r(nr,"__init__.py"),nr.forEach(e),gp=r(Pa," file, since this is a new folder"),Pa.forEach(e),Ht=d(s),Hs=p(s,"PRE",{class:!0});var Rc=l(Hs);Rc.forEach(e),qt=d(s),me=p(s,"P",{});var tr=l(me);xp=r(tr,"Then the Cloze itself"),tr.forEach(e),Mt=d(s),qs=p(s,"PRE",{class:!0});var Oc=l(qs);Oc.forEach(e),Nt=d(s),we=p(s,"P",{});var ar=l(we);Ep=r(ar,"The test will fail with a more sensible message"),ar.forEach(e),Ut=d(s),Ms=p(s,"PRE",{class:!0});var Fc=l(Ms);Fc.forEach(e),Gt=d(s),X=p(s,"P",{});var Ca=l(X);Ip=r(Ca,"Getting closer. All I need to pass the text is a "),fn=p(Ca,"CODE",{});var or=l(fn);Tp=r(or,"__str__"),or.forEach(e),Ap=r(Ca," method"),Ca.forEach(e),jt=d(s),Ns=p(s,"PRE",{class:!0});var Bc=l(Ns);Bc.forEach(e),Vt=d(s),_e=p(s,"P",{});var pr=l(_e);Sp=r(pr,"The next requirements are about being able to guess, and the state changing. Here\u2019s the test\u2026"),pr.forEach(e),Wt=d(s),Us=p(s,"PRE",{class:!0});var Hc=l(Us);Hc.forEach(e),Jt=d(s),ye=p(s,"P",{});var lr=l(ye);Pp=r(lr,"And the code that makes it pass"),lr.forEach(e),Qt=d(s),Gs=p(s,"PRE",{class:!0});var qc=l(Gs);qc.forEach(e),Zt=d(s),be=p(s,"P",{});var ir=l(be);Cp=r(ir,"I can also improve the previous test to ensure the string representation changes to mimic the state. It should still pass."),ir.forEach(e),Yt=d(s),js=p(s,"PRE",{class:!0});var Mc=l(js);Mc.forEach(e),Xt=d(s),ve=p(s,"P",{});var rr=l(ve);zp=r(rr,"And it does. The last requirement is an immutable ID field."),rr.forEach(e),$t=d(s),Vs=p(s,"PRE",{class:!0});var Nc=l(Vs);Nc.forEach(e),Kt=d(s),$=p(s,"P",{});var za=l($);Dp=r(za,"To make a Pydantic field immutable after initialisation, I create a similarly named private field (in this case, \u201D_id\u201D). It doesn\u2019t "),hn=p(za,"EM",{});var cr=l(hn);Lp=r(cr,"need"),cr.forEach(e),Rp=r(za," to be similarly named, but it would just be confusing if it wasn\u2019t. Then I add a @property with the name I want to expose. But I will not add a setter - therefore there will be no way to overwrite the id"),za.forEach(e),sa=d(s),Ws=p(s,"PRE",{class:!0});var Uc=l(Ws);Uc.forEach(e),ea=d(s),ge=p(s,"P",{});var ur=l(ge);Op=r(ur,"This is enough for now - all tests pass. As usual, there\u2019s plenty more I could do. But right now the focus is on getting a working prototype"),ur.forEach(e),na=d(s),K=p(s,"H5",{id:!0});var Yl=l(K);ss=p(Yl,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var dr=l(ss);mn=p(dr,"SPAN",{class:!0}),l(mn).forEach(e),dr.forEach(e),Fp=r(Yl,"Creating a Faker provider for the Cloze"),Yl.forEach(e),ta=d(s),E=p(s,"P",{});var Fe=l(E);Bp=r(Fe,"Actually, that was "),wn=p(Fe,"EM",{});var kr=l(wn);Hp=r(kr,"almost"),kr.forEach(e),qp=r(Fe," all. Another thing I tend to do when creating data structures, is to create Faker providers for them. It makes stubbing them in tests much more easier. I have "),xe=p(Fe,"A",{href:!0});var fr=l(xe);Mp=r(fr,"a post about creating Faker providers"),fr.forEach(e),Np=r(Fe,", so I won\u2019t repeat myself here."),Fe.forEach(e),aa=d(s),es=p(s,"H4",{id:!0});var Xl=l(es);ns=p(Xl,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var hr=l(ns);_n=p(hr,"SPAN",{class:!0}),l(_n).forEach(e),hr.forEach(e),Up=r(Xl,"An exercise entity"),Xl.forEach(e),oa=d(s),Ee=p(s,"P",{});var mr=l(Ee);Gp=r(mr,"An Exercise is also an entity; it contains a list of either strings or clozes. Since it is an entity, some of the tests will be the same as Cloze. That\u2019s repetitive and hence error prone. It\u2019s time for the first refactor! I will extract the entity-ness of Cloze into a BaseEntity class. Since I\u2019m refactoring, no need for new tests just yet; the existing ones should do."),mr.forEach(e),pa=d(s),Ie=p(s,"P",{});var wr=l(Ie);jp=r(wr,"I extract the _id/id handling part to a new class, BaseEntity\u2026"),wr.forEach(e),la=d(s),Js=p(s,"PRE",{class:!0});var Gc=l(Js);Gc.forEach(e),ia=d(s),Te=p(s,"P",{});var _r=l(Te);Vp=r(_r,"\u2026and make the Cloze inherit from it"),_r.forEach(e),ra=d(s),Qs=p(s,"PRE",{class:!0});var jc=l(Qs);jc.forEach(e),ca=d(s),ts=p(s,"P",{});var Da=l(ts);Wp=r(Da,"The tests still pass. I move the id test from test_cloze to a new test file, "),yn=p(Da,"CODE",{});var yr=l(yn);Jp=r(yr,"test_base_entity.py"),yr.forEach(e),Qp=r(Da,", and commit."),Da.forEach(e),ua=d(s),Ae=p(s,"P",{});var br=l(Ae);Zp=r(br,"I\u2019m read to start on the Exercise entity. The requirement:"),br.forEach(e),da=d(s),f=p(s,"UL",{});var b=l(f);bn=p(b,"LI",{});var vr=l(bn);Yp=r(vr,"it consists of a list of tokens which can be either strings or Clozes"),vr.forEach(e),Xp=d(b),vn=p(b,"LI",{});var gr=l(vn);$p=r(gr,"there must be at least one of each in the list"),gr.forEach(e),Kp=d(b),Zs=p(b,"LI",{});var La=l(Zs);sl=r(La,"it can provide a guessed flag, which is true if "),gn=p(La,"EM",{});var xr=l(gn);el=r(xr,"all"),xr.forEach(e),nl=r(La," Clozes are guessed, false otherwise"),La.forEach(e),tl=d(b),xn=p(b,"LI",{});var Er=l(xn);al=r(Er,"it returns the current string representation of the exercise, with each Cloze either obfuscated or not depending on status"),Er.forEach(e),ol=d(b),En=p(b,"LI",{});var Ir=l(En);pl=r(Ir,"it can receive a guess for a given Cloze, and flip its state accordingly"),Ir.forEach(e),ll=d(b),In=p(b,"LI",{});var Tr=l(In);il=r(Tr,"it can return a list of all Clozes"),Tr.forEach(e),b.forEach(e),ka=d(s),as=p(s,"P",{});var Ra=l(as);rl=r(Ra,"So there are quite a lot of requirements. I\u2019m not going to go through all of them here; this entity is not that interesting. It\u2019s similar to Cloze but with a couple more bells and whistles. Just like a Cloze, I built a fake for it. If interested, "),Se=p(Ra,"A",{href:!0});var Ar=l(Se);cl=r(Ar,"the code is available in the repo"),Ar.forEach(e),ul=r(Ra,"."),Ra.forEach(e),fa=d(s),os=p(s,"H4",{id:!0});var $l=l(os);ps=p($l,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var Sr=l(ps);Tn=p(Sr,"SPAN",{class:!0}),l(Tn).forEach(e),Sr.forEach(e),dl=r($l,"An exercises service"),$l.forEach(e),ha=d(s),ls=p(s,"P",{});var Oa=l(ls);kl=r(Oa,"Anki would encode the exercise above as "),An=p(Oa,"CODE",{});var Pr=l(An);fl=r(Pr,"foo foo {{c1::nach dem::xxxx dxxx}} bar bar {{c2::f\xFCr einen::xxxx einxxx}} pop pop"),Pr.forEach(e),hl=r(Oa,", where"),Oa.forEach(e),ma=d(s),I=p(s,"UL",{});var Be=l(I);Pe=p(Be,"LI",{});var Kl=l(Pe);Sn=p(Kl,"CODE",{});var Cr=l(Sn);ml=r(Cr,"{{ }}"),Cr.forEach(e),wl=r(Kl," is the cloze delimiter"),Kl.forEach(e),_l=d(Be),Ce=p(Be,"LI",{});var si=l(Ce);Pn=p(si,"CODE",{});var zr=l(Pn);yl=r(zr,"::"),zr.forEach(e),bl=r(si," is the field separator within it"),si.forEach(e),vl=d(Be),ze=p(Be,"LI",{});var ei=l(ze);gl=r(ei,"there are 3 fields, the first starting with \u2018c\u2019 is the label, the second is the text to guess, the (optional) third the hint "),Cn=p(ei,"CODE",{});var Dr=l(Cn);xl=r(Dr,"{{c_LABEL::TO_BE_GUESSED::OPTIONAL_HINT}}"),Dr.forEach(e),ei.forEach(e),Be.forEach(e),wa=d(s),k=p(s,"P",{});var h=l(k);El=r(h,"I will add en extra field after the label to store the state of the cloze, and will change the OPTIONAL_HINT to the OFUSCATED VERSION - "),zn=p(h,"CODE",{});var Lr=l(zn);Il=r(Lr,"{{LABEL::STATE::TO_BE_GUESSED::OBFUSCATED}}"),Lr.forEach(e),Tl=r(h,". State will be one of "),Dn=p(h,"CODE",{});var Rr=l(Dn);Al=r(Rr,"__new__"),Rr.forEach(e),Sl=r(h,", "),Ln=p(h,"CODE",{});var Or=l(Ln);Pl=r(Or,"__correct__"),Or.forEach(e),Cl=r(h," or the bad guess. So for example "),Rn=p(h,"CODE",{});var Fr=l(Rn);zl=r(Fr,"{{c1::__new__::nach dem::xxxx dxxx}}"),Fr.forEach(e),Dl=r(h," or "),On=p(h,"CODE",{});var Br=l(On);Ll=r(Br,"{{c1::f\xFCr die::nach dem::xxxx dxxx}}"),Br.forEach(e),Rl=r(h,". In case there are only 3 fields, instead of 4, they are to be treated as "),Fn=p(h,"CODE",{});var Hr=l(Fn);Ol=r(Hr,"{{LABEL::TO_BE_GUESSED::OBFUSCATED}}"),Hr.forEach(e),Fl=r(h,". With "),Bn=p(h,"CODE",{});var qr=l(Bn);Bl=r(qr,"STATE"),qr.forEach(e),Hl=r(h," implied to be true"),h.forEach(e),_a=d(s),De=p(s,"P",{});var Mr=l(De);Ys=p(Mr,"A",{href:!0,rel:!0});var Nr=l(Ys);ql=r(Nr,"https://medium.com/steve-cruz/domain-driven-design-ddd-file-structure-ade7fb26553d"),Nr.forEach(e),Mr.forEach(e),ya=d(s),Le=p(s,"P",{});var Ur=l(Le);Xs=p(Ur,"A",{href:!0,rel:!0});var Gr=l(Xs);Ml=r(Gr,"https://blog.jacobsdata.com/2020/03/02/a-clean-domain-driven-design-architectural-template"),Gr.forEach(e),Ur.forEach(e),this.h()},h(){c(He,"class","icon icon-link"),c(A,"aria-hidden","true"),c(A,"tabindex","-1"),c(A,"href","#what-im-trying-to-do"),c(v,"id","what-im-trying-to-do"),c(us,"href","https://tblock.github.io/10kGNAD/"),c(us,"rel","nofollow"),c(Me,"class","icon icon-link"),c(P,"aria-hidden","true"),c(P,"tabindex","-1"),c(P,"href","#the-end-result"),c(S,"id","the-end-result"),c(ds,"href","https://apps.ankiweb.net/"),c(ds,"rel","nofollow"),c(ks,"href","https://universaldependencies.org/u/pos/ADP.html"),c(ks,"rel","nofollow"),c(fs,"href","https://universaldependencies.org/u/pos/DET.html"),c(fs,"rel","nofollow"),c(hs,"class","language-txt"),c(Ne,"class","icon icon-link"),c(z,"aria-hidden","true"),c(z,"tabindex","-1"),c(z,"href","#the-plan"),c(C,"id","the-plan"),c(Je,"class","icon icon-link"),c(L,"aria-hidden","true"),c(L,"tabindex","-1"),c(L,"href","#step-0-setting-up-the-python-project"),c(D,"id","step-0-setting-up-the-python-project"),c(se,"href","/blog/creating-a-poetry-driven-python-project-template-with-cookiecutter"),c(ws,"class","language-bash"),c(_s,"class","language-bash"),c(Qe,"class","icon icon-link"),c(O,"aria-hidden","true"),c(O,"tabindex","-1"),c(O,"href","#creating-a-cli-app-for-prepositions-training"),c(R,"id","creating-a-cli-app-for-prepositions-training"),c(Ze,"class","icon icon-link"),c(B,"aria-hidden","true"),c(B,"tabindex","-1"),c(B,"href","#step-1-preprocessing-the-documents"),c(F,"id","step-1-preprocessing-the-documents"),c(ys,"href","https://spacy.io/"),c(ys,"rel","nofollow"),c(bs,"href","https://spacy.io/usage/models"),c(bs,"rel","nofollow"),c(vs,"class","language-bash"),c(gs,"class","language-python"),c(xs,"class","language-bash"),c($e,"class","icon icon-link"),c(M,"aria-hidden","true"),c(M,"tabindex","-1"),c(M,"href","#step-2-a-repository-for-german-sentences"),c(q,"id","step-2-a-repository-for-german-sentences"),c(Es,"class","language-python"),c(Is,"class","language-python"),c(ie,"href","TODO"),c(Ts,"class","language-python"),c(As,"class","language-bash"),c(Ss,"class","language-python"),c(Ps,"class","language-python"),c(Cs,"class","language-python"),c(pn,"class","icon icon-link"),c(W,"aria-hidden","true"),c(W,"tabindex","-1"),c(W,"href","#step-3-generating-the-exercises"),c(V,"id","step-3-generating-the-exercises"),c(zs,"class","language-txt"),c(Ds,"href","https://docs.ankiweb.net/editing.html#cloze-deletion"),c(Ds,"rel","nofollow"),c(ln,"class","icon icon-link"),c(Z,"aria-hidden","true"),c(Z,"tabindex","-1"),c(Z,"href","#cloze-an-entity-to-represent-a-guessable-slot-within-exercise"),c(Q,"id","cloze-an-entity-to-represent-a-guessable-slot-within-exercise"),c(Fs,"class","language-python"),c(Bs,"class","language-bash"),c(Hs,"class","language-python"),c(qs,"class","language-python"),c(Ms,"class","language-bash"),c(Ns,"class","language-diff"),c(Us,"class","language-python"),c(Gs,"class","language-diff"),c(js,"class","language-diff"),c(Vs,"class","language-python"),c(Ws,"class","language-diff"),c(mn,"class","icon icon-link"),c(ss,"aria-hidden","true"),c(ss,"tabindex","-1"),c(ss,"href","#creating-a-faker-provider-for-the-cloze"),c(K,"id","creating-a-faker-provider-for-the-cloze"),c(xe,"href","/blog/create-fake-dataset-fixtures-testing-with-faker"),c(_n,"class","icon icon-link"),c(ns,"aria-hidden","true"),c(ns,"tabindex","-1"),c(ns,"href","#an-exercise-entity"),c(es,"id","an-exercise-entity"),c(Js,"class","language-python"),c(Qs,"class","language-diff"),c(Se,"href","TODO"),c(Tn,"class","icon icon-link"),c(ps,"aria-hidden","true"),c(ps,"tabindex","-1"),c(ps,"href","#an-exercises-service"),c(os,"id","an-exercises-service"),c(Ys,"href","https://medium.com/steve-cruz/domain-driven-design-ddd-file-structure-ade7fb26553d"),c(Ys,"rel","nofollow"),c(Xs,"href","https://blog.jacobsdata.com/2020/03/02/a-clean-domain-driven-design-architectural-template"),c(Xs,"rel","nofollow")},m(s,t){a(s,v,t),n(v,A),n(A,He),n(v,Fa),a(s,Hn,t),a(s,$s,t),n($s,Ba),a(s,qn,t),a(s,g,t),n(g,Ha),n(g,qe),n(qe,qa),n(g,Ma),n(g,us),n(us,Na),n(g,Ua),a(s,Mn,t),a(s,S,t),n(S,P),n(P,Me),n(S,Ga),a(s,Nn,t),a(s,w,t),n(w,ja),n(w,ds),n(ds,Va),n(w,Wa),n(w,ks),n(ks,Ja),n(w,Qa),n(w,fs),n(fs,Za),n(w,Ya),a(s,Un,t),a(s,hs,t),hs.innerHTML=Vr,a(s,Gn,t),a(s,Ks,t),n(Ks,Xa),a(s,jn,t),a(s,C,t),n(C,z),n(z,Ne),n(C,$a),a(s,Vn,t),a(s,m,t),n(m,Ue),n(Ue,Ka),n(m,so),n(m,Ge),n(Ge,eo),n(m,no),n(m,je),n(je,to),n(m,ao),n(m,Ve),n(Ve,oo),n(m,po),n(m,We),n(We,lo),a(s,Wn,t),a(s,D,t),n(D,L),n(L,Je),n(D,io),a(s,Jn,t),a(s,ms,t),n(ms,ro),n(ms,se),n(se,co),a(s,Qn,t),a(s,ws,t),ws.innerHTML=Wr,a(s,Zn,t),a(s,ee,t),n(ee,uo),a(s,Yn,t),a(s,_s,t),_s.innerHTML=Jr,a(s,Xn,t),a(s,R,t),n(R,O),n(O,Qe),n(R,ko),a(s,$n,t),a(s,F,t),n(F,B),n(B,Ze),n(F,fo),a(s,Kn,t),a(s,ne,t),n(ne,ho),a(s,st,t),a(s,H,t),n(H,Ye),n(Ye,mo),n(H,wo),n(H,Xe),n(Xe,_o),a(s,et,t),a(s,te,t),n(te,yo),a(s,nt,t),a(s,x,t),n(x,bo),n(x,ys),n(ys,vo),n(x,go),n(x,bs),n(bs,xo),n(x,Eo),a(s,tt,t),a(s,vs,t),vs.innerHTML=Qr,a(s,at,t),a(s,ae,t),n(ae,Io),a(s,ot,t),a(s,gs,t),gs.innerHTML=Zr,a(s,pt,t),a(s,oe,t),n(oe,To),a(s,lt,t),a(s,xs,t),xs.innerHTML=Yr,a(s,it,t),a(s,q,t),n(q,M),n(M,$e),n(q,Ao),a(s,rt,t),a(s,pe,t),n(pe,So),a(s,ct,t),a(s,_,t),n(_,Ke),n(Ke,Po),n(_,Co),n(_,sn),n(sn,zo),n(_,Do),n(_,en),n(en,Lo),n(_,Ro),n(_,nn),n(nn,Oo),a(s,ut,t),a(s,le,t),n(le,Fo),a(s,dt,t),a(s,Es,t),Es.innerHTML=Xr,a(s,kt,t),a(s,N,t),n(N,Bo),n(N,tn),n(tn,Ho),n(N,qo),a(s,ft,t),a(s,Is,t),Is.innerHTML=$r,a(s,ht,t),a(s,U,t),n(U,Mo),n(U,ie),n(ie,No),n(U,Uo),a(s,mt,t),a(s,Ts,t),Ts.innerHTML=Kr,a(s,wt,t),a(s,re,t),n(re,Go),a(s,_t,t),a(s,As,t),As.innerHTML=sc,a(s,yt,t),a(s,ce,t),n(ce,jo),a(s,bt,t),a(s,Ss,t),Ss.innerHTML=ec,a(s,vt,t),a(s,G,t),n(G,Vo),n(G,an),n(an,Wo),n(G,Jo),a(s,gt,t),a(s,Ps,t),Ps.innerHTML=nc,a(s,xt,t),a(s,j,t),n(j,Qo),n(j,on),n(on,Zo),n(j,Yo),a(s,Et,t),a(s,Cs,t),Cs.innerHTML=tc,a(s,It,t),a(s,ue,t),n(ue,Xo),a(s,Tt,t),a(s,V,t),n(V,W),n(W,pn),n(V,$o),a(s,At,t),a(s,de,t),n(de,Ko),a(s,St,t),a(s,zs,t),zs.innerHTML=ac,a(s,Pt,t),a(s,J,t),n(J,sp),n(J,Ds),n(Ds,ep),n(J,np),a(s,Ct,t),a(s,Q,t),n(Q,Z),n(Z,ln),n(Q,tp),a(s,zt,t),a(s,ke,t),n(ke,ap),a(s,Dt,t),a(s,y,t),n(y,Ls),n(Ls,op),n(Ls,rn),n(rn,pp),n(Ls,lp),n(y,ip),n(y,Rs),n(Rs,rp),n(Rs,cn),n(cn,cp),n(Rs,up),n(y,dp),n(y,Os),n(Os,kp),n(Os,un),n(un,fp),n(Os,hp),n(y,mp),n(y,dn),n(dn,wp),a(s,Lt,t),a(s,fe,t),n(fe,_p),a(s,Rt,t),a(s,Fs,t),Fs.innerHTML=oc,a(s,Ot,t),a(s,he,t),n(he,yp),a(s,Ft,t),a(s,Bs,t),Bs.innerHTML=pc,a(s,Bt,t),a(s,Y,t),n(Y,bp),n(Y,kn),n(kn,vp),n(Y,gp),a(s,Ht,t),a(s,Hs,t),Hs.innerHTML=lc,a(s,qt,t),a(s,me,t),n(me,xp),a(s,Mt,t),a(s,qs,t),qs.innerHTML=ic,a(s,Nt,t),a(s,we,t),n(we,Ep),a(s,Ut,t),a(s,Ms,t),Ms.innerHTML=rc,a(s,Gt,t),a(s,X,t),n(X,Ip),n(X,fn),n(fn,Tp),n(X,Ap),a(s,jt,t),a(s,Ns,t),Ns.innerHTML=cc,a(s,Vt,t),a(s,_e,t),n(_e,Sp),a(s,Wt,t),a(s,Us,t),Us.innerHTML=uc,a(s,Jt,t),a(s,ye,t),n(ye,Pp),a(s,Qt,t),a(s,Gs,t),Gs.innerHTML=dc,a(s,Zt,t),a(s,be,t),n(be,Cp),a(s,Yt,t),a(s,js,t),js.innerHTML=kc,a(s,Xt,t),a(s,ve,t),n(ve,zp),a(s,$t,t),a(s,Vs,t),Vs.innerHTML=fc,a(s,Kt,t),a(s,$,t),n($,Dp),n($,hn),n(hn,Lp),n($,Rp),a(s,sa,t),a(s,Ws,t),Ws.innerHTML=hc,a(s,ea,t),a(s,ge,t),n(ge,Op),a(s,na,t),a(s,K,t),n(K,ss),n(ss,mn),n(K,Fp),a(s,ta,t),a(s,E,t),n(E,Bp),n(E,wn),n(wn,Hp),n(E,qp),n(E,xe),n(xe,Mp),n(E,Np),a(s,aa,t),a(s,es,t),n(es,ns),n(ns,_n),n(es,Up),a(s,oa,t),a(s,Ee,t),n(Ee,Gp),a(s,pa,t),a(s,Ie,t),n(Ie,jp),a(s,la,t),a(s,Js,t),Js.innerHTML=mc,a(s,ia,t),a(s,Te,t),n(Te,Vp),a(s,ra,t),a(s,Qs,t),Qs.innerHTML=wc,a(s,ca,t),a(s,ts,t),n(ts,Wp),n(ts,yn),n(yn,Jp),n(ts,Qp),a(s,ua,t),a(s,Ae,t),n(Ae,Zp),a(s,da,t),a(s,f,t),n(f,bn),n(bn,Yp),n(f,Xp),n(f,vn),n(vn,$p),n(f,Kp),n(f,Zs),n(Zs,sl),n(Zs,gn),n(gn,el),n(Zs,nl),n(f,tl),n(f,xn),n(xn,al),n(f,ol),n(f,En),n(En,pl),n(f,ll),n(f,In),n(In,il),a(s,ka,t),a(s,as,t),n(as,rl),n(as,Se),n(Se,cl),n(as,ul),a(s,fa,t),a(s,os,t),n(os,ps),n(ps,Tn),n(os,dl),a(s,ha,t),a(s,ls,t),n(ls,kl),n(ls,An),n(An,fl),n(ls,hl),a(s,ma,t),a(s,I,t),n(I,Pe),n(Pe,Sn),n(Sn,ml),n(Pe,wl),n(I,_l),n(I,Ce),n(Ce,Pn),n(Pn,yl),n(Ce,bl),n(I,vl),n(I,ze),n(ze,gl),n(ze,Cn),n(Cn,xl),a(s,wa,t),a(s,k,t),n(k,El),n(k,zn),n(zn,Il),n(k,Tl),n(k,Dn),n(Dn,Al),n(k,Sl),n(k,Ln),n(Ln,Pl),n(k,Cl),n(k,Rn),n(Rn,zl),n(k,Dl),n(k,On),n(On,Ll),n(k,Rl),n(k,Fn),n(Fn,Ol),n(k,Fl),n(k,Bn),n(Bn,Bl),n(k,Hl),a(s,_a,t),a(s,De,t),n(De,Ys),n(Ys,ql),a(s,ya,t),a(s,Le,t),n(Le,Xs),n(Xs,Ml)},p:ni,i:ni,o:ni,d(s){s&&e(v),s&&e(Hn),s&&e($s),s&&e(qn),s&&e(g),s&&e(Mn),s&&e(S),s&&e(Nn),s&&e(w),s&&e(Un),s&&e(hs),s&&e(Gn),s&&e(Ks),s&&e(jn),s&&e(C),s&&e(Vn),s&&e(m),s&&e(Wn),s&&e(D),s&&e(Jn),s&&e(ms),s&&e(Qn),s&&e(ws),s&&e(Zn),s&&e(ee),s&&e(Yn),s&&e(_s),s&&e(Xn),s&&e(R),s&&e($n),s&&e(F),s&&e(Kn),s&&e(ne),s&&e(st),s&&e(H),s&&e(et),s&&e(te),s&&e(nt),s&&e(x),s&&e(tt),s&&e(vs),s&&e(at),s&&e(ae),s&&e(ot),s&&e(gs),s&&e(pt),s&&e(oe),s&&e(lt),s&&e(xs),s&&e(it),s&&e(q),s&&e(rt),s&&e(pe),s&&e(ct),s&&e(_),s&&e(ut),s&&e(le),s&&e(dt),s&&e(Es),s&&e(kt),s&&e(N),s&&e(ft),s&&e(Is),s&&e(ht),s&&e(U),s&&e(mt),s&&e(Ts),s&&e(wt),s&&e(re),s&&e(_t),s&&e(As),s&&e(yt),s&&e(ce),s&&e(bt),s&&e(Ss),s&&e(vt),s&&e(G),s&&e(gt),s&&e(Ps),s&&e(xt),s&&e(j),s&&e(Et),s&&e(Cs),s&&e(It),s&&e(ue),s&&e(Tt),s&&e(V),s&&e(At),s&&e(de),s&&e(St),s&&e(zs),s&&e(Pt),s&&e(J),s&&e(Ct),s&&e(Q),s&&e(zt),s&&e(ke),s&&e(Dt),s&&e(y),s&&e(Lt),s&&e(fe),s&&e(Rt),s&&e(Fs),s&&e(Ot),s&&e(he),s&&e(Ft),s&&e(Bs),s&&e(Bt),s&&e(Y),s&&e(Ht),s&&e(Hs),s&&e(qt),s&&e(me),s&&e(Mt),s&&e(qs),s&&e(Nt),s&&e(we),s&&e(Ut),s&&e(Ms),s&&e(Gt),s&&e(X),s&&e(jt),s&&e(Ns),s&&e(Vt),s&&e(_e),s&&e(Wt),s&&e(Us),s&&e(Jt),s&&e(ye),s&&e(Qt),s&&e(Gs),s&&e(Zt),s&&e(be),s&&e(Yt),s&&e(js),s&&e(Xt),s&&e(ve),s&&e($t),s&&e(Vs),s&&e(Kt),s&&e($),s&&e(sa),s&&e(Ws),s&&e(ea),s&&e(ge),s&&e(na),s&&e(K),s&&e(ta),s&&e(E),s&&e(aa),s&&e(es),s&&e(oa),s&&e(Ee),s&&e(pa),s&&e(Ie),s&&e(la),s&&e(Js),s&&e(ia),s&&e(Te),s&&e(ra),s&&e(Qs),s&&e(ca),s&&e(ts),s&&e(ua),s&&e(Ae),s&&e(da),s&&e(f),s&&e(ka),s&&e(as),s&&e(fa),s&&e(os),s&&e(ha),s&&e(ls),s&&e(ma),s&&e(I),s&&e(wa),s&&e(k),s&&e(_a),s&&e(De),s&&e(ya),s&&e(Le)}}}const Yc={excerpt:"Textbooks are good for learning grammar, but their exercises tend to be too limited. Doing 10 or 12 exercises once is a good start but does not come anywhere near what I need. Which is hundreds of exercises over the course of days, weeks even.",date:"2022-09-26T00:00:00.000Z",draft:!0,archived:!1,title:"A simple Python app for German grammar exercises, with SpaCy",tags:["german","python","spacy"]};class Xc extends Vc{constructor(v){super(),Wc(this,v,null,Qc,Jc,{})}}export{Xc as default,Yc as metadata};
