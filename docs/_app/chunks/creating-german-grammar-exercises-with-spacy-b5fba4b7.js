import{S as ld,i as id,s as rd,e as o,t as i,k as c,c as p,a as l,d as e,h as r,m as u,b as k,g as t,I as n,E as Cr}from"./index-6e518972.js";function cd(Hu){let x,C,fn,Go,_a,_e,Wo,va,E,Uo,hn,Vo,Ko,_s,Jo,Qo,ba,L,R,mn,Zo,ga,w,Xo,vs,Yo,$o,bs,sp,ep,gs,np,ap,xa,xs,Nu=`<code class="language-txt">foo foo [nach dem] bar bar => foo foo [xxxx dxxx] bar bar
foo foo [in diesem]  bar bar => foo foo [xxxxx diesxxx] bar bar
foo foo [mit einem] bar bar => foo foo [xxxx einxxx] bar bar</code>`,Ea,ve,tp,Ia,D,M,wn,op,Ta,f,yn,pp,lp,_n,ip,rp,vn,cp,up,bn,kp,dp,gn,fp,Pa,q,F,xn,hp,Aa,Es,mp,be,wp,Sa,Is,ju=`<code class="language-bash"><span class="token operator">></span> cookiecutter gh:gotofritz/cookiecutter-gotofritz-poetry
You've downloaded /Users/fritz/.cookiecutters/cookiecutter-gotofritz-poetry before. Is it okay to delete and re-download it? <span class="token punctuation">[</span>yes<span class="token punctuation">]</span>:
project_name <span class="token punctuation">[</span>new-project<span class="token punctuation">]</span>: german-learning
package_name <span class="token punctuation">[</span>germanlearning<span class="token punctuation">]</span>:
verbose_project_name <span class="token punctuation">[</span>My Awesome Project<span class="token punctuation">]</span>: German Language Drills
full_name <span class="token punctuation">[</span>Your Name<span class="token punctuation">]</span>: gotofritz
github_username <span class="token punctuation">[</span>github_username<span class="token punctuation">]</span>: gotofritz
mastodon_handle <span class="token punctuation">[</span>@your_name@mastodon.social<span class="token punctuation">]</span>: @gotofritz@mastodon.social
mastodon_url <span class="token punctuation">[</span>https://mastodon.social/@your_name<span class="token punctuation">]</span>: https://mastodon.social/@gotofritz
project_description <span class="token punctuation">[</span>this is a project<span class="token punctuation">]</span>: Some basic German language drills
python_version <span class="token punctuation">[</span><span class="token number">3.10</span>.4<span class="token punctuation">]</span>: <span class="token number">3.10</span>.6
<span class="token number">3.10</span>.6
Updating dependencies
Resolving dependencies<span class="token punctuation">..</span>. <span class="token punctuation">(</span><span class="token number">0</span>.4s<span class="token punctuation">)</span>
<span class="token punctuation">..</span>.</code>`,za,ge,yp,Ca,Ts,Bu=`<code class="language-bash"><span class="token operator">></span> <span class="token function">mkdir</span> data
<span class="token operator">></span> <span class="token function">cp</span> ~/Downloads/articles.csv data/
<span class="token operator">></span> tree <span class="token builtin class-name">.</span>
<span class="token builtin class-name">.</span>
\u251C\u2500\u2500 CHANGELOG.md
\u251C\u2500\u2500 LICENSE.md
\u251C\u2500\u2500 Makefile
\u251C\u2500\u2500 README.md
\u251C\u2500\u2500 data
\u2502   \u2514\u2500\u2500 articles.csv
\u251C\u2500\u2500 <span class="token function">mkdir</span>
\u251C\u2500\u2500 poetry.lock
\u251C\u2500\u2500 pyproject.toml
\u251C\u2500\u2500 src
\u2502   \u2514\u2500\u2500 germanlearning
\u2502       \u251C\u2500\u2500 __init__.py
\u2502       \u2514\u2500\u2500 __pycache__
\u2514\u2500\u2500 tests
    \u251C\u2500\u2500 __init__.py
    \u251C\u2500\u2500 __pycache__
    \u251C\u2500\u2500 conftest.py
    \u2514\u2500\u2500 test_setup.py</code>`,La,H,N,En,_p,Ra,j,B,In,vp,Da,xe,bp,Ma,O,Tn,gp,xp,Pn,Ep,qa,Ee,Ip,Fa,I,Tp,Ps,Pp,Ap,As,Sp,zp,Ha,Ss,Ou=`<code class="language-bash"><span class="token operator">></span> poetry <span class="token function">add</span> spacy
Using version ^3.4.1 <span class="token keyword">for</span> spacy
<span class="token punctuation">..</span>.
<span class="token operator">></span> python <span class="token parameter variable">-m</span> spacy download de_core_news_sm
Collecting de-core-news-sm<span class="token operator">==</span><span class="token number">3.4</span>.0
<span class="token punctuation">..</span>.</code>`,Na,Ie,Cp,ja,zs,Gu=`<code class="language-python"><span class="token comment"># scripts/convert_raw_csv_to_something_usable.py</span>
<span class="token triple-quoted-string string">"""Converts a raw document with a full article per line to one split into sentences

Usage: python scripts/convert_raw_csv_to_something_usable.py data/articles.csv data/docs.tsv
"""</span>

<span class="token keyword">import</span> re
<span class="token keyword">import</span> sys
<span class="token keyword">from</span> pathlib <span class="token keyword">import</span> Path

<span class="token keyword">import</span> spacy

MIN_WORDS_IN_SENTENCE <span class="token operator">=</span> <span class="token number">3</span>


<span class="token keyword">def</span> <span class="token function">die</span><span class="token punctuation">(</span>msg<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""print an error message and exit"""</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>msg<span class="token punctuation">)</span>
    exit<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>


<span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>sys<span class="token punctuation">.</span>argv<span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token number">3</span><span class="token punctuation">:</span>
    die<span class="token punctuation">(</span><span class="token string">"Expecting two arguments: path to input file and path to output file"</span><span class="token punctuation">)</span>

input_path <span class="token operator">=</span> Path<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>argv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">if</span> <span class="token keyword">not</span> input_path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    die<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Path doesn't exist </span><span class="token interpolation"><span class="token punctuation">&#123;</span>input_path<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>

output_path <span class="token operator">=</span> Path<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>argv<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">if</span> <span class="token keyword">not</span> output_path<span class="token punctuation">.</span>parent<span class="token punctuation">.</span>exists<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    die<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Path doesn't exist </span><span class="token interpolation"><span class="token punctuation">&#123;</span>output_path<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
<span class="token keyword">if</span> output_path<span class="token punctuation">.</span>suffix <span class="token operator">!=</span> <span class="token string">".tsv"</span><span class="token punctuation">:</span>
    die<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Path should be a tsv file </span><span class="token interpolation"><span class="token punctuation">&#123;</span>output_path<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>

<span class="token comment"># loads the fast model for German</span>
nlp <span class="token operator">=</span> spacy<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"de_core_news_sm"</span><span class="token punctuation">)</span>

<span class="token comment"># ARTICLES_FILE is a CSV with two semicolon separated fields; I only care about the second.</span>
<span class="token comment"># Also remove any tabs, as they will interfere with</span>
noise_remover <span class="token operator">=</span> re<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span><span class="token string">r"^.+?;|&#92;t+"</span><span class="token punctuation">)</span>
<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>input_path<span class="token punctuation">,</span> <span class="token string">"r"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> csvfile<span class="token punctuation">:</span>
    docs <span class="token operator">=</span> <span class="token punctuation">[</span>noise_remover<span class="token punctuation">.</span>sub<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">,</span> line<span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> line <span class="token keyword">in</span> csvfile<span class="token punctuation">]</span>

output_lines <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> doc <span class="token keyword">in</span> docs<span class="token punctuation">:</span>
    <span class="token comment"># nlp(doc).sent splits a doc into sentences</span>
    sents <span class="token operator">=</span> <span class="token punctuation">[</span>sent<span class="token punctuation">.</span>text <span class="token keyword">for</span> sent <span class="token keyword">in</span> nlp<span class="token punctuation">(</span>doc<span class="token punctuation">)</span><span class="token punctuation">.</span>sents <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>sent<span class="token punctuation">)</span> <span class="token operator">>=</span> MIN_WORDS_IN_SENTENCE<span class="token punctuation">]</span>
    output_lines<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">"&#92;t"</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>sents<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">"&#92;n"</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>output_lines<span class="token punctuation">)</span>
<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>output_path<span class="token punctuation">,</span> <span class="token string">"w"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> csvfile<span class="token punctuation">:</span>
    csvfile<span class="token punctuation">.</span>writelines<span class="token punctuation">(</span>output_lines<span class="token punctuation">)</span></code>`,Ba,Te,Lp,Oa,Cs,Wu=`<code class="language-bash"><span class="token operator">></span> python scripts/convert_raw_csv_to_something_usable.py data/articles.csv data/docs.tsv

<span class="token comment"># look at first line of source</span>
\u276F <span class="token function">head</span> <span class="token parameter variable">-n1</span> data/articles.csv  <span class="token operator">|</span>  <span class="token function">fold</span> <span class="token parameter variable">-w</span> <span class="token number">64</span>
Etat<span class="token punctuation">;</span>Die ARD-Tochter Degeto hat sich verpflichtet, ab August ein
er Quotenregelung zu folgen, die f\xFCr die Gleichstellung von Regi
sseurinnen sorgen soll. In mindestens <span class="token number">20</span> Prozent der Filme, die
die ARD-Tochter Degeto produziert oder mitfinanziert, sollen ab
Mitte August Frauen Regie f\xFChren. Degeto-Chefin Christine Strobl
 folgt mit dieser Selbstverpflichtung der Forderung von Pro Quot
e Regie. Die Vereinigung von Regisseurinnen hatte im vergangenen
 Jahr eine Quotenregelung gefordert, um den weiblichen Filmschaf
fenden mehr Geh\xF6r und \xF6konomische Gleichstellung zu verschaffen.
 Pro Quote Regie kritisiert, dass, w\xE4hrend rund <span class="token number">50</span> Prozent der R
egie-Studierenden weiblich seien, der Anteil der Regisseurinnen
bei Fernsehfilmen nur bei <span class="token number">13</span> bis <span class="token number">15</span> Prozent liege. In \xD6sterreich
 sieht die Situation \xE4hnlich aus, auch hier wird von unterschied
lichen Seiten Handlungsbedarf angemahnt. Aber wie soll dieser au
ssehen? Ist die Einf\xFChrung der Quotenregelung auch f\xFCr die \xF6ster
reichische Film- und Fernsehlandschaft sinnvoll? Diskutieren Sie
 im Forum.

<span class="token comment"># Try to see whether it has split them into fields</span>
<span class="token operator">></span> <span class="token function">head</span> <span class="token parameter variable">-n1</span> data/docs.tsv  <span class="token operator">|</span>  <span class="token function">awk</span> -F<span class="token punctuation"></span>t <span class="token string">'&#123; print $2 &#125;'</span> <span class="token operator">|</span> <span class="token function">fold</span> <span class="token parameter variable">-w</span> <span class="token number">64</span>
In mindestens <span class="token number">20</span> Prozent der Filme, die die ARD-Tochter Degeto p
roduziert oder mitfinanziert, sollen ab Mitte August Frauen Regi
e f\xFChren.</code>`,Ga,G,W,An,Rp,Wa,Pe,Dp,Ua,y,Sn,Mp,qp,zn,Fp,Hp,Cn,Np,jp,Ln,Bp,Va,Ae,Op,Ka,Ls,Uu=`<code class="language-python"><span class="token comment"># tests/repositories/tsv_sentence_repository.py</span>
<span class="token keyword">from</span> germanlearning<span class="token punctuation">.</span>repositories<span class="token punctuation">.</span>tsv_sentence_repository <span class="token keyword">import</span> TsvSentenceRepository
<span class="token keyword">from</span> tests<span class="token punctuation">.</span>conftest <span class="token keyword">import</span> is_sentence

<span class="token keyword">def</span> <span class="token function">test_first_sentence_from_random_article</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Happy path for get_sentence"""</span>
    sut <span class="token operator">=</span> TsvSentenceRepository<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">assert</span> is_sentence<span class="token punctuation">(</span>sut<span class="token punctuation">.</span>next_sentence<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code>`,Ja,U,Gp,Rn,Wp,Up,Qa,Rs,Vu=`<code class="language-python"><span class="token comment"># tests/test_utils.py</span>
<span class="token keyword">from</span> tests<span class="token punctuation">.</span>conftest <span class="token keyword">import</span> is_sentence

are_sentences <span class="token operator">=</span> <span class="token builtin">frozenset</span><span class="token punctuation">(</span><span class="token punctuation">[</span>
        <span class="token string">"By default, the Faker generates the data in English."</span><span class="token punctuation">,</span>
        <span class="token string">"What if you - want localized data?"</span><span class="token punctuation">,</span>
        <span class="token string">"There are two: different; types of provides!"</span><span class="token punctuation">,</span>
    <span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">test_is_sentence_ends_with_punctuation</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""True if it ends with ?.! and false otherwise"""</span>
    <span class="token keyword">assert</span> <span class="token builtin">all</span><span class="token punctuation">(</span>is_sentence<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span> <span class="token keyword">for</span> sentence <span class="token keyword">in</span> are_sentences<span class="token punctuation">)</span>

    should_not_pass <span class="token operator">=</span> <span class="token punctuation">[</span>sentence<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token keyword">for</span> sentence <span class="token keyword">in</span> are_sentences<span class="token punctuation">]</span>
    <span class="token keyword">assert</span> <span class="token keyword">not</span> <span class="token builtin">any</span><span class="token punctuation">(</span>is_sentence<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span> <span class="token keyword">for</span> sentence <span class="token keyword">in</span> should_not_pass<span class="token punctuation">)</span></code>`,Za,V,Vp,Se,Kp,Jp,Xa,Ds,Ku=`<code class="language-python"><span class="token keyword">class</span> <span class="token class-name">TsvSentenceRepository</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Simple TSV based sentence repository

    Reads sentences from disk storage and allows a consumer to request
    the next one

    next_sentence: returns the next unread sentence from current doc

    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Read document source and stores all line boundaries"""</span>

    <span class="token keyword">def</span> <span class="token function">next_sentence</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">str</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Return a sentence from the data store"""</span>
        <span class="token keyword">return</span> <span class="token string">""</span></code>`,Ya,ze,Qp,$a,Ms,Ju=`<code class="language-bash"><span class="token operator">></span>       assert is_sentence<span class="token punctuation">(</span>sut.get_sentence<span class="token punctuation">(</span><span class="token punctuation">))</span>
E       AssertionError: assert False
E        +  where False <span class="token operator">=</span> is_sentence<span class="token punctuation">(</span><span class="token string">''</span><span class="token punctuation">)</span></code>`,st,Ce,Zp,et,qs,Qu=`<code class="language-python"><span class="token comment"># src/germanlearning/config.py</span>

<span class="token keyword">from</span> pathlib <span class="token keyword">import</span> Path

ROOT_DIR <span class="token operator">=</span> Path<span class="token punctuation">.</span>cwd<span class="token punctuation">(</span><span class="token punctuation">)</span>
DATA_DIR <span class="token operator">=</span> ROOT_DIR <span class="token operator">/</span> <span class="token string">"data"</span>
DOCS_FILE <span class="token operator">=</span> DATA_DIR <span class="token operator">/</span> <span class="token string">"docs.tsv"</span></code>`,nt,K,Xp,Dn,Yp,$p,at,Fs,Zu=`<code class="language-python"><span class="token keyword">from</span> typing <span class="token keyword">import</span> List

<span class="token keyword">from</span> germanlearning<span class="token punctuation">.</span>config <span class="token keyword">import</span> DOCS_FILE

SENTENCES_SEPARATOR <span class="token operator">=</span> <span class="token string">"&#92;t"</span>


<span class="token keyword">class</span> <span class="token class-name">TsvSentenceRepository</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Simple TSV based sentence repository

    Reads sentences from disk storage and allows a consumer to request
    the next one

    fetch_doc: reads a doc's list of sentences from storage
    next_sentence: returns the next unread sentence from current doc

    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Read document source and stores all line boundaries"""</span>
        self<span class="token punctuation">.</span>doc_boundaries<span class="token punctuation">:</span> List<span class="token punctuation">[</span><span class="token builtin">int</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>current<span class="token punctuation">:</span> List<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

        <span class="token comment"># SIM115: file kept open on purpose to minimise I/O activity</span>
        self<span class="token punctuation">.</span>docs <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span>DOCS_FILE<span class="token punctuation">,</span> <span class="token string">"rt"</span><span class="token punctuation">)</span>  <span class="token comment"># noqa SIM115</span>
        <span class="token keyword">while</span> self<span class="token punctuation">.</span>docs<span class="token punctuation">.</span>readline<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>doc_boundaries<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>docs<span class="token punctuation">.</span>tell<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">next_sentence</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">str</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Return a sentence from the data store"""</span>
        self<span class="token punctuation">.</span>_ensure_doc<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>current<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">fetch_doc</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> List<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Use the data boundaries to find a list of sentences for a given line"""</span>
        self<span class="token punctuation">.</span>docs<span class="token punctuation">.</span>seek<span class="token punctuation">(</span>self<span class="token punctuation">.</span>doc_boundaries<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> <span class="token builtin">str</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>docs<span class="token punctuation">.</span>readline<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span>SENTENCES_SEPARATOR<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_ensure_doc</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Ensure there is a current doc"""</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>current<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>current <span class="token operator">=</span> self<span class="token punctuation">.</span>fetch_doc<span class="token punctuation">(</span><span class="token punctuation">)</span></code>`,tt,J,sl,Mn,el,nl,ot,Hs,Xu=`<code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">from</span> germanlearning<span class="token punctuation">.</span>repositories<span class="token punctuation">.</span>tsv_sentence_repository <span class="token keyword">import</span> TsvSentenceRepository
<span class="token operator">>></span><span class="token operator">></span> t <span class="token operator">=</span> TsvSentenceRepository<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span> t<span class="token punctuation">.</span>next_sentence<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token string">'Die ARD-Tochter Degeto hat sich verpflichtet, ab August einer Quotenregelung zu folgen, die f\xFCr die Gleichstellung von Regisseurinnen sorgen soll.'</span>
<span class="token operator">>></span><span class="token operator">></span> t<span class="token punctuation">.</span>next_sentence<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token string">'In mindestens 20 Prozent der Filme, die die ARD-Tochter Degeto produziert oder mitfinanziert, sollen ab Mitte August Frauen Regie f\xFChren.'</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token operator">>></span><span class="token operator">></span> t<span class="token punctuation">.</span>next_sentence<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token string">'Die ARD-Tochter Degeto hat sich verpflichtet, ab August einer Quotenregelung zu folgen, die f\xFCr die Gleichstellung von Regisseurinnen sorgen soll.'</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span></code>`,pt,Le,al,lt,Q,Z,qn,tl,it,Re,ol,rt,Ns,Yu=`<code class="language-txt">foo foo nach  dem  bar bar f\xFCr   einen pop pop
foo foo xxxxx dxxx bar bar xxxxx einxx pop pop</code>`,ct,X,pl,js,ll,il,ut,Y,$,Fn,rl,kt,De,cl,dt,_,Bs,ul,Hn,kl,dl,fl,Os,hl,Nn,ml,wl,yl,Gs,_l,jn,vl,bl,gl,Bn,xl,ft,Me,El,ht,Ws,$u=`<code class="language-python"><span class="token comment"># tests/models/test_cloze_entity.py</span>
<span class="token keyword">from</span> germanlearning<span class="token punctuation">.</span>models <span class="token keyword">import</span> Cloze

<span class="token keyword">def</span> <span class="token function">test_new_cloze_returns_obfuscated</span><span class="token punctuation">(</span>fake<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""When a cloze is created, it will return the obfuscated text in string context"""</span>
    text <span class="token operator">=</span> fake<span class="token punctuation">.</span>pystr<span class="token punctuation">(</span><span class="token punctuation">)</span>
    obfuscated <span class="token operator">=</span> fake<span class="token punctuation">.</span>pystr<span class="token punctuation">(</span><span class="token punctuation">)</span>
    sut <span class="token operator">=</span> Cloze<span class="token punctuation">(</span>text<span class="token operator">=</span>text<span class="token punctuation">,</span> obfuscated<span class="token operator">=</span>obfuscated<span class="token punctuation">)</span>
    <span class="token keyword">assert</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>sut<span class="token punctuation">&#125;</span></span><span class="token string"> 123"</span></span> <span class="token operator">==</span> <span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>obfuscated<span class="token punctuation">&#125;</span></span><span class="token string"> 123"</span></span></code>`,mt,qe,Il,wt,Us,sk=`<code class="language-bash"><span class="token operator">></span> poetry <span class="token function">add</span> pydantic@^1.7
Updating dependencies
Resolving dependencies<span class="token punctuation">..</span>. <span class="token punctuation">(</span><span class="token number">0</span>.3s<span class="token punctuation">)</span>
<span class="token punctuation">..</span>.</code>`,yt,ss,Tl,On,Pl,Al,_t,Vs,ek=`<code class="language-python"><span class="token comment"># src/models/__init__.py</span>
<span class="token keyword">from</span> <span class="token punctuation">.</span>cloze_entity <span class="token keyword">import</span> Cloze</code>`,vt,Fe,Sl,bt,Ks,nk=`<code class="language-python"><span class="token comment"># src/models/cloze_entity.py</span>
<span class="token keyword">from</span> pydantic <span class="token keyword">import</span> BaseModel<span class="token punctuation">,</span> Field


<span class="token keyword">class</span> <span class="token class-name">Cloze</span><span class="token punctuation">(</span>BaseModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    text<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> Field<span class="token punctuation">(</span>description<span class="token operator">=</span><span class="token string">"The text to be guessed"</span><span class="token punctuation">)</span>
    obfuscated<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> Field<span class="token punctuation">(</span>description<span class="token operator">=</span><span class="token string">"Shows the text as"</span><span class="token punctuation">)</span></code>`,gt,He,zl,xt,Js,ak=`<code class="language-bash">\u276F <span class="token function">make</span> <span class="token builtin class-name">test</span>
poetry run pytest tests
<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span> <span class="token builtin class-name">test</span> session starts <span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span>
<span class="token punctuation">..</span>.
<span class="token operator">></span>       assert sut <span class="token operator">==</span> obfuscated
E       AssertionError: assert Cloze<span class="token punctuation">(</span>text<span class="token operator">=</span><span class="token string">'FdscvPGgwNGEUxZCvWnj'</span>, <span class="token assign-left variable">obfuscated</span><span class="token operator">=</span><span class="token string">'ZGErTPzfeNSnbSFvvtPR'</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token string">'ZGErTPzfeNSnbSFvvtPR'</span></code>`,Et,es,Cl,Gn,Ll,Rl,It,Qs,tk=`<code class="language-diff">class Cloze(BaseModel):
<span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   text: str = Field(description="The text to be guessed")
</span><span class="token prefix unchanged"> </span><span class="token line">   obfuscated: str = Field(description="Shows the text as")
</span></span>
<span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    def __str__(self) -> str:
</span><span class="token prefix inserted">+</span><span class="token line">        """In string context it should return the string that reflects its state"""
</span><span class="token prefix inserted">+</span><span class="token line">        return self.text if self.guessed is True else self.obfuscated</span></span></code>`,Tt,Ne,Dl,Pt,Zs,ok=`<code class="language-python"><span class="token comment"># tests/models/test_cloze_entity.py</span>

<span class="token keyword">def</span> <span class="token function">test_cloze_guess</span><span class="token punctuation">(</span>fake<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""When you guess the state can change"""</span>
    text <span class="token operator">=</span> fake<span class="token punctuation">.</span>pystr<span class="token punctuation">(</span>count<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">)</span>
    obfuscated <span class="token operator">=</span> fake<span class="token punctuation">.</span>pystr<span class="token punctuation">(</span><span class="token punctuation">)</span>
    sut <span class="token operator">=</span> Cloze<span class="token punctuation">(</span>text<span class="token operator">=</span>text<span class="token punctuation">,</span> obfuscated<span class="token operator">=</span>obfuscated<span class="token punctuation">)</span>
    <span class="token keyword">assert</span> sut<span class="token punctuation">.</span>guessed <span class="token keyword">is</span> <span class="token boolean">False</span>
    <span class="token keyword">assert</span> sut<span class="token punctuation">.</span>guess<span class="token punctuation">(</span>fake<span class="token punctuation">.</span>pystr<span class="token punctuation">(</span>count<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">is</span> <span class="token boolean">False</span>
    <span class="token keyword">assert</span> sut<span class="token punctuation">.</span>guessed <span class="token keyword">is</span> <span class="token boolean">False</span>
    <span class="token keyword">assert</span> sut<span class="token punctuation">.</span>guess<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
    <span class="token keyword">assert</span> sut<span class="token punctuation">.</span>guessed <span class="token keyword">is</span> <span class="token boolean">True</span></code>`,At,je,Ml,St,Xs,pk=`<code class="language-diff">class Cloze(BaseModel):
<span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   text: str = Field(description="The text to be guessed")
</span><span class="token prefix unchanged"> </span><span class="token line">   obfuscated: str = Field(description="Shows the text as")
</span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    guessed = False
</span></span>
<span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   def __str__(self) -> str:
</span><span class="token prefix unchanged"> </span><span class="token line">       """In string context it should return the string that reflects its state"""
</span><span class="token prefix unchanged"> </span><span class="token line">       return self.text if self.guessed is True else self.obfuscated
</span></span>
<span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    def guess(self, candidate: str) -> bool:
</span><span class="token prefix inserted">+</span><span class="token line">        """If the candidate str is the same as the text, it's guessed"""
</span><span class="token prefix inserted">+</span><span class="token line">        self.guessed = candidate == self.text
</span><span class="token prefix inserted">+</span><span class="token line">        return self.guessed</span></span></code>`,zt,Be,ql,Ct,Ys,lk=`<code class="language-diff">def test_new_cloze_returns_obfuscated(fake):
<span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   """When a cloze is created, it will return the obfuscated text in string context"""
</span><span class="token prefix unchanged"> </span><span class="token line">   text = fake.pystr()
</span><span class="token prefix unchanged"> </span><span class="token line">   obfuscated = fake.pystr()
</span><span class="token prefix unchanged"> </span><span class="token line">   sut = Cloze(text=text, obfuscated=obfuscated)
</span><span class="token prefix unchanged"> </span><span class="token line">   assert f"&#123;sut&#125; 123" == f"&#123;obfuscated&#125; 123"
</span></span>
<span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    sut.guess(fake.pystr())
</span><span class="token prefix inserted">+</span><span class="token line">    assert f"&#123;sut&#125; 123" == f"&#123;obfuscated&#125; 123"
</span></span>
<span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    sut.guess(text)
</span><span class="token prefix inserted">+</span><span class="token line">    assert f"&#123;sut&#125; 123" == f"&#123;text&#125; 123"</span></span></code>`,Lt,Oe,Fl,Rt,$s,ik=`<code class="language-python"><span class="token comment"># tests/models/test_cloze_entity.py</span>

<span class="token keyword">def</span> <span class="token function">test_cloze_has_unique_id</span><span class="token punctuation">(</span>fake<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Like all entities, a Cloze has an immutable, unique ID"""</span>
    sut <span class="token operator">=</span> Cloze<span class="token punctuation">(</span>text<span class="token operator">=</span>fake<span class="token punctuation">.</span>pystr<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> obfuscated<span class="token operator">=</span>fake<span class="token punctuation">.</span>pystr<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    immutable_id <span class="token operator">=</span> sut<span class="token punctuation">.</span><span class="token builtin">id</span>
    <span class="token keyword">assert</span> immutable_id

    <span class="token keyword">with</span> pytest<span class="token punctuation">.</span>raises<span class="token punctuation">(</span>ValueError<span class="token punctuation">,</span> <span class="token keyword">match</span><span class="token operator">=</span><span class="token string">'"Cloze" object has no field "id"'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        sut<span class="token punctuation">.</span><span class="token builtin">id</span> <span class="token operator">=</span> fake<span class="token punctuation">.</span>uuid4<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># type: ignore</span>

    <span class="token keyword">assert</span> sut<span class="token punctuation">.</span><span class="token builtin">id</span> <span class="token operator">==</span> immutable_id</code>`,Dt,ns,Hl,Wn,Nl,jl,Mt,se,rk=`<code class="language-diff"># src/models/cloze_entity.py

class Cloze(BaseModel):
<span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   text: str = Field(description="The text to be guessed")
</span><span class="token prefix unchanged"> </span><span class="token line">   obfuscated: str = Field(description="Shows the text as")
</span><span class="token prefix unchanged"> </span><span class="token line">   guessed: bool = Field(False, read_only=True)
</span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    _id: UUID = uuid4()
</span></span>
<span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    @property
</span><span class="token prefix inserted">+</span><span class="token line">    def id(self):
</span><span class="token prefix inserted">+</span><span class="token line">        """This is read only"""
</span><span class="token prefix inserted">+</span><span class="token line">        return self._id
</span></span>
<span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">    @id.setter
</span><span class="token prefix deleted">-</span><span class="token line">    def id(self, whatever):
</span><span class="token prefix deleted">-</span><span class="token line">        """You don't need this in this case"""</span></span></code>`,qt,Ge,Bl,Ft,as,ts,Un,Ol,Ht,T,Gl,Vn,Wl,Ul,We,Vl,Kl,Nt,os,ps,Kn,Jl,jt,Ue,Ql,Bt,Ve,Zl,Ot,ee,ck=`<code class="language-python"><span class="token comment"># src/models/base_entity.py</span>
<span class="token triple-quoted-string string">"""Represent a very basic DDD entity"""</span>

<span class="token keyword">from</span> uuid <span class="token keyword">import</span> UUID<span class="token punctuation">,</span> uuid4

<span class="token keyword">from</span> pydantic <span class="token keyword">import</span> BaseModel

<span class="token keyword">class</span> <span class="token class-name">BaseEntity</span><span class="token punctuation">(</span>BaseModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Represents a very basic DDD entity

    An entity has an immutable ID, and then some ValueObjects.
    """</span>

    _id<span class="token punctuation">:</span> UUID <span class="token operator">=</span> uuid4<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token decorator annotation punctuation">@property</span>
    <span class="token keyword">def</span> <span class="token function">id</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""This is read only"""</span>  <span class="token comment"># noqa D401</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>_id</code>`,Gt,Ke,Xl,Wt,ne,uk=`<code class="language-diff"># src/models/cloze_entity.py

<span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">class Cloze(BaseModel):
</span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">class Cloze(BaseEntity):
</span></span><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   text: str = Field(description="The text to be guessed")
</span><span class="token prefix unchanged"> </span><span class="token line">   obfuscated: str = Field(description="Shows the text as")
</span><span class="token prefix unchanged"> </span><span class="token line">   guessed: bool = Field(False, read_only=True)
</span></span><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">    _id: UUID = uuid4()
</span></span>
<span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">    @property
</span><span class="token prefix deleted">-</span><span class="token line">    def id(self):
</span><span class="token prefix deleted">-</span><span class="token line">        """This is read only"""
</span><span class="token prefix deleted">-</span><span class="token line">        return self._id
</span></span></code>`,Ut,ls,Yl,Jn,$l,si,Vt,Je,ei,Kt,d,Qn,ni,ai,Zn,ti,oi,ae,pi,Xn,li,ii,ri,Yn,ci,ui,$n,ki,di,sa,fi,Jt,is,hi,Qe,mi,wi,Qt,rs,cs,ea,yi,Zt,Ze,_i,Xt,P,na,vi,bi,aa,gi,xi,ta,Ei,Yt,Xe,Ii,$t,te,kk=`<code class="language-python"><span class="token comment"># tests/services/guess_preposition_exercise_service.py</span>

<span class="token keyword">from</span> germanlearning<span class="token punctuation">.</span>models<span class="token punctuation">.</span>exercise <span class="token keyword">import</span> Exercise
<span class="token keyword">from</span> germanlearning<span class="token punctuation">.</span>services<span class="token punctuation">.</span>guess_preposition_exercise_service <span class="token keyword">import</span> <span class="token punctuation">(</span>
    GuessPrepositionExerciseService<span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">test_return_an_exercise_by_default</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    sut <span class="token operator">=</span> GuessPrepositionExerciseService<span class="token punctuation">(</span><span class="token punctuation">)</span>
    exercises_got <span class="token operator">=</span> sut<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">assert</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>exercises_got<span class="token punctuation">,</span> <span class="token builtin">list</span><span class="token punctuation">)</span></code>`,so,h,Ti,oa,Pi,Ai,pa,Si,zi,la,Ci,Li,oe,Ri,Di,eo,Ye,Mi,no,pe,dk=`<code class="language-python"><span class="token comment"># germanlearning/services/guess_preposition_exercise_service.py</span>

<span class="token keyword">class</span> <span class="token class-name">GuessPrepositionExerciseService</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Fetch raw sentences from Repository and generate Exercises on demand"""</span>

    <span class="token keyword">def</span> <span class="token function">get</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Provide exercises on demand"""</span>
        <span class="token keyword">return</span> <span class="token string">""</span></code>`,ao,$e,qi,to,le,fk=`<code class="language-diff"><span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   def test_return_an_exercise_by_default():
</span><span class="token prefix unchanged"> </span><span class="token line">       sut = GuessPrepositionExerciseService()
</span><span class="token prefix unchanged"> </span><span class="token line">       exercises_got = sut.get()
</span></span><span class="token inserted-arrow inserted"><span class="token prefix inserted">></span><span class="token line">       assert isinstance(exercises_got, Exercise)
</span></span>E       AssertionError: assert False
E        +  where False = isinstance('', Exercise)</code>`,oo,sn,Fi,po,ie,hk=`<code class="language-python"><span class="token comment"># germanlearning/services/guess_preposition_exercise_service.py</span>

<span class="token keyword">class</span> <span class="token class-name">GuessPrepositionExerciseService</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Fetch raw sentences from Repository and generate Exercises on demand"""</span>

    <span class="token keyword">def</span> <span class="token function">get</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Provide exercises on demand"""</span>
        <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token punctuation">]</span></code>`,lo,en,Hi,io,re,mk=`<code class="language-python"><span class="token comment"># tests/services/guess_preposition_exercise_service.py</span>

<span class="token keyword">def</span> <span class="token function">test_can_inject_a_repo_in_service</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Use dependency injection to push repo into service"""</span>

    repo <span class="token operator">=</span> mock<span class="token punctuation">.</span>MagicMock<span class="token punctuation">(</span>spec<span class="token operator">=</span>TsvSentenceRepository<span class="token punctuation">)</span>
    sut <span class="token operator">=</span> GuessPrepositionExerciseService<span class="token punctuation">(</span>repository<span class="token operator">=</span>repo<span class="token punctuation">)</span>
    sut<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span>
    repo<span class="token punctuation">.</span>next_sentence<span class="token punctuation">.</span>assert_called_once<span class="token punctuation">(</span><span class="token punctuation">)</span></code>`,ro,nn,Ni,co,ce,wk=`<code class="language-diff"># germanlearning/services/guess_preposition_exercise_service.py

class GuessPrepositionExerciseService:
<span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   """Fetch raw sentences from Repository and generate Exercises on demand"""
</span></span>
<span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    repository: TsvSentenceRepository | None = Field(
</span><span class="token prefix inserted">+</span><span class="token line">        default_factory=TsvSentenceRepository,
</span><span class="token prefix inserted">+</span><span class="token line">        description="the source of sentences",
</span><span class="token prefix inserted">+</span><span class="token line">    )
</span></span>
<span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">    class Config:
</span><span class="token prefix inserted">+</span><span class="token line">        # this is needed otherwise pydantic will complain
</span><span class="token prefix inserted">+</span><span class="token line">        arbitrary_types_allowed = True
</span></span>
<span class="token unchanged"><span class="token prefix unchanged"> </span><span class="token line">   def get(self):
</span><span class="token prefix unchanged"> </span><span class="token line">       """Provide exercises on demand"""
</span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">        sentence = self.repository.next_sentence()  # type: ignore
</span></span><span class="token deleted-sign deleted"><span class="token prefix deleted">-</span><span class="token line">        return ""
</span></span><span class="token inserted-sign inserted"><span class="token prefix inserted">+</span><span class="token line">        return [sentence]</span></span></code>`,uo,an,ji,ko,ue,yk=`<code class="language-bash"><span class="token operator">></span> <span class="token function">sort</span> <span class="token parameter variable">-R</span> data/docs.tsv <span class="token operator">|</span> <span class="token function">head</span> <span class="token parameter variable">-n1</span> <span class="token operator">|</span> <span class="token function">awk</span> '<span class="token punctuation">&#123;</span>gsub<span class="token punctuation">(</span>/<span class="token punctuation"></span>t/,<span class="token string">"<span class="token entity" title="&#92;n">&#92;n</span>"</span><span class="token punctuation">)</span><span class="token punctuation">;</span> print<span class="token punctuation">&#125;</span>
Kapitalkontrollen bleiben <span class="token keyword">in</span> Kraft \u2013
B\xFCrger bekommen erste Steuererh\xF6hungen zu sp\xFCren \u2013
Neuer Arbeitsminister k\xFCndigt harte Verhandlungen an.
Auch mit Kanzlerin Angela Merkel sei Sch\xE4uble <span class="token keyword">in</span> einem Riesenkonflikt.
<span class="token punctuation">..</span>.
Er wird voraussichtlich <span class="token number">2016</span> von der Zhejiang University <span class="token keyword">in</span> Hangzhou an das Department f\xFCr Molekulare Evolution und Entwicklung der Universit\xE4t Wien wechseln.
<span class="token punctuation">..</span>.</code>`,fo,v,Bi,tn,Oi,Gi,ia,Wi,Ui,ke,Vi,Ki,ho,de,_k=`<code class="language-python"><span class="token comment"># tests/services/test_guess_preposition_exercise_service.py</span>

<span class="token keyword">from</span> tests<span class="token punctuation">.</span>testsupport <span class="token keyword">import</span> sentences_as_arrays <span class="token keyword">as</span> sentences
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

<span class="token decorator annotation punctuation">@pytest<span class="token punctuation">.</span>mark<span class="token punctuation">.</span>parametrize</span><span class="token punctuation">(</span>
    <span class="token string">"sentence"</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span>
        sentences<span class="token punctuation">.</span>with_adp<span class="token punctuation">,</span>
        sentences<span class="token punctuation">.</span>manche<span class="token punctuation">,</span>
        sentences<span class="token punctuation">.</span>diesx<span class="token punctuation">,</span>
        sentences<span class="token punctuation">.</span>with_article_no_prep<span class="token punctuation">,</span>
        sentences<span class="token punctuation">.</span>standard<span class="token punctuation">,</span>
    <span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">test_extract_exercise_starts_with_adp</span><span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Check it works for a variety of sentences that were problematic"""</span>
    repo <span class="token operator">=</span> mock<span class="token punctuation">.</span>Mock<span class="token punctuation">(</span>spec<span class="token operator">=</span>TsvSentenceRepository<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    repo<span class="token punctuation">.</span>next_sentence <span class="token operator">=</span> mock<span class="token punctuation">.</span>Mock<span class="token punctuation">(</span>return_value<span class="token operator">=</span><span class="token string">""</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">)</span>
    sut <span class="token operator">=</span> GuessPrepositionExerciseService<span class="token punctuation">(</span>repository<span class="token operator">=</span>repo<span class="token punctuation">)</span>
    exercises_got <span class="token operator">=</span> sut<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">for</span> <span class="token punctuation">(</span>actual_token<span class="token punctuation">,</span> expected<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>exercises_got<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>tokens<span class="token punctuation">,</span> sentence<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>actual_token<span class="token punctuation">,</span> Cloze<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">assert</span> actual_token<span class="token punctuation">.</span>text <span class="token operator">==</span> expected  <span class="token comment"># type: ignore</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">assert</span> actual_token <span class="token operator">==</span> expected</code>`,mo,us,Ji,ra,Qi,Zi,wo,fe,vk=`<code class="language-python"><span class="token comment"># tests/testsupport/sentences.py</span>
<span class="token triple-quoted-string string">"""Test sentences as array. Each item should be translated to an ExerciseToken"""</span>

standard <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token string">"Auch "</span><span class="token punctuation">,</span>
    <span class="token string">"mit "</span><span class="token punctuation">,</span>
    <span class="token string">"Kanzlerin Angela Merkel sei Sch\xE4uble "</span><span class="token punctuation">,</span>
    <span class="token string">"in einem "</span><span class="token punctuation">,</span>
    <span class="token string">"Riesenkonflikt."</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span>

with_article_no_prep <span class="token operator">=</span> <span class="token punctuation">[</span>
  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span></code>`,yo,m,Xi,ca,Yi,$i,ua,sr,er,ka,nr,ar,da,tr,or,_o,he,bk=`<code class="language-python"><span class="token comment"># germanlearning/services/guess_preposition_exercise_service/guess_preposition_exercise_service.py</span>

<span class="token keyword">class</span> <span class="token class-name">GuessPrepositionExerciseService</span><span class="token punctuation">(</span>BaseModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Fetch raw sentences from Repository and generate Exercises on demand"""</span>

    repository<span class="token punctuation">:</span> TsvSentenceRepository <span class="token operator">|</span> <span class="token boolean">None</span> <span class="token operator">=</span> Field<span class="token punctuation">(</span>
        default_factory<span class="token operator">=</span>TsvSentenceRepository<span class="token punctuation">,</span>
        description<span class="token operator">=</span><span class="token string">"the source of sentences"</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
    _nlp<span class="token punctuation">:</span> Language <span class="token operator">=</span> PrivateAttr<span class="token punctuation">(</span>default_factory<span class="token operator">=</span><span class="token keyword">lambda</span><span class="token punctuation">:</span> spacy<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"de_core_news_sm"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">class</span> <span class="token class-name">Config</span><span class="token punctuation">:</span>  <span class="token comment"># noqa: D106, WPS431</span>
        arbitrary_types_allowed <span class="token operator">=</span> <span class="token boolean">True</span>

    <span class="token keyword">def</span> <span class="token function">get</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> how_many<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> List<span class="token punctuation">[</span>Exercise<span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Provide exercises on demand"""</span>
        collected_exercises<span class="token punctuation">:</span> List<span class="token punctuation">[</span>Exercise<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">while</span> <span class="token builtin">len</span><span class="token punctuation">(</span>collected_exercises<span class="token punctuation">)</span> <span class="token operator">&lt;</span> how_many<span class="token punctuation">:</span>
            sentence <span class="token operator">=</span> self<span class="token punctuation">.</span>repository<span class="token punctuation">.</span>next_sentence<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># type: ignore</span>
            <span class="token keyword">if</span> sentence <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                <span class="token keyword">raise</span> TypeError<span class="token punctuation">(</span><span class="token string">"No sentences available"</span><span class="token punctuation">)</span>
            tokens <span class="token operator">=</span> self<span class="token punctuation">.</span>_parse<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span>
            collected_exercises<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Exercise<span class="token punctuation">(</span>tokens<span class="token operator">=</span>tokens<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> collected_exercises

    <span class="token keyword">def</span> <span class="token function">_parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sentence<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> List<span class="token punctuation">[</span>ExerciseToken<span class="token punctuation">]</span><span class="token punctuation">:</span>
        parser <span class="token operator">=</span> TokenParser<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> parser<span class="token punctuation">.</span>parse<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_nlp<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># type: ignore</span></code>`,vo,on,pr,bo,me,gk=`<code class="language-python"><span class="token keyword">class</span> <span class="token class-name">TokenParser</span><span class="token punctuation">:</span>  <span class="token comment"># noqa: WPS214</span>
    <span class="token triple-quoted-string string">"""Parse nlp tokens generated by Spacy and generate events.

    Inspired by SAX parsers
    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> nlp_tokens<span class="token punctuation">:</span> Doc<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> List<span class="token punctuation">[</span>ExerciseToken<span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Go through the Spacy tokens and generates DSL based ExerciseToken"""</span>
        <span class="token keyword">for</span> nlp_token <span class="token keyword">in</span> nlp_tokens<span class="token punctuation">:</span>
            change_event <span class="token operator">=</span> self<span class="token punctuation">.</span>_change_event<span class="token punctuation">(</span>nlp_token<span class="token punctuation">)</span>
            <span class="token keyword">if</span> change_event <span class="token operator">==</span> ParseEvents<span class="token punctuation">.</span>ENDSTR<span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>_end_string<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">elif</span> change_event <span class="token operator">==</span> ParseEvents<span class="token punctuation">.</span>ENDCLOZE<span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>_end_cloze<span class="token punctuation">(</span><span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>_dispatch_to_buffer<span class="token punctuation">(</span>nlp_token<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_end_doc<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>exercises_tokens</code>`,go,ks,ds,fa,lr,xo,pn,ir,Eo,b,ha,rr,cr,ma,ur,kr,wa,dr,fr,ya,hr,Io,ln,we,mr,To,rn,ye,wr;return{c(){x=o("h2"),C=o("a"),fn=o("span"),Go=i("What I\u2019m trying to do"),_a=c(),_e=o("p"),Wo=i("Oh, the never ending quest to master German. I am at the stage where I get it most of it right and can hold conversations. But I still make a lot of small mistakes. I do take language classes every so often to raise the level. But although the improvement is visible, it never fully eradicates those mistakes. I need many more repetitions than the the 10-12 offered by textbooks. In other words, I need Python."),va=c(),E=o("p"),Uo=i("Textbooks exercises seem to follow a few patterns. Sentences with words in the wrong order, and missing words seem to be the most popular. It shouldn\u2019t be too hard to replicate those exercises. Better, to "),hn=o("em"),Vo=i("automate"),Ko=i(" this replication so that I have an endless supply of exercises. I can use "),_s=o("a"),Jo=i("the free dataset Timo Block made available on GitHub"),Qo=i(" as a starting point."),ba=c(),L=o("h3"),R=o("a"),mn=o("span"),Zo=i("The end result"),ga=c(),w=o("p"),Xo=i("The end result will have some similarities with flashcards software. Particularly "),vs=o("a"),Yo=i("Anki"),$o=i(", which I use a lot. I will use fall back on Anki concepts when needed, as I have vague plans to integrate some of these exercises with it. But what I am building now is a self contained CLI script. It will pick a German document and feed it to the interface sentence by sentence. The UI will obscure, or partially obscure, parts of the sentences. The task is to guess the obscured words. The first iteration will obscure \u201D"),bs=o("a"),sp=i("adpositions"),ep=i("\u201D, (or \u201Cprepositions\u201D in traditional grammar). And will partially obscure \u201D"),gs=o("a"),np=i("determiners"),ap=i("\u201D (articles and some pronounous and some adjectives). An example should make this clearer"),xa=c(),xs=o("pre"),Ea=c(),ve=o("p"),tp=i("A nice to have would also be to insert trick ones where there shouldn\u2019t be any. For example before years - it\u2019s a common mistake by English speakers to say \u201CIn 1989\u2026\u201D, but in German it\u2019s either \u201C1989\u2026\u201D or \u201CIm Jahr 1989\u2026\u201D"),Ia=c(),D=o("h3"),M=o("a"),wn=o("span"),op=i("The plan"),Ta=c(),f=o("ol"),yn=o("li"),pp=i("The raw dataset is a CSV file with a list of \u201Cdocs\u201D and labels. I don\u2019t need the label, but I need each doc\u2019s sequential \u201Csentences\u201D. That makes the CSV in its current form unsuitable. I will need to preprocess the CSV file"),lp=c(),_n=o("li"),ip=i("A repository will load up the persistent storage (the CSV file). It will fetch a \u2018next\u2019 sentence whenever asked. It may also save updated versions of the CSV file."),rp=c(),vn=o("li"),cp=i("An exercise generator will ask the repository for a sentence. It will then use NLP to turn into an exercise."),up=c(),bn=o("li"),kp=i("A session service will ask the exercise generator for N exercises. It will organise them as a queue, and will provide them to the UI when asked. It will also reorganise the queue depending on the outcome of the exercise."),dp=c(),gn=o("li"),fp=i("The UI will ask the session service for the next exercise, display it, and allow the user to guess. It will then communicate success / error to the session service and get the next exercise."),Pa=c(),q=o("h2"),F=o("a"),xn=o("span"),hp=i("Step 0: Setting up the python project"),Aa=c(),Es=o("p"),mp=i("For that I simply use "),be=o("a"),wp=i("my trusty cookiecutter template"),Sa=c(),Is=o("pre"),za=c(),ge=o("p"),yp=i("And I\u2019m good to go. All I need is to create a folder with the article data"),Ca=c(),Ts=o("pre"),La=c(),H=o("h2"),N=o("a"),En=o("span"),_p=i("Creating a CLI app for prepositions training"),Ra=c(),j=o("h3"),B=o("a"),In=o("span"),vp=i("Step 1: Preprocessing the documents"),Da=c(),xe=o("p"),bp=i("So the source data is a csv where each row has two fields: a label which I don\u2019t need, and a \u201Cdoc\u201D which I need to split into \u201Csentences\u201D. I can think of two ways of handling that:"),Ma=c(),O=o("ol"),Tn=o("li"),gp=i("remove the labels from the CSV and turn it into a TXT with a doc on each line. The repository will then be responsible for lazily splitting a doc in sentences, and holding them in memory"),xp=c(),Pn=o("li"),Ep=i("remove the labels and split each row\u2019s single doc field into multiple sentence fields. The repository will not do any splitting"),qa=c(),Ee=o("p"),Ip=i("(1) is easier on the pre-processing side; it can be a one line awk command. But I think (2) is cleaner. Typically a repository only knows about I/O and interactions with the persistent layer. It shouldn\u2019t need to know how to extract sentences from docs."),Fa=c(),I=o("p"),Tp=i("I will use "),Ps=o("a"),Pp=i("SpaCy"),Ap=i(" to split docs into sentences. Before using it I need not only to install SpaCy, but to download the model too. "),As=o("a"),Sp=i("A list of downloadable language models"),zp=i(" is available on the SpaCy website."),Ha=c(),Ss=o("pre"),Na=c(),Ie=o("p"),Cp=i("This is the script I came up with. Nothing fancy and no tests, it\u2019s just a one off script. No CLI frameworks, it simply reads input from sys.argv."),ja=c(),zs=o("pre"),Ba=c(),Te=o("p"),Lp=i("It runs for about 10 mins on an M1 laptop, but gets the job done"),Oa=c(),Cs=o("pre"),Ga=c(),G=o("h3"),W=o("a"),An=o("span"),Rp=i("Step 2: a repository for German sentences"),Wa=c(),Pe=o("p"),Dp=i("The repository module is responsible for"),Ua=c(),y=o("ul"),Sn=o("li"),Mp=i("interacting with the persistent storage"),qp=c(),zn=o("li"),Fp=i("fetching a \u2018next\u2019 sentence from the doc currently marked as \u2018current\u2019"),Hp=c(),Cn=o("li"),Np=i("if no doc is marked as \u2018current\u2019, one will be picked randomly"),jp=c(),Ln=o("li"),Bp=i("when the last sentence of a doc is picked, the \u2018current\u2019 marker is cleared"),Va=c(),Ae=o("p"),Op=i("In my personal projects I like using TDD, so I\u2019ll start with a test"),Ka=c(),Ls=o("pre"),Ja=c(),U=o("p"),Gp=i("The "),Rn=o("i"),Wp=i("is_sentence"),Up=i(" is an as yet to coded util function. It can easily be overcomplicated, so I will create tests for it too."),Qa=c(),Rs=o("pre"),Za=c(),V=o("p"),Vp=i("And a few more; the "),Se=o("a"),Kp=i("test file is available on github"),Jp=i(". But back to the task at and. Here\u2019s the minimum amount of scaffolding needed to run the tests\u2026"),Xa=c(),Ds=o("pre"),Ya=c(),ze=o("p"),Qp=i("\u2026and fail them, as expected - I am returning an empty string after all"),$a=c(),Ms=o("pre"),st=c(),Ce=o("p"),Zp=i("Returning a sentence requires a few steps. First the repo must get hold of the data file and open it. As this is a first iteration, I\u2019m going to keep the \u2018getting hold of the data\u2019 part as simple as possible. But a config file in which to store the location of the data file is a must. Even at this early stage, coupling the location of the code and the data is a bad idea."),et=c(),qs=o("pre"),nt=c(),K=o("p"),Xp=i("Since the .tsv file could be quite large, I\u2019d rather not load it in memory. Instead the "),Dn=o("code"),Yp=i("__init__"),$p=i(" method will store all the boundaries between lines. When needed, it will use that information to fetch a doc\u2019s sentences. And finally, next_sentence will pop the next sentence from memory. When the repo runs out of sentences, it will fetch a new doc. For now it\u2019s always the same doc that is being fetched. That\u2019s enough to pass the test."),at=c(),Fs=o("pre"),tt=c(),J=o("p"),sl=i("A quick smoke test on the REPL shows it works. Calling "),Mn=o("code"),el=i("t.next_sentence"),nl=i(" a few times shows the repo going through all the sentences, and then starting again when they run out"),ot=c(),Hs=o("pre"),pt=c(),Le=o("p"),al=i("There are a lot of improvements to be done - for example the docs file is hard coded, there is no error handling, and so on. But at this stage the aim is to get a working MVP, so I\u2019ll move on to the next step."),lt=c(),Q=o("h3"),Z=o("a"),qn=o("span"),tl=i("Step 3: generating the exercises"),it=c(),Re=o("p"),ol=i("This module will take a German sentence as input. It will turn it into a data structure with some slots at various stages of guessing."),rt=c(),Ns=o("pre"),ct=c(),X=o("p"),pl=i("How best to represent that? I\u2019ll go for a sequence of tokens. Some will be strings (\u201Cfoo foo\u201D) and some will be guessable slots (\u201Cnach dem/xxxxx dxxx\u201D). Anki has a similar concept to \u201Cguessable slot\u201D, i.e., \u2019"),js=o("a"),ll=i("cloze deletions"),il=i("\u2019. I will use that name."),ut=c(),Y=o("h4"),$=o("a"),Fn=o("span"),rl=i("Cloze: an entity to represent a guessable slot within exercise"),kt=c(),De=o("p"),cl=i("The cloze will be initialised with the string to be guessed and the obfuscated version. It will keep an internal state: guessed or not guessed. The behaviours it will need to implement are"),dt=c(),_=o("ul"),Bs=o("li"),ul=i("return the correct string representation depending on state (method: "),Hn=o("code"),kl=i("__str__"),dl=i(")"),fl=c(),Os=o("li"),hl=i("return the state as a boolean (property: "),Nn=o("code"),ml=i("guessed"),wl=i(")"),yl=c(),Gs=o("li"),_l=i("accept a guess and change state accordingly; return state (method: "),jn=o("code"),vl=i("guess(str)"),bl=i(")"),gl=c(),Bn=o("li"),xl=i("being an entity, it has a unique, immutable ID"),ft=c(),Me=o("p"),El=i("That\u2019s it for now - no hints like Anki, no counting wrong attempts, nothing fancy. Just focus on the minimal functionality to get something that works. As usual I\u2019ll start with a test, and watch it fail. \u201Cfake\u201D is a Faker instance which is included in the cookiecutter template which generates the project. I use it all the time."),ht=c(),Ws=o("pre"),mt=c(),qe=o("p"),Il=i("What I am creating is essential an Entity, in DDD speak, and will live in the /models folder. I use pydantic for enforcing validation and some type checking. Pydantic is already installed because it\u2019s a dependency of SpaCy. Sadly SpaCy decided to pin its versions to ^1.7.4, so I am forced to do the same"),wt=c(),Us=o("pre"),yt=c(),ss=o("p"),Tl=i("Never mind. On with the model. First of all I create an "),On=o("code"),Pl=i("__init__.py"),Al=i(" file, since this is a new folder"),_t=c(),Vs=o("pre"),vt=c(),Fe=o("p"),Sl=i("Then the Cloze itself"),bt=c(),Ks=o("pre"),gt=c(),He=o("p"),zl=i("The test will fail with a more sensible message"),xt=c(),Js=o("pre"),Et=c(),es=o("p"),Cl=i("Getting closer. All I need to pass the text is a "),Gn=o("code"),Ll=i("__str__"),Rl=i(" method"),It=c(),Qs=o("pre"),Tt=c(),Ne=o("p"),Dl=i("The next requirements are about being able to guess, and the state changing. Here\u2019s the test\u2026"),Pt=c(),Zs=o("pre"),At=c(),je=o("p"),Ml=i("And the code that makes it pass"),St=c(),Xs=o("pre"),zt=c(),Be=o("p"),ql=i("I can also improve the previous test to ensure the string representation changes to mimic the state. It should still pass."),Ct=c(),Ys=o("pre"),Lt=c(),Oe=o("p"),Fl=i("And it does. The last requirement is an immutable ID field."),Rt=c(),$s=o("pre"),Dt=c(),ns=o("p"),Hl=i("To make a Pydantic field immutable after initialisation, I create a similarly named private field (in this case, \u201D_id\u201D). It doesn\u2019t "),Wn=o("em"),Nl=i("need"),jl=i(" to be similarly named, but it would just be confusing if it wasn\u2019t. Then I add a @property with the name I want to expose. But I will not add a setter - therefore there will be no way to overwrite the id"),Mt=c(),se=o("pre"),qt=c(),Ge=o("p"),Bl=i("This is enough for now - all tests pass. As usual, there\u2019s plenty more I could do. But right now the focus is on getting a working prototype"),Ft=c(),as=o("h5"),ts=o("a"),Un=o("span"),Ol=i("Creating a Faker provider for the Cloze"),Ht=c(),T=o("p"),Gl=i("Actually, that was "),Vn=o("em"),Wl=i("almost"),Ul=i(" all. Another thing I tend to do when creating data structures, is to create Faker providers for them. It makes stubbing them in tests much more easier. I have "),We=o("a"),Vl=i("a post about creating Faker providers"),Kl=i(", so I won\u2019t repeat myself here."),Nt=c(),os=o("h4"),ps=o("a"),Kn=o("span"),Jl=i("An exercise entity"),jt=c(),Ue=o("p"),Ql=i("An Exercise is also an entity; it contains a list of either strings or clozes. Since it is an entity, some of the tests will be the same as Cloze. That\u2019s repetitive and hence error prone. It\u2019s time for the first refactor! I will extract the entity-ness of Cloze into a BaseEntity class. Since I\u2019m refactoring, no need for new tests just yet; the existing ones should do."),Bt=c(),Ve=o("p"),Zl=i("I extract the _id/id handling part to a new class, BaseEntity\u2026"),Ot=c(),ee=o("pre"),Gt=c(),Ke=o("p"),Xl=i("\u2026and make the Cloze inherit from it"),Wt=c(),ne=o("pre"),Ut=c(),ls=o("p"),Yl=i("The tests still pass. I move the id test from test_cloze to a new test file, "),Jn=o("code"),$l=i("test_base_entity.py"),si=i(", and commit."),Vt=c(),Je=o("p"),ei=i("I\u2019m read to start on the Exercise entity. The requirement:"),Kt=c(),d=o("ul"),Qn=o("li"),ni=i("it consists of a list of tokens which can be either strings or Clozes"),ai=c(),Zn=o("li"),ti=i("there must be at least one of each in the list"),oi=c(),ae=o("li"),pi=i("it can provide a guessed flag, which is true if "),Xn=o("em"),li=i("all"),ii=i(" Clozes are guessed, false otherwise"),ri=c(),Yn=o("li"),ci=i("it returns the current string representation of the exercise, with each Cloze either obfuscated or not depending on status"),ui=c(),$n=o("li"),ki=i("it can receive a guess for a given Cloze, and flip its state accordingly"),di=c(),sa=o("li"),fi=i("it can return a list of all Clozes"),Jt=c(),is=o("p"),hi=i("So there are quite a lot of requirements. I\u2019m not going to go through all of them here; this entity is not that interesting. It\u2019s similar to Cloze but with a couple more bells and whistles. Just like a Cloze, I built a fake for it. If interested, "),Qe=o("a"),mi=i("the code is available in the repo"),wi=i("."),Qt=c(),rs=o("h4"),cs=o("a"),ea=o("span"),yi=i("An exercises service"),Zt=c(),Ze=o("p"),_i=i("OK, I have a data structure for an exercise. But what will I do with it? A service will provide them to the rest of the application. The requirements for the service are:"),Xt=c(),P=o("ul"),na=o("li"),vi=i("it can be asked to provide N exercises, where N is one by default"),bi=c(),aa=o("li"),gi=i("these exercises will be based on raw sentences from the sentence repository"),xi=c(),ta=o("li"),Ei=i("it will not remember any information about the last fetched sentence, but it will rely on the repository for that"),Yt=c(),Xe=o("p"),Ii=i("This is just the bare minimum for a working MVP. But it\u2019s totally non-scalable. In a production app I\u2019d either cache or precompute the exercises. But on with the tests. Starting with the simplest step, make sure I get a list back"),$t=c(),te=o("pre"),so=c(),h=o("p"),Ti=i("This is the minimal code that fails the test "),oa=o("em"),Pi=i("properly"),Ai=i(". Note that failing the test properly is part of the process. The point is to avoid tests that never fail, or fail for the wrong reason. Failing with \u201Cmethod not implemented\u201D or similar is not useful. The test should fail with an "),pa=o("em"),Si=i("AssertionError"),zi=i(", so that I know it\u2019s by design. Not only that, but it should be the "),la=o("em"),Ci=i("last"),Li=i(" assertion. Again, it\u2019s all about ensuring failures are controlled, not just coincidental. The worst thing in testing is tests that keep on passing after you have changed the code. "),oe=o("a"),Ri=i("Mark Seemann\u2019s \u201CA red-green-refactor checklist \u201D"),Di=i(" is a good read on this topic"),eo=c(),Ye=o("p"),Mi=i("The minimal code that breaks the test is"),no=c(),pe=o("pre"),ao=c(),$e=o("p"),qi=i("Which indeed fails with my last assertion"),to=c(),le=o("pre"),oo=c(),sn=o("p"),Fi=i("And I can easily fix by turning the return value to a []"),po=c(),ie=o("pre"),lo=c(),en=o("p"),Hi=i("I tend to avoid mocks when building prototypes. They often end up taking up a lot of development effort. But to trust a service that transforms data, one needs to test it with different data. For that, I need the ability to inject different data repositories into the service. I.e., I need mocks, and dependency injection. I start by writing a test for that."),io=c(),re=o("pre"),ro=c(),nn=o("p"),Ni=i("Which fails, as expected, but can be fixed with"),co=c(),ce=o("pre"),uo=c(),an=o("p"),ji=i("Now finally generating an exercise. I need to pick some German sentences to use in the mock. I do it with the following bash command. It sorts all the lines randomly. Then it passes them to head which picks the top (1). The selected, random line has its tabs separated by a \\n character"),ko=c(),ue=o("pre"),fo=c(),v=o("p"),Bi=i("I\u2019m going to skip the rest of the TDD journey, fun as it was, and jump straight to the end result. Here\u2019s probably the most important test (the rest are avaialble "),tn=o("a"),Oi=i("on github"),Gi=i("). It uses the same mock as before, but this time it also returns a sentence (the "),ia=o("code"),Wi=i("repo.next_sentence =..."),Ui=i(" line). The sentence is "),ke=o("a"),Vi=i("parametrised"),Ki=i(", meaning the test will be run once for each sentence."),ho=c(),de=o("pre"),mo=c(),us=o("p"),Ji=i("The sentences are in a separate folder, "),ra=o("code"),Qi=i("tests/testsupport"),Zi=i(". They are organised as an array. Each item should correspond to a token. So \u201CAuch\u201D would be a string, \u201Cmit\u201D would be a cloze, \u201CKanzlerin Angela Merkel sei Sch\xE4uble \u201D would be a string, etc."),wo=c(),fe=o("pre"),yo=c(),m=o("p"),Xi=i("The code that makes it happen was quite complex. "),ca=o("code"),Yi=i("GuessPrepositionExerciseService"),$i=i(" has a private spacy instance, "),ua=o("code"),sr=i("_nlp"),er=i(", which it is used to parse sentences on demand. The result of the spacy parsing is a series of token objects, one for each word or punctuation. Organising those spacy word tokens into my own exercise string fragment tokens was the complex part. I moved it to a separate class, "),ka=o("code"),nr=i("TokenParser"),ar=i(". A new instance is create whenever a sentence needs parsing. It uses the centralised "),da=o("code"),tr=i("_nlp"),or=i(" instance."),_o=c(),he=o("pre"),vo=c(),on=o("p"),pr=i(`The TokenParser is inspired by XML Sax parser. Sax parsers go through the tokens (words, or chars, doesn\u2019t matter), keeping a copy. Whenever there is a boundary (the start of a comment, the closing of a tag, etc) it fires an event, and adds the tokens saved so to the event.
Different parts of the code listen for specific events, and run when those happen. In my case I followed the basic idea of looking out for boundary events and firing off a specific function corresponding to each boundary. But I didn\u2019t use events at this stage. The only three boundaries I am interested in are end of a string (I need to add the string to the list of tokens), and end of cloze (I need to create the Cloze and add it to the list of tokens).`),bo=c(),me=o("pre"),go=c(),ks=o("h4"),ds=o("a"),fa=o("span"),lr=i("A session service"),xo=c(),pn=o("p"),ir=i("I am getting close the user interface now. A Session is a service that manages Exercises for a user. Its requirements"),Eo=c(),b=o("ul"),ha=o("li"),rr=i("When asked by user, it fetches n Exercises from the Exercise service (by default N=4) and organises them as a queue"),cr=c(),ma=o("li"),ur=i("When asked by user, it provides the next exercise to the user"),kr=c(),wa=o("li"),dr=i("It receives the user attempts at guessing and returns either \u2018success\u2019 or \u2018failure\u2019"),fr=c(),ya=o("li"),hr=i("It does a very basic scheduling (like Anki) - exercises need to be solved successfully twice in a row per Session. When the current exercise is solved successfully for the first time, or when it is a failure, it is re-added to the end of the queue. When solved successfully for the second time, it is removed from the queue"),Io=c(),ln=o("p"),we=o("a"),mr=i("https://medium.com/steve-cruz/domain-driven-design-ddd-file-structure-ade7fb26553d"),To=c(),rn=o("p"),ye=o("a"),wr=i("https://blog.jacobsdata.com/2020/03/02/a-clean-domain-driven-design-architectural-template"),this.h()},l(s){x=p(s,"H2",{id:!0});var a=l(x);C=p(a,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var Lr=l(C);fn=p(Lr,"SPAN",{class:!0}),l(fn).forEach(e),Lr.forEach(e),Go=r(a,"What I\u2019m trying to do"),a.forEach(e),_a=u(s),_e=p(s,"P",{});var Rr=l(_e);Wo=r(Rr,"Oh, the never ending quest to master German. I am at the stage where I get it most of it right and can hold conversations. But I still make a lot of small mistakes. I do take language classes every so often to raise the level. But although the improvement is visible, it never fully eradicates those mistakes. I need many more repetitions than the the 10-12 offered by textbooks. In other words, I need Python."),Rr.forEach(e),va=u(s),E=p(s,"P",{});var cn=l(E);Uo=r(cn,"Textbooks exercises seem to follow a few patterns. Sentences with words in the wrong order, and missing words seem to be the most popular. It shouldn\u2019t be too hard to replicate those exercises. Better, to "),hn=p(cn,"EM",{});var Dr=l(hn);Vo=r(Dr,"automate"),Dr.forEach(e),Ko=r(cn," this replication so that I have an endless supply of exercises. I can use "),_s=p(cn,"A",{href:!0,rel:!0});var Mr=l(_s);Jo=r(Mr,"the free dataset Timo Block made available on GitHub"),Mr.forEach(e),Qo=r(cn," as a starting point."),cn.forEach(e),ba=u(s),L=p(s,"H3",{id:!0});var yr=l(L);R=p(yr,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var qr=l(R);mn=p(qr,"SPAN",{class:!0}),l(mn).forEach(e),qr.forEach(e),Zo=r(yr,"The end result"),yr.forEach(e),ga=u(s),w=p(s,"P",{});var fs=l(w);Xo=r(fs,"The end result will have some similarities with flashcards software. Particularly "),vs=p(fs,"A",{href:!0,rel:!0});var Fr=l(vs);Yo=r(Fr,"Anki"),Fr.forEach(e),$o=r(fs,", which I use a lot. I will use fall back on Anki concepts when needed, as I have vague plans to integrate some of these exercises with it. But what I am building now is a self contained CLI script. It will pick a German document and feed it to the interface sentence by sentence. The UI will obscure, or partially obscure, parts of the sentences. The task is to guess the obscured words. The first iteration will obscure \u201D"),bs=p(fs,"A",{href:!0,rel:!0});var Hr=l(bs);sp=r(Hr,"adpositions"),Hr.forEach(e),ep=r(fs,"\u201D, (or \u201Cprepositions\u201D in traditional grammar). And will partially obscure \u201D"),gs=p(fs,"A",{href:!0,rel:!0});var Nr=l(gs);np=r(Nr,"determiners"),Nr.forEach(e),ap=r(fs,"\u201D (articles and some pronounous and some adjectives). An example should make this clearer"),fs.forEach(e),xa=u(s),xs=p(s,"PRE",{class:!0});var xk=l(xs);xk.forEach(e),Ea=u(s),ve=p(s,"P",{});var jr=l(ve);tp=r(jr,"A nice to have would also be to insert trick ones where there shouldn\u2019t be any. For example before years - it\u2019s a common mistake by English speakers to say \u201CIn 1989\u2026\u201D, but in German it\u2019s either \u201C1989\u2026\u201D or \u201CIm Jahr 1989\u2026\u201D"),jr.forEach(e),Ia=u(s),D=p(s,"H3",{id:!0});var _r=l(D);M=p(_r,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var Br=l(M);wn=p(Br,"SPAN",{class:!0}),l(wn).forEach(e),Br.forEach(e),op=r(_r,"The plan"),_r.forEach(e),Ta=u(s),f=p(s,"OL",{});var A=l(f);yn=p(A,"LI",{});var Or=l(yn);pp=r(Or,"The raw dataset is a CSV file with a list of \u201Cdocs\u201D and labels. I don\u2019t need the label, but I need each doc\u2019s sequential \u201Csentences\u201D. That makes the CSV in its current form unsuitable. I will need to preprocess the CSV file"),Or.forEach(e),lp=u(A),_n=p(A,"LI",{});var Gr=l(_n);ip=r(Gr,"A repository will load up the persistent storage (the CSV file). It will fetch a \u2018next\u2019 sentence whenever asked. It may also save updated versions of the CSV file."),Gr.forEach(e),rp=u(A),vn=p(A,"LI",{});var Wr=l(vn);cp=r(Wr,"An exercise generator will ask the repository for a sentence. It will then use NLP to turn into an exercise."),Wr.forEach(e),up=u(A),bn=p(A,"LI",{});var Ur=l(bn);kp=r(Ur,"A session service will ask the exercise generator for N exercises. It will organise them as a queue, and will provide them to the UI when asked. It will also reorganise the queue depending on the outcome of the exercise."),Ur.forEach(e),dp=u(A),gn=p(A,"LI",{});var Vr=l(gn);fp=r(Vr,"The UI will ask the session service for the next exercise, display it, and allow the user to guess. It will then communicate success / error to the session service and get the next exercise."),Vr.forEach(e),A.forEach(e),Pa=u(s),q=p(s,"H2",{id:!0});var vr=l(q);F=p(vr,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var Kr=l(F);xn=p(Kr,"SPAN",{class:!0}),l(xn).forEach(e),Kr.forEach(e),hp=r(vr,"Step 0: Setting up the python project"),vr.forEach(e),Aa=u(s),Es=p(s,"P",{});var br=l(Es);mp=r(br,"For that I simply use "),be=p(br,"A",{href:!0});var Jr=l(be);wp=r(Jr,"my trusty cookiecutter template"),Jr.forEach(e),br.forEach(e),Sa=u(s),Is=p(s,"PRE",{class:!0});var Ek=l(Is);Ek.forEach(e),za=u(s),ge=p(s,"P",{});var Qr=l(ge);yp=r(Qr,"And I\u2019m good to go. All I need is to create a folder with the article data"),Qr.forEach(e),Ca=u(s),Ts=p(s,"PRE",{class:!0});var Ik=l(Ts);Ik.forEach(e),La=u(s),H=p(s,"H2",{id:!0});var gr=l(H);N=p(gr,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var Zr=l(N);En=p(Zr,"SPAN",{class:!0}),l(En).forEach(e),Zr.forEach(e),_p=r(gr,"Creating a CLI app for prepositions training"),gr.forEach(e),Ra=u(s),j=p(s,"H3",{id:!0});var xr=l(j);B=p(xr,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var Xr=l(B);In=p(Xr,"SPAN",{class:!0}),l(In).forEach(e),Xr.forEach(e),vp=r(xr,"Step 1: Preprocessing the documents"),xr.forEach(e),Da=u(s),xe=p(s,"P",{});var Yr=l(xe);bp=r(Yr,"So the source data is a csv where each row has two fields: a label which I don\u2019t need, and a \u201Cdoc\u201D which I need to split into \u201Csentences\u201D. I can think of two ways of handling that:"),Yr.forEach(e),Ma=u(s),O=p(s,"OL",{});var Po=l(O);Tn=p(Po,"LI",{});var $r=l(Tn);gp=r($r,"remove the labels from the CSV and turn it into a TXT with a doc on each line. The repository will then be responsible for lazily splitting a doc in sentences, and holding them in memory"),$r.forEach(e),xp=u(Po),Pn=p(Po,"LI",{});var sc=l(Pn);Ep=r(sc,"remove the labels and split each row\u2019s single doc field into multiple sentence fields. The repository will not do any splitting"),sc.forEach(e),Po.forEach(e),qa=u(s),Ee=p(s,"P",{});var ec=l(Ee);Ip=r(ec,"(1) is easier on the pre-processing side; it can be a one line awk command. But I think (2) is cleaner. Typically a repository only knows about I/O and interactions with the persistent layer. It shouldn\u2019t need to know how to extract sentences from docs."),ec.forEach(e),Fa=u(s),I=p(s,"P",{});var un=l(I);Tp=r(un,"I will use "),Ps=p(un,"A",{href:!0,rel:!0});var nc=l(Ps);Pp=r(nc,"SpaCy"),nc.forEach(e),Ap=r(un," to split docs into sentences. Before using it I need not only to install SpaCy, but to download the model too. "),As=p(un,"A",{href:!0,rel:!0});var ac=l(As);Sp=r(ac,"A list of downloadable language models"),ac.forEach(e),zp=r(un," is available on the SpaCy website."),un.forEach(e),Ha=u(s),Ss=p(s,"PRE",{class:!0});var Tk=l(Ss);Tk.forEach(e),Na=u(s),Ie=p(s,"P",{});var tc=l(Ie);Cp=r(tc,"This is the script I came up with. Nothing fancy and no tests, it\u2019s just a one off script. No CLI frameworks, it simply reads input from sys.argv."),tc.forEach(e),ja=u(s),zs=p(s,"PRE",{class:!0});var Pk=l(zs);Pk.forEach(e),Ba=u(s),Te=p(s,"P",{});var oc=l(Te);Lp=r(oc,"It runs for about 10 mins on an M1 laptop, but gets the job done"),oc.forEach(e),Oa=u(s),Cs=p(s,"PRE",{class:!0});var Ak=l(Cs);Ak.forEach(e),Ga=u(s),G=p(s,"H3",{id:!0});var Er=l(G);W=p(Er,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var pc=l(W);An=p(pc,"SPAN",{class:!0}),l(An).forEach(e),pc.forEach(e),Rp=r(Er,"Step 2: a repository for German sentences"),Er.forEach(e),Wa=u(s),Pe=p(s,"P",{});var lc=l(Pe);Dp=r(lc,"The repository module is responsible for"),lc.forEach(e),Ua=u(s),y=p(s,"UL",{});var hs=l(y);Sn=p(hs,"LI",{});var ic=l(Sn);Mp=r(ic,"interacting with the persistent storage"),ic.forEach(e),qp=u(hs),zn=p(hs,"LI",{});var rc=l(zn);Fp=r(rc,"fetching a \u2018next\u2019 sentence from the doc currently marked as \u2018current\u2019"),rc.forEach(e),Hp=u(hs),Cn=p(hs,"LI",{});var cc=l(Cn);Np=r(cc,"if no doc is marked as \u2018current\u2019, one will be picked randomly"),cc.forEach(e),jp=u(hs),Ln=p(hs,"LI",{});var uc=l(Ln);Bp=r(uc,"when the last sentence of a doc is picked, the \u2018current\u2019 marker is cleared"),uc.forEach(e),hs.forEach(e),Va=u(s),Ae=p(s,"P",{});var kc=l(Ae);Op=r(kc,"In my personal projects I like using TDD, so I\u2019ll start with a test"),kc.forEach(e),Ka=u(s),Ls=p(s,"PRE",{class:!0});var Sk=l(Ls);Sk.forEach(e),Ja=u(s),U=p(s,"P",{});var Ao=l(U);Gp=r(Ao,"The "),Rn=p(Ao,"I",{});var dc=l(Rn);Wp=r(dc,"is_sentence"),dc.forEach(e),Up=r(Ao," is an as yet to coded util function. It can easily be overcomplicated, so I will create tests for it too."),Ao.forEach(e),Qa=u(s),Rs=p(s,"PRE",{class:!0});var zk=l(Rs);zk.forEach(e),Za=u(s),V=p(s,"P",{});var So=l(V);Vp=r(So,"And a few more; the "),Se=p(So,"A",{href:!0});var fc=l(Se);Kp=r(fc,"test file is available on github"),fc.forEach(e),Jp=r(So,". But back to the task at and. Here\u2019s the minimum amount of scaffolding needed to run the tests\u2026"),So.forEach(e),Xa=u(s),Ds=p(s,"PRE",{class:!0});var Ck=l(Ds);Ck.forEach(e),Ya=u(s),ze=p(s,"P",{});var hc=l(ze);Qp=r(hc,"\u2026and fail them, as expected - I am returning an empty string after all"),hc.forEach(e),$a=u(s),Ms=p(s,"PRE",{class:!0});var Lk=l(Ms);Lk.forEach(e),st=u(s),Ce=p(s,"P",{});var mc=l(Ce);Zp=r(mc,"Returning a sentence requires a few steps. First the repo must get hold of the data file and open it. As this is a first iteration, I\u2019m going to keep the \u2018getting hold of the data\u2019 part as simple as possible. But a config file in which to store the location of the data file is a must. Even at this early stage, coupling the location of the code and the data is a bad idea."),mc.forEach(e),et=u(s),qs=p(s,"PRE",{class:!0});var Rk=l(qs);Rk.forEach(e),nt=u(s),K=p(s,"P",{});var zo=l(K);Xp=r(zo,"Since the .tsv file could be quite large, I\u2019d rather not load it in memory. Instead the "),Dn=p(zo,"CODE",{});var wc=l(Dn);Yp=r(wc,"__init__"),wc.forEach(e),$p=r(zo," method will store all the boundaries between lines. When needed, it will use that information to fetch a doc\u2019s sentences. And finally, next_sentence will pop the next sentence from memory. When the repo runs out of sentences, it will fetch a new doc. For now it\u2019s always the same doc that is being fetched. That\u2019s enough to pass the test."),zo.forEach(e),at=u(s),Fs=p(s,"PRE",{class:!0});var Dk=l(Fs);Dk.forEach(e),tt=u(s),J=p(s,"P",{});var Co=l(J);sl=r(Co,"A quick smoke test on the REPL shows it works. Calling "),Mn=p(Co,"CODE",{});var yc=l(Mn);el=r(yc,"t.next_sentence"),yc.forEach(e),nl=r(Co," a few times shows the repo going through all the sentences, and then starting again when they run out"),Co.forEach(e),ot=u(s),Hs=p(s,"PRE",{class:!0});var Mk=l(Hs);Mk.forEach(e),pt=u(s),Le=p(s,"P",{});var _c=l(Le);al=r(_c,"There are a lot of improvements to be done - for example the docs file is hard coded, there is no error handling, and so on. But at this stage the aim is to get a working MVP, so I\u2019ll move on to the next step."),_c.forEach(e),lt=u(s),Q=p(s,"H3",{id:!0});var Ir=l(Q);Z=p(Ir,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var vc=l(Z);qn=p(vc,"SPAN",{class:!0}),l(qn).forEach(e),vc.forEach(e),tl=r(Ir,"Step 3: generating the exercises"),Ir.forEach(e),it=u(s),Re=p(s,"P",{});var bc=l(Re);ol=r(bc,"This module will take a German sentence as input. It will turn it into a data structure with some slots at various stages of guessing."),bc.forEach(e),rt=u(s),Ns=p(s,"PRE",{class:!0});var qk=l(Ns);qk.forEach(e),ct=u(s),X=p(s,"P",{});var Lo=l(X);pl=r(Lo,"How best to represent that? I\u2019ll go for a sequence of tokens. Some will be strings (\u201Cfoo foo\u201D) and some will be guessable slots (\u201Cnach dem/xxxxx dxxx\u201D). Anki has a similar concept to \u201Cguessable slot\u201D, i.e., \u2019"),js=p(Lo,"A",{href:!0,rel:!0});var gc=l(js);ll=r(gc,"cloze deletions"),gc.forEach(e),il=r(Lo,"\u2019. I will use that name."),Lo.forEach(e),ut=u(s),Y=p(s,"H4",{id:!0});var Tr=l(Y);$=p(Tr,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var xc=l($);Fn=p(xc,"SPAN",{class:!0}),l(Fn).forEach(e),xc.forEach(e),rl=r(Tr,"Cloze: an entity to represent a guessable slot within exercise"),Tr.forEach(e),kt=u(s),De=p(s,"P",{});var Ec=l(De);cl=r(Ec,"The cloze will be initialised with the string to be guessed and the obfuscated version. It will keep an internal state: guessed or not guessed. The behaviours it will need to implement are"),Ec.forEach(e),dt=u(s),_=p(s,"UL",{});var ms=l(_);Bs=p(ms,"LI",{});var Ro=l(Bs);ul=r(Ro,"return the correct string representation depending on state (method: "),Hn=p(Ro,"CODE",{});var Ic=l(Hn);kl=r(Ic,"__str__"),Ic.forEach(e),dl=r(Ro,")"),Ro.forEach(e),fl=u(ms),Os=p(ms,"LI",{});var Do=l(Os);hl=r(Do,"return the state as a boolean (property: "),Nn=p(Do,"CODE",{});var Tc=l(Nn);ml=r(Tc,"guessed"),Tc.forEach(e),wl=r(Do,")"),Do.forEach(e),yl=u(ms),Gs=p(ms,"LI",{});var Mo=l(Gs);_l=r(Mo,"accept a guess and change state accordingly; return state (method: "),jn=p(Mo,"CODE",{});var Pc=l(jn);vl=r(Pc,"guess(str)"),Pc.forEach(e),bl=r(Mo,")"),Mo.forEach(e),gl=u(ms),Bn=p(ms,"LI",{});var Ac=l(Bn);xl=r(Ac,"being an entity, it has a unique, immutable ID"),Ac.forEach(e),ms.forEach(e),ft=u(s),Me=p(s,"P",{});var Sc=l(Me);El=r(Sc,"That\u2019s it for now - no hints like Anki, no counting wrong attempts, nothing fancy. Just focus on the minimal functionality to get something that works. As usual I\u2019ll start with a test, and watch it fail. \u201Cfake\u201D is a Faker instance which is included in the cookiecutter template which generates the project. I use it all the time."),Sc.forEach(e),ht=u(s),Ws=p(s,"PRE",{class:!0});var Fk=l(Ws);Fk.forEach(e),mt=u(s),qe=p(s,"P",{});var zc=l(qe);Il=r(zc,"What I am creating is essential an Entity, in DDD speak, and will live in the /models folder. I use pydantic for enforcing validation and some type checking. Pydantic is already installed because it\u2019s a dependency of SpaCy. Sadly SpaCy decided to pin its versions to ^1.7.4, so I am forced to do the same"),zc.forEach(e),wt=u(s),Us=p(s,"PRE",{class:!0});var Hk=l(Us);Hk.forEach(e),yt=u(s),ss=p(s,"P",{});var qo=l(ss);Tl=r(qo,"Never mind. On with the model. First of all I create an "),On=p(qo,"CODE",{});var Cc=l(On);Pl=r(Cc,"__init__.py"),Cc.forEach(e),Al=r(qo," file, since this is a new folder"),qo.forEach(e),_t=u(s),Vs=p(s,"PRE",{class:!0});var Nk=l(Vs);Nk.forEach(e),vt=u(s),Fe=p(s,"P",{});var Lc=l(Fe);Sl=r(Lc,"Then the Cloze itself"),Lc.forEach(e),bt=u(s),Ks=p(s,"PRE",{class:!0});var jk=l(Ks);jk.forEach(e),gt=u(s),He=p(s,"P",{});var Rc=l(He);zl=r(Rc,"The test will fail with a more sensible message"),Rc.forEach(e),xt=u(s),Js=p(s,"PRE",{class:!0});var Bk=l(Js);Bk.forEach(e),Et=u(s),es=p(s,"P",{});var Fo=l(es);Cl=r(Fo,"Getting closer. All I need to pass the text is a "),Gn=p(Fo,"CODE",{});var Dc=l(Gn);Ll=r(Dc,"__str__"),Dc.forEach(e),Rl=r(Fo," method"),Fo.forEach(e),It=u(s),Qs=p(s,"PRE",{class:!0});var Ok=l(Qs);Ok.forEach(e),Tt=u(s),Ne=p(s,"P",{});var Mc=l(Ne);Dl=r(Mc,"The next requirements are about being able to guess, and the state changing. Here\u2019s the test\u2026"),Mc.forEach(e),Pt=u(s),Zs=p(s,"PRE",{class:!0});var Gk=l(Zs);Gk.forEach(e),At=u(s),je=p(s,"P",{});var qc=l(je);Ml=r(qc,"And the code that makes it pass"),qc.forEach(e),St=u(s),Xs=p(s,"PRE",{class:!0});var Wk=l(Xs);Wk.forEach(e),zt=u(s),Be=p(s,"P",{});var Fc=l(Be);ql=r(Fc,"I can also improve the previous test to ensure the string representation changes to mimic the state. It should still pass."),Fc.forEach(e),Ct=u(s),Ys=p(s,"PRE",{class:!0});var Uk=l(Ys);Uk.forEach(e),Lt=u(s),Oe=p(s,"P",{});var Hc=l(Oe);Fl=r(Hc,"And it does. The last requirement is an immutable ID field."),Hc.forEach(e),Rt=u(s),$s=p(s,"PRE",{class:!0});var Vk=l($s);Vk.forEach(e),Dt=u(s),ns=p(s,"P",{});var Ho=l(ns);Hl=r(Ho,"To make a Pydantic field immutable after initialisation, I create a similarly named private field (in this case, \u201D_id\u201D). It doesn\u2019t "),Wn=p(Ho,"EM",{});var Nc=l(Wn);Nl=r(Nc,"need"),Nc.forEach(e),jl=r(Ho," to be similarly named, but it would just be confusing if it wasn\u2019t. Then I add a @property with the name I want to expose. But I will not add a setter - therefore there will be no way to overwrite the id"),Ho.forEach(e),Mt=u(s),se=p(s,"PRE",{class:!0});var Kk=l(se);Kk.forEach(e),qt=u(s),Ge=p(s,"P",{});var jc=l(Ge);Bl=r(jc,"This is enough for now - all tests pass. As usual, there\u2019s plenty more I could do. But right now the focus is on getting a working prototype"),jc.forEach(e),Ft=u(s),as=p(s,"H5",{id:!0});var Pr=l(as);ts=p(Pr,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var Bc=l(ts);Un=p(Bc,"SPAN",{class:!0}),l(Un).forEach(e),Bc.forEach(e),Ol=r(Pr,"Creating a Faker provider for the Cloze"),Pr.forEach(e),Ht=u(s),T=p(s,"P",{});var kn=l(T);Gl=r(kn,"Actually, that was "),Vn=p(kn,"EM",{});var Oc=l(Vn);Wl=r(Oc,"almost"),Oc.forEach(e),Ul=r(kn," all. Another thing I tend to do when creating data structures, is to create Faker providers for them. It makes stubbing them in tests much more easier. I have "),We=p(kn,"A",{href:!0});var Gc=l(We);Vl=r(Gc,"a post about creating Faker providers"),Gc.forEach(e),Kl=r(kn,", so I won\u2019t repeat myself here."),kn.forEach(e),Nt=u(s),os=p(s,"H4",{id:!0});var Ar=l(os);ps=p(Ar,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var Wc=l(ps);Kn=p(Wc,"SPAN",{class:!0}),l(Kn).forEach(e),Wc.forEach(e),Jl=r(Ar,"An exercise entity"),Ar.forEach(e),jt=u(s),Ue=p(s,"P",{});var Uc=l(Ue);Ql=r(Uc,"An Exercise is also an entity; it contains a list of either strings or clozes. Since it is an entity, some of the tests will be the same as Cloze. That\u2019s repetitive and hence error prone. It\u2019s time for the first refactor! I will extract the entity-ness of Cloze into a BaseEntity class. Since I\u2019m refactoring, no need for new tests just yet; the existing ones should do."),Uc.forEach(e),Bt=u(s),Ve=p(s,"P",{});var Vc=l(Ve);Zl=r(Vc,"I extract the _id/id handling part to a new class, BaseEntity\u2026"),Vc.forEach(e),Ot=u(s),ee=p(s,"PRE",{class:!0});var Jk=l(ee);Jk.forEach(e),Gt=u(s),Ke=p(s,"P",{});var Kc=l(Ke);Xl=r(Kc,"\u2026and make the Cloze inherit from it"),Kc.forEach(e),Wt=u(s),ne=p(s,"PRE",{class:!0});var Qk=l(ne);Qk.forEach(e),Ut=u(s),ls=p(s,"P",{});var No=l(ls);Yl=r(No,"The tests still pass. I move the id test from test_cloze to a new test file, "),Jn=p(No,"CODE",{});var Jc=l(Jn);$l=r(Jc,"test_base_entity.py"),Jc.forEach(e),si=r(No,", and commit."),No.forEach(e),Vt=u(s),Je=p(s,"P",{});var Qc=l(Je);ei=r(Qc,"I\u2019m read to start on the Exercise entity. The requirement:"),Qc.forEach(e),Kt=u(s),d=p(s,"UL",{});var g=l(d);Qn=p(g,"LI",{});var Zc=l(Qn);ni=r(Zc,"it consists of a list of tokens which can be either strings or Clozes"),Zc.forEach(e),ai=u(g),Zn=p(g,"LI",{});var Xc=l(Zn);ti=r(Xc,"there must be at least one of each in the list"),Xc.forEach(e),oi=u(g),ae=p(g,"LI",{});var jo=l(ae);pi=r(jo,"it can provide a guessed flag, which is true if "),Xn=p(jo,"EM",{});var Yc=l(Xn);li=r(Yc,"all"),Yc.forEach(e),ii=r(jo," Clozes are guessed, false otherwise"),jo.forEach(e),ri=u(g),Yn=p(g,"LI",{});var $c=l(Yn);ci=r($c,"it returns the current string representation of the exercise, with each Cloze either obfuscated or not depending on status"),$c.forEach(e),ui=u(g),$n=p(g,"LI",{});var su=l($n);ki=r(su,"it can receive a guess for a given Cloze, and flip its state accordingly"),su.forEach(e),di=u(g),sa=p(g,"LI",{});var eu=l(sa);fi=r(eu,"it can return a list of all Clozes"),eu.forEach(e),g.forEach(e),Jt=u(s),is=p(s,"P",{});var Bo=l(is);hi=r(Bo,"So there are quite a lot of requirements. I\u2019m not going to go through all of them here; this entity is not that interesting. It\u2019s similar to Cloze but with a couple more bells and whistles. Just like a Cloze, I built a fake for it. If interested, "),Qe=p(Bo,"A",{href:!0});var nu=l(Qe);mi=r(nu,"the code is available in the repo"),nu.forEach(e),wi=r(Bo,"."),Bo.forEach(e),Qt=u(s),rs=p(s,"H4",{id:!0});var Sr=l(rs);cs=p(Sr,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var au=l(cs);ea=p(au,"SPAN",{class:!0}),l(ea).forEach(e),au.forEach(e),yi=r(Sr,"An exercises service"),Sr.forEach(e),Zt=u(s),Ze=p(s,"P",{});var tu=l(Ze);_i=r(tu,"OK, I have a data structure for an exercise. But what will I do with it? A service will provide them to the rest of the application. The requirements for the service are:"),tu.forEach(e),Xt=u(s),P=p(s,"UL",{});var dn=l(P);na=p(dn,"LI",{});var ou=l(na);vi=r(ou,"it can be asked to provide N exercises, where N is one by default"),ou.forEach(e),bi=u(dn),aa=p(dn,"LI",{});var pu=l(aa);gi=r(pu,"these exercises will be based on raw sentences from the sentence repository"),pu.forEach(e),xi=u(dn),ta=p(dn,"LI",{});var lu=l(ta);Ei=r(lu,"it will not remember any information about the last fetched sentence, but it will rely on the repository for that"),lu.forEach(e),dn.forEach(e),Yt=u(s),Xe=p(s,"P",{});var iu=l(Xe);Ii=r(iu,"This is just the bare minimum for a working MVP. But it\u2019s totally non-scalable. In a production app I\u2019d either cache or precompute the exercises. But on with the tests. Starting with the simplest step, make sure I get a list back"),iu.forEach(e),$t=u(s),te=p(s,"PRE",{class:!0});var Zk=l(te);Zk.forEach(e),so=u(s),h=p(s,"P",{});var S=l(h);Ti=r(S,"This is the minimal code that fails the test "),oa=p(S,"EM",{});var ru=l(oa);Pi=r(ru,"properly"),ru.forEach(e),Ai=r(S,". Note that failing the test properly is part of the process. The point is to avoid tests that never fail, or fail for the wrong reason. Failing with \u201Cmethod not implemented\u201D or similar is not useful. The test should fail with an "),pa=p(S,"EM",{});var cu=l(pa);Si=r(cu,"AssertionError"),cu.forEach(e),zi=r(S,", so that I know it\u2019s by design. Not only that, but it should be the "),la=p(S,"EM",{});var uu=l(la);Ci=r(uu,"last"),uu.forEach(e),Li=r(S," assertion. Again, it\u2019s all about ensuring failures are controlled, not just coincidental. The worst thing in testing is tests that keep on passing after you have changed the code. "),oe=p(S,"A",{href:!0,rel:!0});var ku=l(oe);Ri=r(ku,"Mark Seemann\u2019s \u201CA red-green-refactor checklist \u201D"),ku.forEach(e),Di=r(S," is a good read on this topic"),S.forEach(e),eo=u(s),Ye=p(s,"P",{});var du=l(Ye);Mi=r(du,"The minimal code that breaks the test is"),du.forEach(e),no=u(s),pe=p(s,"PRE",{class:!0});var Xk=l(pe);Xk.forEach(e),ao=u(s),$e=p(s,"P",{});var fu=l($e);qi=r(fu,"Which indeed fails with my last assertion"),fu.forEach(e),to=u(s),le=p(s,"PRE",{class:!0});var Yk=l(le);Yk.forEach(e),oo=u(s),sn=p(s,"P",{});var hu=l(sn);Fi=r(hu,"And I can easily fix by turning the return value to a []"),hu.forEach(e),po=u(s),ie=p(s,"PRE",{class:!0});var $k=l(ie);$k.forEach(e),lo=u(s),en=p(s,"P",{});var mu=l(en);Hi=r(mu,"I tend to avoid mocks when building prototypes. They often end up taking up a lot of development effort. But to trust a service that transforms data, one needs to test it with different data. For that, I need the ability to inject different data repositories into the service. I.e., I need mocks, and dependency injection. I start by writing a test for that."),mu.forEach(e),io=u(s),re=p(s,"PRE",{class:!0});var sd=l(re);sd.forEach(e),ro=u(s),nn=p(s,"P",{});var wu=l(nn);Ni=r(wu,"Which fails, as expected, but can be fixed with"),wu.forEach(e),co=u(s),ce=p(s,"PRE",{class:!0});var ed=l(ce);ed.forEach(e),uo=u(s),an=p(s,"P",{});var yu=l(an);ji=r(yu,"Now finally generating an exercise. I need to pick some German sentences to use in the mock. I do it with the following bash command. It sorts all the lines randomly. Then it passes them to head which picks the top (1). The selected, random line has its tabs separated by a \\n character"),yu.forEach(e),ko=u(s),ue=p(s,"PRE",{class:!0});var nd=l(ue);nd.forEach(e),fo=u(s),v=p(s,"P",{});var ws=l(v);Bi=r(ws,"I\u2019m going to skip the rest of the TDD journey, fun as it was, and jump straight to the end result. Here\u2019s probably the most important test (the rest are avaialble "),tn=p(ws,"A",{href:!0});var _u=l(tn);Oi=r(_u,"on github"),_u.forEach(e),Gi=r(ws,"). It uses the same mock as before, but this time it also returns a sentence (the "),ia=p(ws,"CODE",{});var vu=l(ia);Wi=r(vu,"repo.next_sentence =..."),vu.forEach(e),Ui=r(ws," line). The sentence is "),ke=p(ws,"A",{href:!0,rel:!0});var bu=l(ke);Vi=r(bu,"parametrised"),bu.forEach(e),Ki=r(ws,", meaning the test will be run once for each sentence."),ws.forEach(e),ho=u(s),de=p(s,"PRE",{class:!0});var ad=l(de);ad.forEach(e),mo=u(s),us=p(s,"P",{});var Oo=l(us);Ji=r(Oo,"The sentences are in a separate folder, "),ra=p(Oo,"CODE",{});var gu=l(ra);Qi=r(gu,"tests/testsupport"),gu.forEach(e),Zi=r(Oo,". They are organised as an array. Each item should correspond to a token. So \u201CAuch\u201D would be a string, \u201Cmit\u201D would be a cloze, \u201CKanzlerin Angela Merkel sei Sch\xE4uble \u201D would be a string, etc."),Oo.forEach(e),wo=u(s),fe=p(s,"PRE",{class:!0});var td=l(fe);td.forEach(e),yo=u(s),m=p(s,"P",{});var z=l(m);Xi=r(z,"The code that makes it happen was quite complex. "),ca=p(z,"CODE",{});var xu=l(ca);Yi=r(xu,"GuessPrepositionExerciseService"),xu.forEach(e),$i=r(z," has a private spacy instance, "),ua=p(z,"CODE",{});var Eu=l(ua);sr=r(Eu,"_nlp"),Eu.forEach(e),er=r(z,", which it is used to parse sentences on demand. The result of the spacy parsing is a series of token objects, one for each word or punctuation. Organising those spacy word tokens into my own exercise string fragment tokens was the complex part. I moved it to a separate class, "),ka=p(z,"CODE",{});var Iu=l(ka);nr=r(Iu,"TokenParser"),Iu.forEach(e),ar=r(z,". A new instance is create whenever a sentence needs parsing. It uses the centralised "),da=p(z,"CODE",{});var Tu=l(da);tr=r(Tu,"_nlp"),Tu.forEach(e),or=r(z," instance."),z.forEach(e),_o=u(s),he=p(s,"PRE",{class:!0});var od=l(he);od.forEach(e),vo=u(s),on=p(s,"P",{});var Pu=l(on);pr=r(Pu,`The TokenParser is inspired by XML Sax parser. Sax parsers go through the tokens (words, or chars, doesn\u2019t matter), keeping a copy. Whenever there is a boundary (the start of a comment, the closing of a tag, etc) it fires an event, and adds the tokens saved so to the event.
Different parts of the code listen for specific events, and run when those happen. In my case I followed the basic idea of looking out for boundary events and firing off a specific function corresponding to each boundary. But I didn\u2019t use events at this stage. The only three boundaries I am interested in are end of a string (I need to add the string to the list of tokens), and end of cloze (I need to create the Cloze and add it to the list of tokens).`),Pu.forEach(e),bo=u(s),me=p(s,"PRE",{class:!0});var pd=l(me);pd.forEach(e),go=u(s),ks=p(s,"H4",{id:!0});var zr=l(ks);ds=p(zr,"A",{"aria-hidden":!0,tabindex:!0,href:!0});var Au=l(ds);fa=p(Au,"SPAN",{class:!0}),l(fa).forEach(e),Au.forEach(e),lr=r(zr,"A session service"),zr.forEach(e),xo=u(s),pn=p(s,"P",{});var Su=l(pn);ir=r(Su,"I am getting close the user interface now. A Session is a service that manages Exercises for a user. Its requirements"),Su.forEach(e),Eo=u(s),b=p(s,"UL",{});var ys=l(b);ha=p(ys,"LI",{});var zu=l(ha);rr=r(zu,"When asked by user, it fetches n Exercises from the Exercise service (by default N=4) and organises them as a queue"),zu.forEach(e),cr=u(ys),ma=p(ys,"LI",{});var Cu=l(ma);ur=r(Cu,"When asked by user, it provides the next exercise to the user"),Cu.forEach(e),kr=u(ys),wa=p(ys,"LI",{});var Lu=l(wa);dr=r(Lu,"It receives the user attempts at guessing and returns either \u2018success\u2019 or \u2018failure\u2019"),Lu.forEach(e),fr=u(ys),ya=p(ys,"LI",{});var Ru=l(ya);hr=r(Ru,"It does a very basic scheduling (like Anki) - exercises need to be solved successfully twice in a row per Session. When the current exercise is solved successfully for the first time, or when it is a failure, it is re-added to the end of the queue. When solved successfully for the second time, it is removed from the queue"),Ru.forEach(e),ys.forEach(e),Io=u(s),ln=p(s,"P",{});var Du=l(ln);we=p(Du,"A",{href:!0,rel:!0});var Mu=l(we);mr=r(Mu,"https://medium.com/steve-cruz/domain-driven-design-ddd-file-structure-ade7fb26553d"),Mu.forEach(e),Du.forEach(e),To=u(s),rn=p(s,"P",{});var qu=l(rn);ye=p(qu,"A",{href:!0,rel:!0});var Fu=l(ye);wr=r(Fu,"https://blog.jacobsdata.com/2020/03/02/a-clean-domain-driven-design-architectural-template"),Fu.forEach(e),qu.forEach(e),this.h()},h(){k(fn,"class","icon icon-link"),k(C,"aria-hidden","true"),k(C,"tabindex","-1"),k(C,"href","#what-im-trying-to-do"),k(x,"id","what-im-trying-to-do"),k(_s,"href","https://tblock.github.io/10kGNAD/"),k(_s,"rel","nofollow"),k(mn,"class","icon icon-link"),k(R,"aria-hidden","true"),k(R,"tabindex","-1"),k(R,"href","#the-end-result"),k(L,"id","the-end-result"),k(vs,"href","https://apps.ankiweb.net/"),k(vs,"rel","nofollow"),k(bs,"href","https://universaldependencies.org/u/pos/ADP.html"),k(bs,"rel","nofollow"),k(gs,"href","https://universaldependencies.org/u/pos/DET.html"),k(gs,"rel","nofollow"),k(xs,"class","language-txt"),k(wn,"class","icon icon-link"),k(M,"aria-hidden","true"),k(M,"tabindex","-1"),k(M,"href","#the-plan"),k(D,"id","the-plan"),k(xn,"class","icon icon-link"),k(F,"aria-hidden","true"),k(F,"tabindex","-1"),k(F,"href","#step-0-setting-up-the-python-project"),k(q,"id","step-0-setting-up-the-python-project"),k(be,"href","/blog/creating-a-poetry-driven-python-project-template-with-cookiecutter"),k(Is,"class","language-bash"),k(Ts,"class","language-bash"),k(En,"class","icon icon-link"),k(N,"aria-hidden","true"),k(N,"tabindex","-1"),k(N,"href","#creating-a-cli-app-for-prepositions-training"),k(H,"id","creating-a-cli-app-for-prepositions-training"),k(In,"class","icon icon-link"),k(B,"aria-hidden","true"),k(B,"tabindex","-1"),k(B,"href","#step-1-preprocessing-the-documents"),k(j,"id","step-1-preprocessing-the-documents"),k(Ps,"href","https://spacy.io/"),k(Ps,"rel","nofollow"),k(As,"href","https://spacy.io/usage/models"),k(As,"rel","nofollow"),k(Ss,"class","language-bash"),k(zs,"class","language-python"),k(Cs,"class","language-bash"),k(An,"class","icon icon-link"),k(W,"aria-hidden","true"),k(W,"tabindex","-1"),k(W,"href","#step-2-a-repository-for-german-sentences"),k(G,"id","step-2-a-repository-for-german-sentences"),k(Ls,"class","language-python"),k(Rs,"class","language-python"),k(Se,"href","TODO"),k(Ds,"class","language-python"),k(Ms,"class","language-bash"),k(qs,"class","language-python"),k(Fs,"class","language-python"),k(Hs,"class","language-python"),k(qn,"class","icon icon-link"),k(Z,"aria-hidden","true"),k(Z,"tabindex","-1"),k(Z,"href","#step-3-generating-the-exercises"),k(Q,"id","step-3-generating-the-exercises"),k(Ns,"class","language-txt"),k(js,"href","https://docs.ankiweb.net/editing.html#cloze-deletion"),k(js,"rel","nofollow"),k(Fn,"class","icon icon-link"),k($,"aria-hidden","true"),k($,"tabindex","-1"),k($,"href","#cloze-an-entity-to-represent-a-guessable-slot-within-exercise"),k(Y,"id","cloze-an-entity-to-represent-a-guessable-slot-within-exercise"),k(Ws,"class","language-python"),k(Us,"class","language-bash"),k(Vs,"class","language-python"),k(Ks,"class","language-python"),k(Js,"class","language-bash"),k(Qs,"class","language-diff"),k(Zs,"class","language-python"),k(Xs,"class","language-diff"),k(Ys,"class","language-diff"),k($s,"class","language-python"),k(se,"class","language-diff"),k(Un,"class","icon icon-link"),k(ts,"aria-hidden","true"),k(ts,"tabindex","-1"),k(ts,"href","#creating-a-faker-provider-for-the-cloze"),k(as,"id","creating-a-faker-provider-for-the-cloze"),k(We,"href","/blog/create-fake-dataset-fixtures-testing-with-faker"),k(Kn,"class","icon icon-link"),k(ps,"aria-hidden","true"),k(ps,"tabindex","-1"),k(ps,"href","#an-exercise-entity"),k(os,"id","an-exercise-entity"),k(ee,"class","language-python"),k(ne,"class","language-diff"),k(Qe,"href","TODO"),k(ea,"class","icon icon-link"),k(cs,"aria-hidden","true"),k(cs,"tabindex","-1"),k(cs,"href","#an-exercises-service"),k(rs,"id","an-exercises-service"),k(te,"class","language-python"),k(oe,"href","https://blog.ploeh.dk/2019/10/21/a-red-green-refactor-checklist/"),k(oe,"rel","nofollow"),k(pe,"class","language-python"),k(le,"class","language-diff"),k(ie,"class","language-python"),k(re,"class","language-python"),k(ce,"class","language-diff"),k(ue,"class","language-bash"),k(tn,"href","TODO"),k(ke,"href","https://docs.pytest.org/en/7.1.x/example/parametrize.html"),k(ke,"rel","nofollow"),k(de,"class","language-python"),k(fe,"class","language-python"),k(he,"class","language-python"),k(me,"class","language-python"),k(fa,"class","icon icon-link"),k(ds,"aria-hidden","true"),k(ds,"tabindex","-1"),k(ds,"href","#a-session-service"),k(ks,"id","a-session-service"),k(we,"href","https://medium.com/steve-cruz/domain-driven-design-ddd-file-structure-ade7fb26553d"),k(we,"rel","nofollow"),k(ye,"href","https://blog.jacobsdata.com/2020/03/02/a-clean-domain-driven-design-architectural-template"),k(ye,"rel","nofollow")},m(s,a){t(s,x,a),n(x,C),n(C,fn),n(x,Go),t(s,_a,a),t(s,_e,a),n(_e,Wo),t(s,va,a),t(s,E,a),n(E,Uo),n(E,hn),n(hn,Vo),n(E,Ko),n(E,_s),n(_s,Jo),n(E,Qo),t(s,ba,a),t(s,L,a),n(L,R),n(R,mn),n(L,Zo),t(s,ga,a),t(s,w,a),n(w,Xo),n(w,vs),n(vs,Yo),n(w,$o),n(w,bs),n(bs,sp),n(w,ep),n(w,gs),n(gs,np),n(w,ap),t(s,xa,a),t(s,xs,a),xs.innerHTML=Nu,t(s,Ea,a),t(s,ve,a),n(ve,tp),t(s,Ia,a),t(s,D,a),n(D,M),n(M,wn),n(D,op),t(s,Ta,a),t(s,f,a),n(f,yn),n(yn,pp),n(f,lp),n(f,_n),n(_n,ip),n(f,rp),n(f,vn),n(vn,cp),n(f,up),n(f,bn),n(bn,kp),n(f,dp),n(f,gn),n(gn,fp),t(s,Pa,a),t(s,q,a),n(q,F),n(F,xn),n(q,hp),t(s,Aa,a),t(s,Es,a),n(Es,mp),n(Es,be),n(be,wp),t(s,Sa,a),t(s,Is,a),Is.innerHTML=ju,t(s,za,a),t(s,ge,a),n(ge,yp),t(s,Ca,a),t(s,Ts,a),Ts.innerHTML=Bu,t(s,La,a),t(s,H,a),n(H,N),n(N,En),n(H,_p),t(s,Ra,a),t(s,j,a),n(j,B),n(B,In),n(j,vp),t(s,Da,a),t(s,xe,a),n(xe,bp),t(s,Ma,a),t(s,O,a),n(O,Tn),n(Tn,gp),n(O,xp),n(O,Pn),n(Pn,Ep),t(s,qa,a),t(s,Ee,a),n(Ee,Ip),t(s,Fa,a),t(s,I,a),n(I,Tp),n(I,Ps),n(Ps,Pp),n(I,Ap),n(I,As),n(As,Sp),n(I,zp),t(s,Ha,a),t(s,Ss,a),Ss.innerHTML=Ou,t(s,Na,a),t(s,Ie,a),n(Ie,Cp),t(s,ja,a),t(s,zs,a),zs.innerHTML=Gu,t(s,Ba,a),t(s,Te,a),n(Te,Lp),t(s,Oa,a),t(s,Cs,a),Cs.innerHTML=Wu,t(s,Ga,a),t(s,G,a),n(G,W),n(W,An),n(G,Rp),t(s,Wa,a),t(s,Pe,a),n(Pe,Dp),t(s,Ua,a),t(s,y,a),n(y,Sn),n(Sn,Mp),n(y,qp),n(y,zn),n(zn,Fp),n(y,Hp),n(y,Cn),n(Cn,Np),n(y,jp),n(y,Ln),n(Ln,Bp),t(s,Va,a),t(s,Ae,a),n(Ae,Op),t(s,Ka,a),t(s,Ls,a),Ls.innerHTML=Uu,t(s,Ja,a),t(s,U,a),n(U,Gp),n(U,Rn),n(Rn,Wp),n(U,Up),t(s,Qa,a),t(s,Rs,a),Rs.innerHTML=Vu,t(s,Za,a),t(s,V,a),n(V,Vp),n(V,Se),n(Se,Kp),n(V,Jp),t(s,Xa,a),t(s,Ds,a),Ds.innerHTML=Ku,t(s,Ya,a),t(s,ze,a),n(ze,Qp),t(s,$a,a),t(s,Ms,a),Ms.innerHTML=Ju,t(s,st,a),t(s,Ce,a),n(Ce,Zp),t(s,et,a),t(s,qs,a),qs.innerHTML=Qu,t(s,nt,a),t(s,K,a),n(K,Xp),n(K,Dn),n(Dn,Yp),n(K,$p),t(s,at,a),t(s,Fs,a),Fs.innerHTML=Zu,t(s,tt,a),t(s,J,a),n(J,sl),n(J,Mn),n(Mn,el),n(J,nl),t(s,ot,a),t(s,Hs,a),Hs.innerHTML=Xu,t(s,pt,a),t(s,Le,a),n(Le,al),t(s,lt,a),t(s,Q,a),n(Q,Z),n(Z,qn),n(Q,tl),t(s,it,a),t(s,Re,a),n(Re,ol),t(s,rt,a),t(s,Ns,a),Ns.innerHTML=Yu,t(s,ct,a),t(s,X,a),n(X,pl),n(X,js),n(js,ll),n(X,il),t(s,ut,a),t(s,Y,a),n(Y,$),n($,Fn),n(Y,rl),t(s,kt,a),t(s,De,a),n(De,cl),t(s,dt,a),t(s,_,a),n(_,Bs),n(Bs,ul),n(Bs,Hn),n(Hn,kl),n(Bs,dl),n(_,fl),n(_,Os),n(Os,hl),n(Os,Nn),n(Nn,ml),n(Os,wl),n(_,yl),n(_,Gs),n(Gs,_l),n(Gs,jn),n(jn,vl),n(Gs,bl),n(_,gl),n(_,Bn),n(Bn,xl),t(s,ft,a),t(s,Me,a),n(Me,El),t(s,ht,a),t(s,Ws,a),Ws.innerHTML=$u,t(s,mt,a),t(s,qe,a),n(qe,Il),t(s,wt,a),t(s,Us,a),Us.innerHTML=sk,t(s,yt,a),t(s,ss,a),n(ss,Tl),n(ss,On),n(On,Pl),n(ss,Al),t(s,_t,a),t(s,Vs,a),Vs.innerHTML=ek,t(s,vt,a),t(s,Fe,a),n(Fe,Sl),t(s,bt,a),t(s,Ks,a),Ks.innerHTML=nk,t(s,gt,a),t(s,He,a),n(He,zl),t(s,xt,a),t(s,Js,a),Js.innerHTML=ak,t(s,Et,a),t(s,es,a),n(es,Cl),n(es,Gn),n(Gn,Ll),n(es,Rl),t(s,It,a),t(s,Qs,a),Qs.innerHTML=tk,t(s,Tt,a),t(s,Ne,a),n(Ne,Dl),t(s,Pt,a),t(s,Zs,a),Zs.innerHTML=ok,t(s,At,a),t(s,je,a),n(je,Ml),t(s,St,a),t(s,Xs,a),Xs.innerHTML=pk,t(s,zt,a),t(s,Be,a),n(Be,ql),t(s,Ct,a),t(s,Ys,a),Ys.innerHTML=lk,t(s,Lt,a),t(s,Oe,a),n(Oe,Fl),t(s,Rt,a),t(s,$s,a),$s.innerHTML=ik,t(s,Dt,a),t(s,ns,a),n(ns,Hl),n(ns,Wn),n(Wn,Nl),n(ns,jl),t(s,Mt,a),t(s,se,a),se.innerHTML=rk,t(s,qt,a),t(s,Ge,a),n(Ge,Bl),t(s,Ft,a),t(s,as,a),n(as,ts),n(ts,Un),n(as,Ol),t(s,Ht,a),t(s,T,a),n(T,Gl),n(T,Vn),n(Vn,Wl),n(T,Ul),n(T,We),n(We,Vl),n(T,Kl),t(s,Nt,a),t(s,os,a),n(os,ps),n(ps,Kn),n(os,Jl),t(s,jt,a),t(s,Ue,a),n(Ue,Ql),t(s,Bt,a),t(s,Ve,a),n(Ve,Zl),t(s,Ot,a),t(s,ee,a),ee.innerHTML=ck,t(s,Gt,a),t(s,Ke,a),n(Ke,Xl),t(s,Wt,a),t(s,ne,a),ne.innerHTML=uk,t(s,Ut,a),t(s,ls,a),n(ls,Yl),n(ls,Jn),n(Jn,$l),n(ls,si),t(s,Vt,a),t(s,Je,a),n(Je,ei),t(s,Kt,a),t(s,d,a),n(d,Qn),n(Qn,ni),n(d,ai),n(d,Zn),n(Zn,ti),n(d,oi),n(d,ae),n(ae,pi),n(ae,Xn),n(Xn,li),n(ae,ii),n(d,ri),n(d,Yn),n(Yn,ci),n(d,ui),n(d,$n),n($n,ki),n(d,di),n(d,sa),n(sa,fi),t(s,Jt,a),t(s,is,a),n(is,hi),n(is,Qe),n(Qe,mi),n(is,wi),t(s,Qt,a),t(s,rs,a),n(rs,cs),n(cs,ea),n(rs,yi),t(s,Zt,a),t(s,Ze,a),n(Ze,_i),t(s,Xt,a),t(s,P,a),n(P,na),n(na,vi),n(P,bi),n(P,aa),n(aa,gi),n(P,xi),n(P,ta),n(ta,Ei),t(s,Yt,a),t(s,Xe,a),n(Xe,Ii),t(s,$t,a),t(s,te,a),te.innerHTML=kk,t(s,so,a),t(s,h,a),n(h,Ti),n(h,oa),n(oa,Pi),n(h,Ai),n(h,pa),n(pa,Si),n(h,zi),n(h,la),n(la,Ci),n(h,Li),n(h,oe),n(oe,Ri),n(h,Di),t(s,eo,a),t(s,Ye,a),n(Ye,Mi),t(s,no,a),t(s,pe,a),pe.innerHTML=dk,t(s,ao,a),t(s,$e,a),n($e,qi),t(s,to,a),t(s,le,a),le.innerHTML=fk,t(s,oo,a),t(s,sn,a),n(sn,Fi),t(s,po,a),t(s,ie,a),ie.innerHTML=hk,t(s,lo,a),t(s,en,a),n(en,Hi),t(s,io,a),t(s,re,a),re.innerHTML=mk,t(s,ro,a),t(s,nn,a),n(nn,Ni),t(s,co,a),t(s,ce,a),ce.innerHTML=wk,t(s,uo,a),t(s,an,a),n(an,ji),t(s,ko,a),t(s,ue,a),ue.innerHTML=yk,t(s,fo,a),t(s,v,a),n(v,Bi),n(v,tn),n(tn,Oi),n(v,Gi),n(v,ia),n(ia,Wi),n(v,Ui),n(v,ke),n(ke,Vi),n(v,Ki),t(s,ho,a),t(s,de,a),de.innerHTML=_k,t(s,mo,a),t(s,us,a),n(us,Ji),n(us,ra),n(ra,Qi),n(us,Zi),t(s,wo,a),t(s,fe,a),fe.innerHTML=vk,t(s,yo,a),t(s,m,a),n(m,Xi),n(m,ca),n(ca,Yi),n(m,$i),n(m,ua),n(ua,sr),n(m,er),n(m,ka),n(ka,nr),n(m,ar),n(m,da),n(da,tr),n(m,or),t(s,_o,a),t(s,he,a),he.innerHTML=bk,t(s,vo,a),t(s,on,a),n(on,pr),t(s,bo,a),t(s,me,a),me.innerHTML=gk,t(s,go,a),t(s,ks,a),n(ks,ds),n(ds,fa),n(ks,lr),t(s,xo,a),t(s,pn,a),n(pn,ir),t(s,Eo,a),t(s,b,a),n(b,ha),n(ha,rr),n(b,cr),n(b,ma),n(ma,ur),n(b,kr),n(b,wa),n(wa,dr),n(b,fr),n(b,ya),n(ya,hr),t(s,Io,a),t(s,ln,a),n(ln,we),n(we,mr),t(s,To,a),t(s,rn,a),n(rn,ye),n(ye,wr)},p:Cr,i:Cr,o:Cr,d(s){s&&e(x),s&&e(_a),s&&e(_e),s&&e(va),s&&e(E),s&&e(ba),s&&e(L),s&&e(ga),s&&e(w),s&&e(xa),s&&e(xs),s&&e(Ea),s&&e(ve),s&&e(Ia),s&&e(D),s&&e(Ta),s&&e(f),s&&e(Pa),s&&e(q),s&&e(Aa),s&&e(Es),s&&e(Sa),s&&e(Is),s&&e(za),s&&e(ge),s&&e(Ca),s&&e(Ts),s&&e(La),s&&e(H),s&&e(Ra),s&&e(j),s&&e(Da),s&&e(xe),s&&e(Ma),s&&e(O),s&&e(qa),s&&e(Ee),s&&e(Fa),s&&e(I),s&&e(Ha),s&&e(Ss),s&&e(Na),s&&e(Ie),s&&e(ja),s&&e(zs),s&&e(Ba),s&&e(Te),s&&e(Oa),s&&e(Cs),s&&e(Ga),s&&e(G),s&&e(Wa),s&&e(Pe),s&&e(Ua),s&&e(y),s&&e(Va),s&&e(Ae),s&&e(Ka),s&&e(Ls),s&&e(Ja),s&&e(U),s&&e(Qa),s&&e(Rs),s&&e(Za),s&&e(V),s&&e(Xa),s&&e(Ds),s&&e(Ya),s&&e(ze),s&&e($a),s&&e(Ms),s&&e(st),s&&e(Ce),s&&e(et),s&&e(qs),s&&e(nt),s&&e(K),s&&e(at),s&&e(Fs),s&&e(tt),s&&e(J),s&&e(ot),s&&e(Hs),s&&e(pt),s&&e(Le),s&&e(lt),s&&e(Q),s&&e(it),s&&e(Re),s&&e(rt),s&&e(Ns),s&&e(ct),s&&e(X),s&&e(ut),s&&e(Y),s&&e(kt),s&&e(De),s&&e(dt),s&&e(_),s&&e(ft),s&&e(Me),s&&e(ht),s&&e(Ws),s&&e(mt),s&&e(qe),s&&e(wt),s&&e(Us),s&&e(yt),s&&e(ss),s&&e(_t),s&&e(Vs),s&&e(vt),s&&e(Fe),s&&e(bt),s&&e(Ks),s&&e(gt),s&&e(He),s&&e(xt),s&&e(Js),s&&e(Et),s&&e(es),s&&e(It),s&&e(Qs),s&&e(Tt),s&&e(Ne),s&&e(Pt),s&&e(Zs),s&&e(At),s&&e(je),s&&e(St),s&&e(Xs),s&&e(zt),s&&e(Be),s&&e(Ct),s&&e(Ys),s&&e(Lt),s&&e(Oe),s&&e(Rt),s&&e($s),s&&e(Dt),s&&e(ns),s&&e(Mt),s&&e(se),s&&e(qt),s&&e(Ge),s&&e(Ft),s&&e(as),s&&e(Ht),s&&e(T),s&&e(Nt),s&&e(os),s&&e(jt),s&&e(Ue),s&&e(Bt),s&&e(Ve),s&&e(Ot),s&&e(ee),s&&e(Gt),s&&e(Ke),s&&e(Wt),s&&e(ne),s&&e(Ut),s&&e(ls),s&&e(Vt),s&&e(Je),s&&e(Kt),s&&e(d),s&&e(Jt),s&&e(is),s&&e(Qt),s&&e(rs),s&&e(Zt),s&&e(Ze),s&&e(Xt),s&&e(P),s&&e(Yt),s&&e(Xe),s&&e($t),s&&e(te),s&&e(so),s&&e(h),s&&e(eo),s&&e(Ye),s&&e(no),s&&e(pe),s&&e(ao),s&&e($e),s&&e(to),s&&e(le),s&&e(oo),s&&e(sn),s&&e(po),s&&e(ie),s&&e(lo),s&&e(en),s&&e(io),s&&e(re),s&&e(ro),s&&e(nn),s&&e(co),s&&e(ce),s&&e(uo),s&&e(an),s&&e(ko),s&&e(ue),s&&e(fo),s&&e(v),s&&e(ho),s&&e(de),s&&e(mo),s&&e(us),s&&e(wo),s&&e(fe),s&&e(yo),s&&e(m),s&&e(_o),s&&e(he),s&&e(vo),s&&e(on),s&&e(bo),s&&e(me),s&&e(go),s&&e(ks),s&&e(xo),s&&e(pn),s&&e(Eo),s&&e(b),s&&e(Io),s&&e(ln),s&&e(To),s&&e(rn)}}}const kd={excerpt:"Textbooks are good for learning grammar, but their exercises tend to be too limited. Doing 10 or 12 exercises once is a good start but does not come anywhere near what I need. Which is hundreds of exercises over the course of days, weeks even.",date:"2022-09-26T00:00:00.000Z",draft:!0,archived:!1,title:"A simple Python app for German grammar exercises, with SpaCy",tags:["german","python","spacy"]};class dd extends ld{constructor(x){super(),id(this,x,null,cd,rd,{})}}export{dd as default,kd as metadata};
